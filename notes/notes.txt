9/30/2013

[x]: correct the current subfigure fiasco in icml. additive_faithful.tex
[ ]: don't say signal strength, say the "relevant variables" need to be sufficiently relevant.

[ ]: read over Theorem 6.8 (uniform risk deviation) appendix.tex
[ ]: add references to some more existing variable selection papers
[ ]: elaborate deterministic condition proof
[x]: elaborate additive faithfulness proof


the following papers should be cited: 
* Garvesh Raskutti, Martin J. Wainwright, and Bin Yu. 
Minimax-optimal rates for sparse additive models over 
kernel classes via convex programming. 
J. Mach. Learn. Res., 13:389–427, 2012. 

* Vladimir Koltchinskii and Ming Yuan. 
Sparsity in multiple kernel learning. 
Ann. Statist., 38 (6):3660–3695, 2010 




* Laetitia Comminges, Arnak S. Dalalyan 
Tight conditions for consistency of variable selection 
in the context of high dimensionality. 
Ann. Statist, 40(5), 2667-2696, 2012 

Comment: They estimate the coefficient of the regression function with respect to a Fourier basis and then threshold the estimated coefficients. They prove that sparsistency is achievable so long as $n > log p$ and $n > exp(s)$. They assume that the functions are smooth with respect to the Fourier basis. The fourier coefficients however seem difficult to estimate; it appears that the knowledge of the true density is required.


* Bertin, K. and Lecu´e, G. (2008). Selection of variables 
and dimension reduction in high-dimensional non-parametric 
regression. Electron. J. Stat. 2 1224–1241 

Comment: They assume Holder smoothness and use multiple L1 regularized local polynomial smoothing to detect the sparsity pattern. They require that $d = log n$ as well.


* "Approximation of Functions of Few Variables in High Dimensions" 
by DeVore, Petrova and Wojtaszczyk. 

Comment: They show that a scaling of $n > log p, n > exp(l)$ is possible if one adaptively selects the points on which the high-dimensional functions are evaluated.


9/29/2013

We can either input de-meaned Y into the algorithm.
Or we can add an additional centering term in the optimization


9/27/2013

EDIT NOTE:

Right now, $\mathcal{C}_0[s,b,B,L]$ is the $\epsilon$-cover. 
Need to replace _0 with _\epsilon

Sometimes, \mathcal{C}^s[b,B,L] represent set of additive convex functions, 
replace with C[s,b,B,L]

sparsistency.tex: called restricted regression --> called THE restricted
					n-dimensional change, \mathbf{1}_n


9/25/2013

* Correct proof mistakes in the Additive Faithfulness section.
* Download and try ICML format.