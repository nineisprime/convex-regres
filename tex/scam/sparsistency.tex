\section{Analysis of Variable Selection Consistency}

We divide our analysis into two parts. We first establish a sufficient deterministic condition for sparsistency.  We then consider the
stochastic setting and argue that the deterministic conditions hold with high probability. In all of our results and analysis, we let $c, C$ represent absolute constants; the actual values of $c,C$ could change from line to line.

\subsection{Deterministic Setting}

We construct an additive convex solution $\{\hat{d}_k\}_{k=1,...,p}$ that is zero for $k \in S^c$ and show that it satisfies the KKT conditions for optimality of optimization~\ref{opt:alternate_opt}. We define $\hat{d}_k$ for $k \in S$ to be a solution to the restricted regression (defined below). We will then also show that $\hat{c}_k = 0$ satisfies the optimality condition of optimization~\ref{opt:alternate_opt_concave} for all $k \in S^c$.

\begin{definition}
\label{def:restricted_regression}
We define as \emph{restricted regression} where we restrict the indices $k$ in
optimization (\ref{opt:alternate_opt}) to lie in the set $S$ instead of ranging from $1,...,p$:
\[
\min_{d_k} \frac{1}{n} \Big\| Y - \sum_{k \in S} \bar{\Delta}_k d_k \Big\|_2^2 + 
   \lambda_n \sum_{k \in S} \| \bar{\Delta}_k d_k \|_\infty \quad \trm{such that} \, d_{k,1}, ..., d_{k,n-1} \geq 0
\]
\end{definition}

\begin{theorem} (Deterministic setting)
\label{thm:deterministic}
Let $\{\hat{d}_k \}_{k \in S}$ be a minimizer of the restricted regression as defined above.
Let $\hat{r} \coloneqq Y - \sum_{k \in S} \bar{\Delta}_k \hat{d}_k$ be the restricted regression residual. \\

Suppose for all $k\in S^c$, for all $i=1,...,n$, $\lambda_n > | \frac{1}{2n}
\hat{r}^\tran \mathbf{1}_{(i:n)}|$ where $\mathbf{1}_{(i:n)}$ is 1 on the coordinates of the $i$-th largest to the $n$-th largest entries of $X_k$ and 0 elsewhere.\\

Then the following are true:
\begin{enumerate}
\item Let $\hat{d}_k = 0$ for $k \in S^c$, then \{$\hat{d}_k\}_{k=1,...,p}$ is an optimal solution to optimization~\ref{opt:alternate_opt}. Furthermore, any solution to the optimization program \ref{opt:alternate_opt} must be zero on $S^c$.
\item For all $k \in S^c$, the solution $\hat{c}_k$ to optimization~\ref{opt:alternate_opt_concave} must be 0 and must be unique.
\end{enumerate}

\end{theorem}

This result holds regardless of whether we impose the Lipschitz conditions in optimization~\ref{opt:alternate_opt} and~\ref{opt:alternate_opt_concave} or not.
The full proof of Theorem~\ref{thm:deterministic} is in Section~\ref{sec:deterministic_proof} of the Appendix.\\

Theorem~\ref{thm:deterministic} allows us to analyze false negative
rates and false positive rates separately. To control false positives,
we analyze when the condition on $\lambda_n$ is fulfilled for all $j$ and all $k \in S^c$. To control false negatives, we analyze the restricted regression. \\

The proof of theorem~\ref{thm:deterministic} looks at KKT conditions of optimization~\ref{opt:alternate_opt}, similar to the now standard \emph{primal-dual witness} technique~\cite{wainwright2009sharp}. We cannot derive analogous \emph{mutual incoherence} conditions because the estimation is nonparametric -- even the low dimensional restricted regression has $s(n-1)$ variables. The details of the proof are in section~\ref{sec:deterministic_proof} of the Appendix.

\subsection{Probabilistic Setting}

We use the following statistical setting:

\begin{packed_enum}
\item Let $F$ be a distribution supported and positive on $\mathcal{X}=[-b,b]^p$. Let $X^{(1)},..., X^{(n)} \sim F$ be iid.
\item Let $Y = f_0(X) + w$ where $w$ is zero-mean noise. Let $Y^{(1)},...,Y^{(n)}$ be iid.
\item Let $S_0 = \{1,...,s_0\}$ denote the relevant variables where $s_0 \leq p$, i.e.,
  $f_0(X) = f_0(X_{S_0})$.
\end{packed_enum}


Let $\mathcal{C}^1$ denote the set of univariate convex functions supported on $[-b,b]$. Define $\mathcal{C}^{p}$ as the set of convex additive function
$\mathcal{C}^p \equiv \{ f \,:\, f = \sum_{k=1}^p f_k, \,
   f_k \in \mathcal{C}^1 \} $
Let $f^*(x) = \sum_{k=1}^p f^*_k(x_k)$ be the population risk minimizer:
$f^* \equiv \arg\min_{f \in \mathcal{C}^p} \E\big(f_0(X) - f^*(X) \big)^2$.\\

Similarly, we define $\mh \mathcal{C}^1$ as the set of univariate concave functions supported on $[-b, b]$ and let 
$g^*_k = \arg\min_{g_k \in \mh \mathcal{C}^1} \E \big( f_0(X) - f^*(X) - g_k(X_k) \big)^2$. \\

We let $S = \{ k = 1,...,p \,:\, f^*_k \neq 0 \trm{ or } g^*_k \neq 0\}$. By additive faithfulness (theorem~\ref{thm:acdc_faithful}), it must be that $S_0 \subset S$. 


Each of our theorems will use a subset of the following assumptions:
\begin{packed_enum}
\item[A1:] $X_S, X_{S^c}$ are independent. 
\item[A2:] $f_0$ is convex and twice-differentiable. 
\item[A3:] $|\partial_{x_k} f_0 | \leq L$ for all $k$
\item[A4:] $w$ is mean-zero subgaussian, independent of $X$, with subgaussian scale $\sigma$, i.e. for all $t \in \R$, $\E e^{t \epsilon} \leq e^{\sigma^2 t^2 / 2}$.
\end{packed_enum}
By assumption A1, $f^*_k$ is must be zero for $k\notin \{1,...,s\}$. 


We define $\alpha_f, \alpha_g$ as a measure of the signal strength of the weakest variable:
\begin{align*}
\alpha_f &= \inf_{f \in \mathcal{C}^p \,:\, \exists k ,\, f^*_k \neq 0 \,\wedge\, f_k = 0} 
       \Big\{ \mathbb{E} \big( f_0(X) - f(X) \big)^2 - 
        \mathbb{E} \big( f_0(X) - f^*(X) \big)^2  \Big\}\\
\alpha_g &=   \min_{k \in S \,:\, g^*_k \neq 0}
      \Big\{ \mathbb{E} \big( f_0(X) - f^*(X) \big)^2 - 
    \mathbb{E} \big( f_0(X) - f^*(X) - g^*_k(X_k) \big)^2 \Big\}
\end{align*}

Intuitively, if $\alpha_f$ is smaller, then it is easier to make a false omission in the additive convex stage of the procedure. If $\alpha_g$ is smaller, then it is easier to make a false omission in the decoupled concave stage of the procedure.

\begin{remark}
  We make strong assumptions on the covariates in A1 in order to make
  very weak assumptions on the true regression function $f_0$ in
  A2. In particular, we do not assume that $f_0$ is additive. 
  %Strong assumptions on the covariates are not uncommon in nonparametric
  %variable selection analysis~\cite{lafferty2008rodeo}. 
  %[TODO: refer to correlated design experiment].
\end{remark}


\begin{theorem} (Controlling false positives) \\
\label{thm:false_positive}
Suppose assumptions A1-A4 hold. 

Suppose $\lambda_n \geq c s Lb \sigma  \sqrt{ \frac{1}{n} \log^2 np}$, then with probability at least $ 1 - \frac{C}{n}$, for all $k \in S^c$, and for all $i'=1,...,n$:
\[
\lambda_n > \Big| \frac{1}{2n}\hat{r}^\tran \mathbf{1}_{(i':n)} \Big|
\]

And thereforeï¼Œfor all $k \in S^c$, both the AC solution $\hat{f}_k$, from optimization~\ref{opt:alternate_opt}, and the DC solution $\hat{g}_k$, from optimization~\ref{opt:alternate_opt_concave} are zero. \\
\end{theorem}

The proof of Theorem~\ref{thm:false_positive} exploits independence of
$\hat{r}$ and $X_k$ from A1; when $\hat{r}$ and $X_k$ are independent, $\hat{r}^\tran \mathbf{1}_{(i':n)}$ is the sum of $n - i' +1$ random coordinates of $\hat{r}$.  We can then uses the concentration of measure result for sampling without replacement to argue that $| \frac{1}{n} \hat{r}^\tran\mathbf{1}_{(i':n)}|$ is small with high probability. The full proof of
Theorem~\ref{thm:false_positive} is in
Section~\ref{sec:false_positive_proof} of the Appendix.

\begin{theorem} (Controlling false negatives) \\
\label{thm:false_negative}
Suppose assumptions A1-A4 hold. Let $\hat{f}$ be any AC solution to the restricted regression with $L$-Lipschitz constraint and let $\hat{g}_k$'s be any DC solution to the restricted regression with $L$-Lipschitz constraint.\\

Suppose $\lambda \leq c sLb  \sqrt{\frac{1}{n} \log^2 np}$ and suppose $n$ is large enough such that $(Lb)^3 \sigma \sqrt{\frac{s^5}{n^{4/5}} \log^2 np} \leq 1$.\\

Suppose 
$\frac{\alpha_f}{\sigma} \geq c (Lb)^3 \sqrt{\frac{s^5}{n^{4/5}} \log^2 np}$ and $\frac{\alpha_g^2}{\sigma} \geq c (Lb)^3 \sqrt{\frac{s^5}{n^{4/5}} \log^2 2np}$.\\

Then, with probability at least $1 - \frac{1}{n}$, we have that for all $k \in S$, $\hat{f}_k \neq 0$ or $\hat{g}_k \neq 0$.

\end{theorem}

This is a finite sample version of
Theorem~\ref{thm:convex_faithful}. We need stronger assumptions in
Theorem~\ref{thm:false_negative} to use our additive faithfulness
result, Theorem~\ref{thm:convex_faithful}. We also include an extra
Lipschitz constraint so that we can use existing covering number
results \cite{Bronshtein:76}. Recent work
\cite{Guntu:13} shows that the Lipschitz constraint
is not required with more advanced empirical process theory
techniques; we leave the incorporation of this development as future work. 
We give the full proof of Theorem~\ref{thm:false_negative}
in Section~\ref{sec:false_negative_proof} of the Appendix.

Combining Theorem~\ref{thm:false_positive} and
~\ref{thm:false_negative} 
we have the following result.
\begin{corollary}
Suppose assumptions A1-A4 hold. Let $\hat{f}$ be any AC solution and $\hat{g}_k$'s be any DC solution, both with $L$-Lipschitz constraints, both with 
$\lambda = \Theta \Big( sLb \sqrt{\frac{1}{n} \log^2 np} \Big)$. \\


Suppose 
$\frac{\alpha_f}{\sigma} \geq c (Lb)^3 \sqrt{\frac{s^5}{n^{4/5}} \log^2 np}$ and $\frac{\alpha_g^2}{\sigma} \geq c (Lb)^3 \sqrt{\frac{s^5}{n^{4/5}} \log^2 2np}$.\\

Then, for all large enough $n$, we have that with probability at least $1-\frac{1}{n}$:
\begin{align*}
\hat{f}_k \neq 0 \trm{ or } \hat{g}_k \neq 0 &\trm{ for all } k \in S\\
\hat{f}_k = 0 \trm{ and } \hat{g}_k = 0 & \trm{ for all } k \notin S
\end{align*}



\end{corollary}
The above corollary implies that sparsistency is achievable at the same exponential scaling of the ambient dimension $p = O(\exp(n^c)), c<1$ rate as parametric models. The cost of nonparametric modeling is reflected in the scaling with respect to $s$, which can only scale at $o(n^{4/25})$.

\begin{remark}
\citet{dalalyan:12} have shown that under tradtional smoothness constraints, variable selection is achievable only if $n > O(e^s)$. It is interesting to observe that because of additive faithfulness, the convexity assumption enables a much better scaling of $n = O(\textrm{poly}(s))$, demonstrating that geometric constraints can be quite different from the previously studied smoothness conditions.
\end{remark}

%\textbf{Comparison with Related Work.} 


% DO NOT CHANGE; RefTex variables -minx
 
%%% Local Variables: ***
%%% mode:latex ***
%%% TeX-master: "paper.tex" ***
%%% End: ***
