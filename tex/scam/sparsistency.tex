\section{Analysis of Variable Selection Consistency}
\label{sec:finitesample}

We divide our analysis into two parts. We first establish a sufficient
deterministic condition for consistency of the sparsity pattern
selection procedure---a property sometimes called \textit{sparsistency}.
We then consider the
stochastic setting and argue that the deterministic conditions hold
with high probability. Note that in all of our results and analysis, we let $c,
C$ represent absolute constants; the actual values of $c,C$ may change from line to line.

\subsection{Deterministic Setting}

We construct an additive convex solution $\{\hat{d}_k\}_{k=1,\ldots,p}$
that is zero for $k \in S^c$, where $S$ is the set of relevant
variables, and show that it satisfies the KKT
conditions for optimality of optimization~\eqref{opt:alternate_opt}. We
define $\hat{d}_k$ for $k \in S$ to be a solution to the restricted
regression (defined below). We also show that $\hat{c}_k =
0$ satisfies the optimality condition of
optimization~\eqref{opt:alternate_opt_concave} for all $k \in S^c$.

\begin{definition}
\label{def:restricted_regression}
We define the \emph{restricted regression} problem 
\[
\min_{d_k} \frac{1}{n} \Big\| Y - \sum_{k \in S} \bar{\Delta}_k d_k \Big\|_2^2 + 
   \lambda_n \sum_{k \in S} \| \bar{\Delta}_k d_k \|_\infty \quad \trm{such that} \, d_{k,1}, \ldots, d_{k,n-1} \geq 0
\]
where we restrict the indices $k$ in
optimization \eqref{opt:alternate_opt} to lie in the set $S$ of true
relevant variables.
\end{definition}

\begin{theorem}[Deterministic setting]
\label{thm:deterministic}
Let $\{\hat{d}_k \}_{k \in S}$ be a minimizer of the restricted regression as defined above.
Let $\hat{r} \coloneqq Y - \sum_{k \in S} \bar{\Delta}_k \hat{d}_k$ be the restricted regression residual. 
Suppose for all $k\in S^c$, for all $i=1,\ldots,n$, $\lambda_n > | \frac{1}{2n}
\hat{r}^\tran \mathbf{1}_{(i:n)}|$ where $\mathbf{1}_{(i:n)}$ is 1 on
the coordinates of the 
$i$-th largest to the $n$-th largest entries of $X_k$ and 0
elsewhere.  Then the following two statements hold.
\begin{enumerate}
\item Let $\hat{d}_k = 0$ for $k \in S^c$.  Then
  \{$\hat{d}_k\}_{k=1,\ldots,p}$ is an optimal solution to
  optimization~\eqref{opt:alternate_opt}. Furthermore, any solution to
  the optimization program \eqref{opt:alternate_opt} must be zero on
  $S^c$.
\item For all $k \in S^c$, the solution $\hat{c}_k$ to optimization~\eqref{opt:alternate_opt_concave} must be zero.
\end{enumerate}

\end{theorem}

This result holds regardless of whether or not we impose the boundedness conditions in optimization~\eqref{opt:alternate_opt} and~\eqref{opt:alternate_opt_concave}.
The full proof of Theorem~\ref{thm:deterministic} is in Section~\ref{sec:deterministic_proof} of the Appendix.

Theorem~\ref{thm:deterministic} allows us to separately analyze the false negative
rates and false positive rates. To control false positives,
we analyze the condition on $\lambda_n$ for $k \in S^c$. To control
false negatives, we analyze the restricted regression. 

The proof of Theorem~\ref{thm:deterministic} analyses the KKT
conditions of optimization~\eqref{opt:alternate_opt}.  This parallels
the now standard \emph{primal-dual witness}
technique~\citep{wainwright2009sharp}. However, we cannot derive analogous
\emph{mutual incoherence} conditions because the estimation is
nonparametric---even the low dimensional restricted regression has
$s(n-1)$ variables. The details of the proof are given in
Section~\ref{sec:deterministic_proof} of the Appendix.

\subsection{Probabilistic Setting}

In the probabilistic setting we treat the covariates as random.  We
adopt the following standard setup:

\begin{enumerate}
\item The data $X^{(1)},\ldots, X^{(n)} \sim F$ are iid from
a distribution $F$ that is supported and strictly positive on $\mathcal{X}=[-b,b]^p$. 
\item The response is $Y = f_0(X) + W$ where $W$ is
  independent, zero-mean noise; thus $Y^{(i)} = f_0(X^{(i)}) + W^{(i)}$.
\item The regression function $f_0$ satisfies
$f_0(X) = f_0(X_{S_0})$, where $S_0 = \{1,\ldots,s_0\}$ is the set of
relevant variables.
\end{enumerate}


Let $\mathcal{C}^1$ denote the set of univariate convex functions
supported on $[-b,b]$, 
and let  $\mathcal{C}_1^{p}$ denote the set of convex additive functions
$\mathcal{C}_1^p \equiv \{ f \,:\, f = \sum_{k=1}^p f_k, \,
   f_k \in \mathcal{C}^1 \} $.  
Let $f^*(x) = \sum_{k=1}^p f^*_k(x_k)$ be the population risk
minimizer in $\mathcal{C}_1^p$, 
\begin{equation}
f^* \equiv \arg\min_{f \in \mathcal{C}_1^p} \E\big(f_0(X) - f^*(X)
\big)^2.
\end{equation}
Similarly, we define $\mh \mathcal{C}^1$ as the set of univariate concave functions supported on $[-b, b]$ and define
\begin{equation}
g^*_k = \arg\min_{g_k \in \mh \mathcal{C}^1} \E \big( f_0(X) - f^*(X)
- g_k(X_k) \big)^2.
\end{equation}
We let $S = \{ k = 1,\ldots,p \,:\, f^*_k \neq 0 \trm{ or } g^*_k \neq 0\}$. By additive faithfulness (Theorem~\ref{thm:acdc_faithful}), it must be that $S_0 \subset S$. 
Each of our theorems will use a subset of the following assumptions:
\begin{packed_enum}
\item[A1:] $X_S, X_{S^c}$ are independent. 
\item[A2:] $f_0$ is convex and twice-differentiable. 
\item[A3:] $\|f_0\|_\infty \leq sB$ and $\| f^*_k \|_\infty \leq B$ for all $k$.
\item[A4:] $W$ is mean-zero sub-Gaussian, independent of $X$, with scale $\sigma$; i.e., for all $t \in \R$, $\E e^{t \epsilon} \leq e^{\sigma^2 t^2 / 2}$.
\end{packed_enum}
By assumption A1, $f^*_k$ is must be zero for $k\notin S$.
We define $\alpha_{+}, \alpha_{-}$ as a measure of the signal strength of the weakest variable:
\begin{align*}
\alpha_{+} &= \inf_{f \in \mathcal{C}_1^p \,:\, \exists k ,\, f^*_k \neq 0 \,\wedge\, f_k = 0} 
       \Big\{ \mathbb{E} \big( f_0(X) - f(X) \big)^2 - 
        \mathbb{E} \big( f_0(X) - f^*(X) \big)^2  \Big\}\\
\alpha_{-} &=   \min_{k \in S \,:\, g^*_k \neq 0}
      \Big\{ \mathbb{E} \big( f_0(X) - f^*(X) \big)^2 - 
    \mathbb{E} \big( f_0(X) - f^*(X) - g^*_k(X_k) \big)^2 \Big\}
\end{align*}
These quantities play the role of the absolute value of the smallest nonzero
coefficient in the true linear model in lasso theory.  Intuitively, if
$\alpha_{+}$ is small, then it is easier to make a false omission in the
additive convex stage of the procedure. If $\alpha_{-}$ is small, then
it is easier to make a false omission in the decoupled concave stage
of the procedure.

\begin{remark}
  We make strong assumptions on the covariates in A1 in order to make
  very weak assumptions on the true regression function $f_0$ in
  A2. In particular, we do not assume that $f_0$ is additive. In
  important direction for future work is to weaken this assumption.
  Our simulation experiments indicate that the procedure can be
  effective even when the relevant and irrelevant variables are correlated.
  %Strong assumptions on the covariates are not uncommon in nonparametric
  %variable selection analysis~\cite{lafferty2008rodeo}. 
  %[TODO: refer to correlated design experiment].
\end{remark}


\begin{theorem}[Controlling false positives]
\label{thm:false_positive}
Suppose assumptions A1-A4 hold. Define $\tilde{\sigma} \equiv \max(\sigma, B)$,
and suppose 
\begin{equation}
\lambda_n \geq 8 s_0 \tilde{\sigma}  \sqrt{ \frac{\log^2 np}{n}}.
\end{equation}  
Then with probability at least $ 1 - \frac{12}{np}$, for all $k \in
S^c$, and for all $i'=1,\ldots, n$,
\begin{equation}
\Big| \frac{1}{2n}\hat{r}^\tran \mathbf{1}_{(i':n)_k} \Big| < \lambda_n
\end{equation}
and for all $k \in S^c$, both the AC solution $\hat{f}_k$ from optimization~\eqref{opt:alternate_opt} and the DC solution $\hat{g}_k$ from optimization~\eqref{opt:alternate_opt_concave} are zero. 
\end{theorem}

The proof of Theorem~\ref{thm:false_positive} exploits independence of
$\hat{r}$ and $X_k$ from A1; when $\hat{r}$ and $X_k$ are independent,
$\hat{r}^\tran \mathbf{1}_{(i':n)}$ is the sum of $n - i' +1$ random
coordinates of $\hat{r}$.  We can then use concentration of
measure results for sampling without replacement to argue that $|
\frac{1}{n} \hat{r}^\tran\mathbf{1}_{(i':n)}|$ is small with high
probability. The result of Theorem~\ref{thm:deterministic} is then
used. The full proof of Theorem~\ref{thm:false_positive} is in
Section~\ref{sec:false_positive_proof} of the Appendix.

\begin{theorem}[Controlling false negatives]
\label{thm:false_negative}
Suppose assumptions A1-A4 hold. Let $\hat{f}$ be any AC solution to
the restricted regression with $B$-boundedness constraint, and let
$\hat{g}_k$ be any DC solution to the restricted regression with
$B$-boundedness constraint. Let $\tilde{\sigma}$ denote $\max(\sigma,
B)$.  Suppose 
\begin{equation}
\lambda_n \leq 9 s_0 \tilde{\sigma} \sqrt{\frac{1}{n} \log^2 np}
\end{equation}
and that $n$ is sufficiently large so that 
\begin{equation}
\frac{n^{4/5}}{\log np} \geq B^4 \tilde{\sigma}^2 s_0^5.
\end{equation}
Assume that the signal-to-noise ratio satisfies
\begin{align}
\frac{\alpha_{+}}{\tilde{\sigma}} & \geq c B^2
\sqrt{\frac{s_0^5}{n^{4/5}} \log^2 np}\\
\frac{\alpha_{-}^2}{\tilde{\sigma}} &\geq c B^2
\sqrt{\frac{s_0^5}{n^{4/5}} \log^2 np}
\end{align}
where $c$ is a constant.  Then with probability at least $1 -
\frac{C}{n}$ for some constant $C$, 
$\hat{f}_k \neq 0$ or $\hat{g}_k \neq 0$ 
for all $k \in S$.
\end{theorem}

This is a finite sample version of
Theorem~\ref{thm:convex_faithful}. We need stronger assumptions in
Theorem~\ref{thm:false_negative} to use our additive faithfulness
result, Theorem~\ref{thm:convex_faithful}. We also include an extra
boundedness constraint so that we can use recent bracketing number
results \cite{kim2014global}.

Combining Theorems~\ref{thm:false_positive} and
~\ref{thm:false_negative} 
we obtain the following result.
\begin{corollary}
  Suppose the assumptions of Theorem~\ref{thm:false_positive} and
  Theorem~\ref{thm:false_negative} hold.  
Then with probability at least $1-\frac{C}{n}$
\begin{align}
\hat{f}_k \neq 0 \trm{ or }\; \hat{g}_k \neq 0 &\trm{ for all } k \in S\\
\hat{f}_k = 0 \trm{ and }\; \hat{g}_k = 0 & \trm{ for all } k \notin S
\end{align}
for some constant $C$.

\end{corollary}
The above corollary implies that sparsistency is achievable at the
same exponential scaling of the ambient dimension scaling attained
with parametric models, $p = O(\exp(n^c))$ for $c<1$.  The cost of
nonparametric modeling through shape constraints 
is reflected in the scaling with respect to
the number of relevant variables, which can scale as $s_0 = o(n^{4/25})$.

\begin{remark}
  \citet{dalalyan:12} have shown that under tradtional smoothness
  constraints, even with a product distribution, variable selection is
  achievable only if $n > O(e^{s_0})$. It is interesting to observe that
  because of additive faithfulness, the convexity assumption enables a
  much better scaling of $n = O(\textrm{poly}(s_0))$, demonstrating that
  geometric constraints can be quite different from the previously
  studied smoothness conditions.
\end{remark}

%\textbf{Comparison with Related Work.} 


% DO NOT CHANGE; RefTex variables -minx
 
%%% Local Variables: ***
%%% mode:latex ***
%%% TeX-master: "paper.tex" ***
%%% End: ***
