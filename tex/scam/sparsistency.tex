\section{Analysis of Variable Screening Consistency}
\label{sec:finitesample}

Our goal is to show that variable screening consistency. That is, as $n,p \rightarrow \infty$, $\mathbb{P}( \hat{S} = S)$ approaches 0 where $\hat{S}$ is the set of variables outputted by AC/DC in the finite sample setting (Algorithm~\ref{fig:backfitting:algo}) and $S$ is the set of variables outputted in the population setting~\eqref{eqn:acdc_vars_pop}.

We divide our analysis into two parts. We first establish a sufficient
deterministic condition for consistency of the sparsity pattern
screening procedure.  We then consider the stochastic setting and argue that the 
deterministic conditions hold with high probability. Note that in all of our results 
and analysis, we let $c,
C$ represent absolute constants; the actual values of $c,C$ may change from line to line. We derived two equivalent optimizations for AC/DC: \eqref{np} outputs $\hat{f}_k, \hat{g}_k$ and \eqref{opt:alternate_opt} outputs the second derivatives $\hat{d}_k$. Their equivalence is established in Proposition~\ref{prop:alt_opt_form} and we use both $\hat{d}_k$ and $\hat{f}_k$ in our analysis.

In our analysis, we assume that an upper bound $B$ to $\| \hat{f}_k \|_\infty$ 
is imposed in the optimization procedure, where $B$ is chosen to also upper bound $\| f^*_k \|_\infty$. This $B$-boundedness constraint is added so that we may use the convex function bracketing results from~\cite{kim2014global} to establish uniform convergence between the empirical risk and the population risk. We emphasize that this constraint is not needed in practice and we do not use it for any of our simulations. 


\subsection{Deterministic Setting}

We analyze Optimization~\ref{opt:alternate_opt} and construct an additive convex solution $\{\hat{d}_k\}_{k=1,\ldots,p}$
that is zero for $k \in S^c$, where $S$ is the set of relevant
variables, and show that it satisfies the KKT
conditions for optimality of optimization~\eqref{opt:alternate_opt}. We
define $\hat{d}_k$ for $k \in S$ to be a solution to the restricted
regression (defined below). We also show that $\hat{c}_k =
0$ satisfies the optimality condition of
optimization~\eqref{opt:alternate_opt_concave} for all $k \in S^c$.

\begin{definition}
\label{def:restricted_regression}
We define the \emph{restricted regression} problem 
\[
\min_{d_k} \frac{1}{n} \Big\| Y - \sum_{k \in S} \bar{\Delta}_k d_k \Big\|_2^2 + 
   \lambda_n \sum_{k \in S} \| \bar{\Delta}_k d_k \|_\infty \quad \trm{such that} \, d_{k,1}, \ldots, d_{k,n-1} \geq 0
\]
where we restrict the indices $k$ in
optimization \eqref{opt:alternate_opt} to lie in some set $S$ which contains the true
relevant variables.
\end{definition}



\begin{theorem}[Deterministic setting]
\label{thm:deterministic}
Let $\{\hat{d}_k \}_{k \in S}$ be a minimizer of the restricted regression as defined above.
Let $\hat{r} \coloneqq Y - \sum_{k \in S} \bar{\Delta}_k \hat{d}_k$ be the restricted regression residual. 

Let $\pi_k(i)$ be a reordering of $X_k$ in ascending order so that $X_{k \pi_k(n)}$ is the largest entry. Let $\mathbf{1}_{\pi_k(i:n)}$ be 1 on the coordinates $\pi_k(i),\pi_k(i+1),...,\pi_k(n)$ and 0 elsewhere. Define $\mathsf{range}_k = X_{k\pi_k(n)} - X_{k\pi_k(1)}$.

Suppose for all $k\in S^c$, for all $i=1,\ldots,n$, $\lambda_n > \mathsf{range}_k| \frac{32}{n} \hat{r}^\tran \mathbf{1}_{\pi_k(i:n)}|$. Suppose also that for all $k \in S^c$, $\max_{i=1,...,n-1} \frac{X_{k\pi_k(i+1)} - X_{k\pi_k(i)}}{\mathsf{range}_k} \geq \frac{1}{16}$, and $\mathsf{range}_k \geq 1$.

Then the following two statements hold.
\begin{enumerate}
\item Let $\hat{d}_k = 0$ for $k \in S^c$.  Then
  \{$\hat{d}_k\}_{k=1,\ldots,p}$ is an optimal solution to
  optimization~\eqref{opt:alternate_opt}. Furthermore, any solution to
  the optimization program \eqref{opt:alternate_opt} must be zero on
  $S^c$.
\item For all $k \in S^c$, the solution $\hat{c}_k$ to optimization~\eqref{opt:alternate_opt_concave} must be zero and unique.
\end{enumerate}

\end{theorem}

Theorem~\ref{thm:deterministic} states that the estimator produces no false positive so long as $\lambda_n$ upper bounds the partial sums of the residual $\hat{r}$ and that the maximum gap between ordered values of $X_k$ is small. 

This result holds regardless of whether or not we impose the boundedness conditions in optimization~\eqref{opt:alternate_opt} and~\eqref{opt:alternate_opt_concave}.
The full proof of Theorem~\ref{thm:deterministic} is in Section~\ref{sec:deterministic_proof} of the Appendix. We allow $S$ in Theorem~\ref{thm:deterministic} to be any set containing the relevant variables; in Lasso analysis, $S$ is taken to be the set of relevant variables; we will take $S$ to be the set of variables chosen by the additive convex and decoupled concave procedure in the population setting, which is guaranteed to contain the relevant variables because of additive faithfulness.

Theorem~\ref{thm:deterministic} allows us to separately analyze the false negative
rates and false positive rates. To control false positives,
we analyze the condition on $\lambda_n$ for $k \in S^c$. To control
false negatives, we need only analyze the restricted regression with only $|S|$ variables.

The proof of Theorem~\ref{thm:deterministic} analyses the KKT
conditions of optimization~\eqref{opt:alternate_opt}.  This parallels
the now standard \emph{primal-dual witness}
technique~\citep{wainwright2009sharp}. However, we cannot derive analogous
\emph{mutual incoherence} conditions because the estimation is
nonparametric---even the low dimensional restricted regression has
$s(n-1)$ variables. The details of the proof are given in
Section~\ref{sec:deterministic_proof} of the Appendix.

\subsection{Probabilistic Setting}

In the probabilistic setting we treat the covariates as random.  We
adopt the following standard setup:

\begin{enumerate}
\item The data $X^{(1)},\ldots, X^{(n)} \sim F$ are iid from
a distribution $F$ with a density $p(\mathbf{x})$ that is supported and strictly positive on $\mathcal{X}=[-1,1]^p$.
\item The response is $Y = f_0(X) + W$ where $W$ is
  independent, zero-mean noise; thus $Y^{(i)} = f_0(X^{(i)}) + W^{(i)}$.
\item The regression function $f_0$ satisfies
$f_0(X) = f_0(X_{S_0})$, where $S_0 = \{1,\ldots,s_0\}$ is the set of
relevant variables.
\end{enumerate}


Let $\mathcal{C}^1$ denote the set of univariate convex functions
supported on $[-1,1]$, 
and let  $\mathcal{C}_1^{p}$ denote the set of convex additive functions
$\mathcal{C}_1^p \equiv \{ f \,:\, f = \sum_{k=1}^p f_k, \,
   f_k \in \mathcal{C}^1 \} $.  
Let $f^*(\mathbf{x}) = \sum_{k=1}^p f^*_k(x_k)$ be the population risk
minimizer in $\mathcal{C}_1^p$, 
\begin{equation}
f^* = \arg\min_{f \in \mathcal{C}_1^p} \E\big(f_0(X) - f^*(X)
\big)^2.
\end{equation}
$f^*$ is the unique minimizer by Theorem~\ref{thm:acdc_faithful}. Similarly, we define $\mh \mathcal{C}^1$ as the set of univariate concave functions supported on $[-1, 1]$ and define
\begin{equation}
g^*_k = \arg\min_{g_k \in \mh \mathcal{C}^1} \E \big( f_0(X) - f^*(X)
- g_k(X_k) \big)^2.
\end{equation}
The $\hat{g}_k$'s are unique minimizers as well. We let $S = \{ k = 1,\ldots,p \,:\, f^*_k \neq 0 \trm{ or } g^*_k \neq 0\}$ and let $s = |S|$. By additive faithfulness (Theorem~\ref{thm:acdc_faithful}), it must be that $S_0 \subset S$ and thus $s \geq s_0$. In some cases, such as when $X_{S_0}, X_{S^c_0}$ are independent, we have $S = S_0$.
Each of our theorems will use a subset of the following assumptions:
\begin{packed_enum}
\item[A1:] $X_S, X_{S^c}$ are independent. 
\item[A2:] $f_0$ is convex and twice-differentiable. 
\item[A3:] $\|f_0\|_\infty \leq sB$ and $\| f^*_k \|_\infty \leq B$ for all $k$.
\item[A4:] $W$ is mean-zero sub-Gaussian, independent of $X$, with scale $\sigma$; i.e., for all $t \in \R$, $\E e^{t \epsilon} \leq e^{\sigma^2 t^2 / 2}$.
\item[A5:] The density $p(\mathbf{x})$ is bounded away from 0 and satisfies the boundary flatness condition. (Definition~\ref{defn:boundary-point})
\end{packed_enum}
By assumption A1, $f^*_k$ must be zero for $k\notin S$.
We define $\alpha_{+}, \alpha_{-}$ as a measure of the signal strength of the weakest variable:
\begin{align}
\alpha_{+} &= \inf_{f \in \mathcal{C}_1^p \,:\, \textrm{supp}(f) \subsetneq \textrm{supp}(f^*)} 
       \Big\{ \mathbb{E} \big( f_0(X) - f(X) \big)^2 - 
        \mathbb{E} \big( f_0(X) - f^*(X) \big)^2  \Big\} \label{eqn:signal_level_defn} \\
\alpha_{-} &=   \min_{k \in S \,:\, g^*_k \neq 0}
      \Big\{ \mathbb{E} \big( f_0(X) - f^*(X) \big)^2 - 
    \mathbb{E} \big( f_0(X) - f^*(X) - g^*_k(X_k) \big)^2 \Big\} \nonumber
\end{align}
$\alpha_+$ is a lower bound on the excess risk incurred by any additive convex function whose support is strictly smaller than $f^*$; $\alpha_+ > 0$ since $f^*$ is the unique risk minimizer. Likewise, $\alpha_-$ lower bounds the excess risk of any decoupled concave fit of the residual $f_0 - f^*$ that is strictly more sparse than the optimal decoupled concave fit $\{\hat{g}_k^*\}$; $\alpha_- > 0$ by the uniqueness of $\{g^*_k\}$ as well. These quantities play the role of the absolute value of the smallest nonzero coefficient in the true linear model in lasso theory.  Intuitively, if
$\alpha_{+}$ is small, then it is easier to make a false omission in the
additive convex stage of the procedure. If $\alpha_{-}$ is small, then
it is easier to make a false omission in the decoupled concave stage
of the procedure.

\begin{remark}
  We make strong assumptions on the covariates in A1 in order to make
  weak assumptions on the true regression function $f_0$ in
  A2. In particular, we do not assume that $f_0$ is additive. In
  important direction for future work is to weaken assumption A1.
  Our simulation experiments indicate that the procedure can be
  effective even when the relevant and irrelevant variables are correlated.
  %Strong assumptions on the covariates are not uncommon in nonparametric
  %variable selection analysis~\cite{lafferty2008rodeo}. 
  %[TODO: refer to correlated design experiment].
\end{remark}


\begin{theorem}[Controlling false positives]
\label{thm:false_positive}
Suppose assumptions A1-A5 hold. Define $\tilde{\sigma} \equiv \max(\sigma, B)$ and define $\mathsf{range}_k = X_{k \pi_k(n)} - X_{k \pi_k(1)}$. Suppose $p \leq O\big( \exp(c n) \big)$ and $n \geq C$ for some positive constants $c, C$ dependent on $b$.  Suppose also
\begin{equation}
\lambda_n \geq 512 s \tilde{\sigma}  \sqrt{ \frac{\log^2 np}{n}}.
\end{equation}  
Then with probability at least $ 1 - \frac{24}{n}$, for all $k \in
S^c$, for all $i=1,\ldots,n$,
\begin{equation}
\lambda_n \geq \mathsf{range}_k \Big| \frac{32}{n}\hat{r}^\tran \mathbf{1}_{(i':n)_k} \Big| ,
\end{equation}
$\max_{i'} \frac{X_{k\pi_k(i'+1)} - X_{k \pi_k(i')}}{\mathsf{range}_k} \leq \frac{1}{16}$, $\mathsf{range}_k \geq 1$, and both the AC solution $\hat{f}_k$ from optimization~\eqref{opt:alternate_opt} and the DC solution $\hat{g}_k$ from optimization~\eqref{opt:alternate_opt_concave} are zero. 
\end{theorem}

The proof of Theorem~\ref{thm:false_positive} exploits independence of
$\hat{r}$ and $X_k$ under assumption A1; when $\hat{r}$ and $X_k$ are independent,
$\hat{r}^\tran \mathbf{1}_{(i':n)}$ is the sum of $n - i' +1$ random
coordinates of $\hat{r}$.  We can then use concentration of
measure results for sampling without replacement to argue that $|
\frac{1}{n} \hat{r}^\tran\mathbf{1}_{(i':n)}|$ is small with high
probability. The result of Theorem~\ref{thm:deterministic} is then
used. The full proof of Theorem~\ref{thm:false_positive} is in
Section~\ref{sec:false_positive_proof} of the Appendix.

\begin{theorem}[Controlling false negatives]
\label{thm:false_negative}
Suppose assumptions A1-A5 hold. Let $\hat{f}$ be any AC solution to
the restricted regression with $B$-boundedness constraint, and let
$\hat{g}_k$ be any DC solution to the restricted regression with
$B$-boundedness constraint. Let $\tilde{\sigma}$ denote $\max(\sigma,
B)$.  Suppose 
\begin{equation}
\lambda_n \leq 512 s \tilde{\sigma} \sqrt{\frac{\log^2 np}{n}}
\end{equation}
and that $n$ is sufficiently large so that, for some constant $c' > 1$,
\begin{equation}
\frac{n^{4/5}}{\log np} \geq c' B^4 \tilde{\sigma}^2 s^5.
\end{equation}
Assume that the signal-to-noise ratio satisfies
\begin{align}
\frac{\alpha_{+}}{\tilde{\sigma}} & \geq c B^2
\sqrt{\frac{s^5}{n^{4/5}} \log^2 np}\\
\frac{\alpha_{-}^2}{\tilde{\sigma}} &\geq c B^2
\sqrt{\frac{s^5}{n^{4/5}} \log^2 np}
\end{align}
where $c$ is a constant.  Then with probability at least $1 -
\frac{C}{n}$ for some constant $C$, 
$\hat{f}_k \neq 0$ or $\hat{g}_k \neq 0$ 
for all $k \in S$.
\end{theorem}

This is a finite sample version of
Theorem~\ref{thm:convex_faithful}. We need stronger assumptions in
Theorem~\ref{thm:false_negative} to use our additive faithfulness
result, Theorem~\ref{thm:convex_faithful}. The full proof of Theorem~\ref{thm:false_negative} is in Section~\ref{sec:false_negative_proof} of the appendix.

Combining Theorems~\ref{thm:false_positive} and
~\ref{thm:false_negative} 
we obtain the following result.
\begin{corollary}
  Suppose the assumptions of Theorem~\ref{thm:false_positive} and
  Theorem~\ref{thm:false_negative} hold.  
Then with probability at least $1-\frac{C}{n}$
\begin{align}
\hat{f}_k \neq 0 \trm{ or }\; \hat{g}_k \neq 0 &\trm{ for all } k \in S\\
\hat{f}_k = 0 \trm{ and }\; \hat{g}_k = 0 & \trm{ for all } k \notin S
\end{align}
for some constant $C$.

\end{corollary}
The above corollary implies that consistent variable selection is
achievable with an exponential scaling of the ambient dimension
scaling, $p = O(\exp(n^c))$ for some $0<c<1$, just as in parametric models.
The cost of nonparametric modeling through shape constraints is
reflected in the scaling with respect to the number of relevant
variables, which can scale as $s = o(n^{4/25})$.

\begin{remark}
  \citet{dalalyan:12} have shown that under tradtional smoothness
  constraints, even with a product distribution, variable selection is
  achievable only if $n > O(e^{s_0})$. It is interesting to observe that
  because of additive faithfulness, the convexity assumption enables a
  much better scaling of $n = O(\textrm{poly}(s_0))$, demonstrating that
  geometric constraints can be quite different from the previously
  studied smoothness conditions.
\end{remark}

%\textbf{Comparison with Related Work.} 


% DO NOT CHANGE; RefTex variables -minx
 
%%% Local Variables: ***
%%% mode:latex ***
%%% TeX-master: "paper.tex" ***
%%% End: ***
