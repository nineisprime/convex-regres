\def\x{\mathbf{x}}

\section{Overview of Results}

In this section we provide a high-level description of our technical
results.  The full technical details, the precise statement of the
results, and their detailed proofs are provided in following sections.

Our main contribution is an analysis of an additive approximation for identifying
relevant variables in convex regression.  
We prove a result that shows when and how the additive approximation
can be used without introducing false negatives in the population
setting.  In addition, we develop algorithms for the efficient implementation of
the quadratic programs required by the procedure.  

We first establish some notation, to be used throughout the  paper.
If $\mathbf{x}$ is a vector, we use $\mathbf{x}_{-k}$ to denote the
vector with the $k$-th coordinate removed. If $\mathbf{v} \in \R^n$, then
$v_{(1)}$ denotes the smallest coordinate of $\mathbf{v}$ in
magnitude, and $v_{(j)}$ denotes the $j$-th smallest; $\mathbf{1}_n \in \R^n$
is the all ones vector. If $X \in \R^p$ is a random variable and $S \subset
\{1,...,p\}$, then $X_S$ is the subvector of $X$ restricted to
the coordinates in $S$. Given $n$ samples $X^{(1)},...,X^{(n)}$, we use
$\bar{X}$ to denote the sample mean. Given a random variable
$X_k$ and a scalar $x_k$, we use $\E[\,\cdot \given x_k]$ as a shorthand
for $\E[\, \cdot \given X_k = x_k]$. If we say a function is integrable, we 
mean it is Lebesgue integrable.


\subsection{Faithful screening}

The starting point for our approach is the observation that least squares
nonparametric estimation under convexity constraints is equivalent to
a finite dimensional quadratic program.  Specifically, the infinite
dimensional optimization 
\begin{align}
\begin{split}
\text{minimize} & \quad \sum_{i=1}^n (Y_i - f(\x_i))^2 \\
\text{subject to} &  \quad f:\reals^p\rightarrow\reals\ \text{is
  convex}
\end{split}
\end{align}
is equivalent to the finite dimensional quadratic
program 
\begin{align}
\label{eq:convreg}
\begin{split}
\text{minimize}_{f, \beta} & \;\; \sum_{i=1}^n (Y_i - f_i)^2 \\
\text{subject to} & \;\; f_j \geq f_i + \beta_i^T (\x_j-\x_i),\; \text{for
    all $i,j$}.
\end{split}
\end{align}
Here $f_i$ is the estimated function value $f(\x_i)$, and the vectors
$\beta_i \in \reals^d$ represent supporting hyperplanes to the
epigraph of $f$.  See \cite{Boyd04}, Section 6.5.5.
Importantly, this finite dimensional quadratic program does
not have tuning parameters for smoothing the function. 

This formulation of convex regression is subject to the curse
of dimensionality.  Moreover, attempting to select variables
by regularizing the subgradient vectors $\beta_i$ with
a group sparsity penality is not effective.  Intuitively, the reason
is that all $p$ components of the subgradient $\beta_i$
appear in every convexity constraint 
$ f_j \geq f_i + \beta_i^T (\x_j-\x_i)$; small changes to the 
subgradients may not violate the constraints.  Experimentally,
we find that regularization with a group sparsity penality
will make the subgradients of irrelevant variables 
small, but may not zero them out completely.

This motivates us to consider an additive approximation. 
%Under a convex additive model, each component of the subgradient
%appears only in the convexity constraint for
%the corresponding variable:
%\begin{equation}
%f_{ki'} \geq f_{ki} + \beta_{ki}(x_{ki'}-x_{ki}) 
%\end{equation}
%where $f_{ki} = f_{k}(x_{ki})$ and $\beta_{ki}$ is the
%subgradient at point $x_{ki}$.   
As we show, this leads to
an effective variable selection procedure.  The
shape constraints play an essential role.
For general regression, using an additive approximation for variable
selection may make errors.  In particular, the nonlinearities in the
regression function may result in an additive component being wrongly
zeroed out.  We show that this cannot happen for convex regression
under appropriate conditions.

We say that a differentiable function $f$ depends on variable $x_k$ if
$\partial_{x_k} f \neq 0$ with probability greater than zero.  An additive approximation is given by
\begin{equation}
\{f_k^*\}, \mu^* \coloneqq \argmin_{f_1,\ldots, f_p, \mu} \Bigl\{ 
             \E \Bigl( f(X) - \mu - \sum_{k=1}^p f_k(X_k)\Bigr)^2 
         \,:\, \E f_k(X_k) = 0 \Bigr\}.
\end{equation}
We say that $f$ is \textit{additively faithful} in case $f^*_k = 0$
implies that $f$ does not depend on coordinate $k$.
% We can define the support $\trm{supp}(f) \coloneqq \{ k \,:\,
% \trm{$k$ is relevant to $f$}\}$. Let $f^* = \sum_{k=1}^s$, then $f$
% is additively faith if $\trm{supp}(f) = \trm{supp}(f^*)$.
Additive faithfulness is a desirable property
since it implies that an additive approximation may allow us to 
screen out irrelevant variables.

Our first result shows that convex multivariate functions are
additively faithful
under the following assumption on the distribution of the data.
\begin{definition}
  Let $p(\mathbf{x})$ be a density supported on $[0,1]^p$. Then $p$
  satisfies the \emph{boundary flatness condition} if it satisfies
  certain regularity conditions (see the precise statement in
  Definition~\ref{defn:boundary-point}) and if for all $j$, and for
  all $\mathbf{x}_{-j}$,
\[
\frac{\partial p(\mathbf{x}_{-j} \given x_j)}{\partial x_j}  =  
\frac{\partial^2 p(\mathbf{x}_{-j} \given x_j)}{\partial x_j^2} = 0
\quad \trm{at $x_j = 0$ and $x_j = 1$}.
\]
\end{definition}

As discussed in Section~\ref{sec:additivefaithful}, this is a relatively weak condition. 
Our first result is that this condition suffices in the population
setting of convex regression.

\begin{stheorem}
  Let $p(\mathbf{x})$ be a positive density supported on $C=[0,1]^p$ that
  satisfies the boundary flatness property. If $f$ is convex with a bounded second derivative on an open set around $C$, then $f$ is additively faithful under $p$.
\end{stheorem}

% We give the full proof in Section~\ref{sec:faithful_proof} of the
% Appendix, but pause here to provide some intuition. 

Intuitively, an additive approximation zeroes out variable $k$ when, fixing $x_k$, every
``slice'' of $f$ integrates to zero. We prove this result
by showing that ``slices'' of convex
functions that integrate to zero cannot be ``glued together'' while
still maintaining convexity.

While this shows that convex functions are additively faithful, it is difficult to
estimate the optimal additive functions.  The difficulty
is that $f^*_k$ need not be
a convex function, as we show through a counterexample
in Section~\ref{sec:additivefaithful}. It may be possible to estimate $f^*_k$ with smoothing parameters, but, for the purpose of variable screening, it is sufficient in fact to approximate $f^*_k$ by a \emph{convex} additive model. 

Our next result states that a convex additive fit, combined with a series of univariate concave fits, is faithful. We abuse notation in Theorem~\ref{thm:summary_acdc_population} and let the notation $f^*_k$ represent convex additive components.
%Since the true regression function $f$ is convex, it is natural to ask
%when it is sufficient to estimate a convex additive model.  
%Unfortunately, a convex additive approximation is not generally
%faithful.  In other words, it could be that $f^*_k\equiv 0$
%even for a relevant variable $x_k$.  But our next result
%shows that this type of error can be detected by fitting 
%a \textit{concave} function to the residual.  If this
%concave function is zero, we can then safely
%mark $x_k$ as an irrelevant variable.

\begin{stheorem}
\label{thm:summary_acdc_population}
Suppose $p(\mathbf{x})$ is a positive
density on $C=[0,1]^p$ that satisfies the boundary flatness
condition. Suppose that $f$ is convex and continuously twice-differentiable on an open set around $C$
and that the derivatives $\partial_{x_k} f$, $\partial_{x_k} p( \mathbf{x}_{-k}
\given x_k )$, and $\partial_{x_k}^2 p( \mathbf{x}_{-k} \given x_k)$
are all continuous as functions on $C$.
Define
\begin{equation}
\{ f^*_k \}_{k=1}^p,\mu^* = \arg\min_{\{f_k\},\mu} \Big \{
\E\Bigl( f(X) - \mu - \sum_{k=1}^s f_k(X_k) \Bigr)^2 \,:\, f_k \in
\mathcal{C}^1, \, \E f_k(X_k) = 0 \Big \}
\end{equation} where $\mathcal{C}^1$ is the set of univariate convex
functions.  Using the $f^*_k$s above, define
\begin{equation}
g^*_k = \arg\min_{g_k} \Big\{ \E\Bigl( f(X) - \mu^* - 
\sum_{k' \neq k} f^*_{k'}(X_{k'}) - g_k(X_k) \Bigr)^2 \,:\, g_k \in \mh
\mathcal{C}^1, \E g_k(X_k) = 0 \Big\},
\end{equation}
with $\mh{}\mathcal{C}^1$ denoting the set of univariate concave
functions.  Then $f^*_k = 0$ and $g^*_k = 0$ implies that $f$ does not
depend on $x_k$, i.e., $\partial_{x_k} f(\mathbf{x}) = 0$ with
probability one.
\end{stheorem}   

This result naturally suggests a two-stage screening
procedure for variable selection. In the first stage we fit a sparse convex
additive model $\{\hat f_k\}$.  In the second stage we
fit a concave function $\hat g_k$ to the residual for each variable
having a zero convex component $\hat f_k$.  If both $\hat f_k = 0$ and
$\hat g_k = 0$, we can safely discard variable $x_k$.  
As a shorthand, we refer to this two-stage procedure as AC/DC.  In 
the AC stage we fit an additive convex model.  In the DC 
stage we fit decoupled concave functions on the residuals.  The
decoupled nature of the DC stage allows all of the fits to
be carried out in parallel. The entire process involves no smoothing parameters.
Our next result concerns the required optimizations, and their finite
sample statistical performance.


\subsection{Optimization}

Given samples $(y_i, X_i)$, AC/DC becomes the following optimization:
\begin{align*}
\{\hat{f}_k\}_{k=1}^p &= \arg\min_{\{f_k \in \mathcal{C}^1\}} \frac{1}{n}\sum_{i=1}^n \left(
        y_i - \bar{y} - \sum_{k=1}^p f_k(X_{ik}) \right)^2 + \lambda \sum_{k=1}^p \| f_k \|_\infty \\
\forall k, \, \hat{g}_k &= \arg\min_{g_k \in \mathcal{C}^1} \frac{1}{n} \sum_{i=1}^n \left(
    y_i - \bar{y} - \sum_{k' \neq k} \hat{f}_{k'}(X_{ik'}) - g_k(X_{ik}) \right)^2 + \lambda \| g_k \|_\infty
\end{align*}
where $\bar{y}$ is the empirical mean of $y$. Our estimate of the relevant variables is $\hat{S} = \{ k \,:\, \| \hat{f}_k \| > 0 \trm{ or } \| \hat{g}_k \| > 0\}$.

We present the optimization algorithms in Section~\ref{sec:optimization}.
The convex constraints for the additive functions, analogous to 
the multivariate constraints \eqref{eq:convreg},
are  that each component $f_{k}(\cdot)$ 
can be represented by its supporting hyperplanes, i.e.,
\begin{equation}
      f_{ki'} \geq f_{ki} + \beta_{ki}(x_{ki'}-x_{ki}) \quad \text{for
        all $i,i'$}
\end{equation}
where $f_{ki}\coloneqq f_{k}(x_{ki})$ and $\beta_{ki}$ is the
subgradient at point $x_{ki}$. While this apparently requires $O(n^2
p)$ equations to impose the supporting hyperplane constraints, 
in fact, only $O(np)$ constraints suffice.  This is because univariate convex functions are
characterized by the condition that the subgradient, which is a scalar, must
increase monotonically. This observation leads to a reduced quadratic
program with $O(np)$ variables and $O(np)$ constraints. 

Directly applying a QP solver to this optimization is still computationally
expensive for relatively large
$n$ and $p$.  We thus develop a block
coordinate descent method, where in each step we solve a sparse
quadratic program involving $O(n)$ variables and $O(n)$ constraints.  This 
is efficiently solved using optimization packages 
such as {\sc mosek}.  The details of these optimizations
are given in Section~\ref{sec:optimization}.


\subsection{Finite sample analysis}


In Section~\ref{sec:finitesample} 
we analyze the finite sample variable selection consistency of AC/DC, without assuming that the true
regression function $f_0$ is additive. Our analysis first establishes
a sufficient deterministic condition for variable selection 
consistency, and then considers a stochastic setting.
Our proof technique decomposes the KKT conditions for the optimization
in a manner that is similar to the now standard \emph{primal-dual
  witness} method~\citep{wainwright2009sharp}. 

We prove separate results that allow us to analyze false negative
rates and false positive rates.  To control false positives,
we analyze scaling conditions on the regularization parameter
$\lambda_n$ for 
group sparsity needed to zero out irrelevant variables
$k \in S^c$, where $S\subset \{1,\ldots, p\}$ is the set of
variables selected by the AC/DC algorithm in the population setting.
To control false negatives, we analyze the restricted regression
where the variables in $S^c$ are zeroed out, following the primal-dual
strategy.  



Each of our theorems uses a subset of the following assumptions:
\begin{packed_enum}
\item[A1:] $X_S, X_{S^c}$ are independent. 
\item[A2:] $f_0$ is convex with a bounded second derivative. $\E f_0(X) = 0$.
\item[A3:] $\|f_0\|_\infty \leq sB$ and $\|f^*_k \|_\infty \leq B$ for all $k$.
\item[A4:] The noise is mean-zero sub-Gaussian with scale $\sigma$, independent of $X$.
\item[A5:] The density $p(\mathbf{x})$ is bounded away from $0/\infty$ and satisfies the boundary flatness condition.
\end{packed_enum}
In Assumption A3, $f^*=\sum_k f^*_k$ denotes the optimal additive projection of $f_0$ in the population setting.

Our analysis involves parameters $\alpha_+$ and $\alpha_-$,
which are measures of the signal strength of the weakest variable:
\begin{align*}
\alpha_+ &= \inf_{f \in \mathcal{C}^p \,:\, \textrm{supp}(f)\subsetneq \textrm{supp}(f^*)} 
       \Big\{ \mathbb{E} \big( f_0(X) - f(X) \big)^2 - 
        \mathbb{E} \big( f_0(X) - f^*(X) \big)^2  \Big\}\\
\alpha_- &=   \min_{k \in S \,:\, g^*_k \neq 0}
      \Big\{ \mathbb{E} \big( f_0(X) - f^*(X) \big)^2 - 
    \mathbb{E} \big( f_0(X) - f^*(X) - g^*_k(X_k) \big)^2 \Big\}.
\end{align*}

 Intuitively, if $\alpha_+$ is small, then it is easier to make a
false omission in the additive convex stage of the procedure. If
$\alpha_-$ is small, then it is easier to make a false omission in
the decoupled concave stage of the procedure.

We make strong assumptions on the covariates in A1 in order to make
very weak assumptions on the true regression function $f_0$ in
A2; in particular, we do not assume that $f_0$ is additive. 
Relaxing this condition is an important direction for future work.
We also include an extra
boundedness constraint to use new bracketing number
results \citep{kim2014global}.

Our main result is the following.
\begin{stheorem}
Suppose assumptions A1-A5 hold. Let $\{\hat{f}_i\}$ be any AC solution and
let $\{\hat{g}_k\}$ be any DC solution, both estimated with 
regularization parameter $\lambda$ scaling as
$\lambda = \Theta \Big( s\tilde{\sigma} \sqrt{\frac{1}{n} \log^2 np} \Big)$. 
Suppose in addition that
\begin{gather}
\alpha_f/\tilde{\sigma} \geq c B^2 \sqrt{\frac{s^5}{n^{4/5}} \log^2
  np}\\
\alpha_g^2/\tilde{\sigma} \geq c B^4 \sqrt{\frac{s^5}{n^{4/5}}
  \log^2 2np}.
\end{gather} 
where $\tilde{\sigma} \equiv \max(\sigma, B)$ and $c$ is a constant dependent only on $b, c_1$.
Then, for sufficiently large $n$, with probability at least $1-\frac{1}{n}$,
\begin{align*}
\hat{f}_k \neq 0 \trm{ or } \hat{g}_k \neq 0 &\trm{ for all } k \in S\\
\hat{f}_k = 0 \trm{ and } \hat{g}_k = 0 & \trm{ for all } k \notin S.
\end{align*}

%% this statement "for sufficiently large n" should be changed.  the
%% result is stronger than this, since we get a sample complexity.

\end{stheorem}

This shows that variable selection consistency is achievable under
exponential scaling of the ambient dimension, $p = O(\exp(cn))$
for some $0<c<1$, as for linear models. The cost of nonparametric estimation is
reflected in the scaling with respect to $s=|S|$, which can grow only
as $o(n^{4/25})$.

We remark that \citet{dalalyan:12} show that, even under the product distribution,
variable selection is achievable under traditional smoothness constraints only if $n > O(e^s)$. 
Here we demonstrate that convexity yields the scaling $n =
O(\textrm{poly}(s))$.


