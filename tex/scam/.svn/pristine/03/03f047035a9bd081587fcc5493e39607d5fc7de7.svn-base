\section{Analysis of Variable Selection Consistency}

We divide our analysis into two parts: we first establish a sufficient \emph{deterministic} condition for sparsistency and then reason under a stochastic setting and argue that the deterministic conditions hold with high probability. 

\subsection{Deterministic Setting}

We follow Wainwright \cite{wainwright2009sharp} and define a purely theoretical construct called \emph{restricted regression}.
\begin{definition}
In \emph{restricted regression}, we restrict $k$ in optimization (\ref{opt:alternate_opt}) to be only in $S$ instead of ranging from $1,...,p$. 
\end{definition}

Our analysis then diverges from the Primal Dual Witness technique~\cite{wainwright2009sharp} somewhat. Because our optimization is much more complicated, we do not solve for the dual variables of the restricted regression explicitly as \cite{wainwright2009sharp} does; we instead reason with the restricted regression residue, which is implicitly a function of the dual variables. 

\begin{theorem} (Deterministic) \\
\label{thm:deterministic}
The following holds regardless of whether we impose the boundness and Lipschitz condition in optimization~\ref{opt:alternate_opt} or not. 

Let $\{\hat{d}_k, \hat{c}_k \}_{k \in S}$ be the minimizer of the restricted regression, that is, the solution to optimization (\ref{opt:alternate_opt}) where we restrict $k \in S$. Let $\hat{d}_k = 0$ and $ \hat{c}_k = 0$ for $k \in S^c$.

Let $\hat{r} \coloneqq Y - \sum_{k \in S} (\Delta_k \hat{d}_k - \hat{c}_k \mathbf{1})$ be the restricted regression residue. For $k \in \{1,...,p\}$, let $\Delta_{k, j}$ denote the $n$-dimensional vector $\max( X_k - X_{k (j)} \mathbf{1}, 0)$. \\

Suppose for all $j$ and all $k\in S^c$, $\lambda_n > | \frac{1}{n} \hat{r}^\tran \Delta_{k,j}|$, then $\hat{d}_k, \hat{c}_k$ for $k=1,...,p$ is an optimal solution to the full regression \ref{opt:alternate_opt}. Furthermore, any solution to the optimization program \ref{opt:alternate_opt} must be zero on $S^c$.
\end{theorem}

\begin{remark}
The incoherence condition of \cite{wainwright2009sharp} is implicitly encoded in our condition on $\lambda_n, \hat{r}, \Delta_{k,j}$. We can reconstruct the incoherence condition if we assume that the true function $f_0$ is linear and that our fitted function $\hat{f}_k$'s are linear as well.
\end{remark}

Theorem~\ref{thm:deterministic} lets analyze false negative rates and false positive rates separately. To control false positives, we study when the condition $\lambda_n > | \frac{1}{n} \hat{r}^\tran \Delta_{k,j}|$ is fulfilled for all $j$ and all $k \in S^c$. To control false negatives, we study the restricted regression.

\subsection{Probabilistic Setting}

We will use the following statistical \textbf{setting}:

\begin{packed_enum}
\item Let $\mathcal{X} = [-b, b]^p$. Let $\mathcal{P}$ be a distribution on $\mathcal{X}$. Let $X^{(1)},..., X^{(n)} $ be independent samples from $\mathcal{P}$. 
\item Let $Y' = f_0(X) + \epsilon$ where $f_0$ is the true function and $\epsilon$ is noise.  Given $n$ samples $Y'^{(1)},...,Y'^{(n)}$, we input into our optimization $Y = Y' - \bar{Y}'$.
\item Let $S \subset \{1,...,p\}$ denote the set of relevant variables, that is, $f_0(X) = f_0(X_S)$ and let $s \coloneqq |S|$. 
\end{packed_enum}

Each of our statistical theorems will use a subset of the following list of \textbf{assumptions}:
\begin{packed_enum}
\item[A1:] $X_S, X_{S^c}$ are independent. A1': $\{ X_k \}_{k \in S}$ are all independent.
\item[A2:] $\|f_0\|_\infty \leq B$. A2': $f_0$ is convex, twice-differentaible, and $L$-Lipschitz, depends on all coordinates in $S$.
\item[A3:] Suppose $\epsilon$ is mean-zero subgaussian, independent of $X$, with subgaussian scale $\sigma$.
\end{packed_enum}

We will use assumptions A1, A2, A3 to control the probability of false positives and the stronger assumptions A1', A2', A3 to control the probability of false positives.

\begin{remark}
We make very strong assumptions on the covariates in A1 in order to make a very weak assumptions on the true regression function $f_0$ in A2. In particular, we do not assume that $f_0$ is additive. Strong assumptions on the covariates are not uncommon in nonparametric variable selection analysis~\cite{lafferty2008rodeo}. [TODO: refer to correlated design experiment].
\end{remark}

\begin{theorem} (Controlling False Positives) \\
\label{thm:false_positive}
Suppose assumptions A1, A2, A3 hold. Suppose also that we run optimization~\ref{opt:alternate_opt} with the $B$-boundness constraint. Let $c,C$ be absolute constants.

Suppose $\lambda_n \geq c b (sB + \sigma) \sqrt{ \frac{s}{n} \log n \log (pn)}$, then with probability at least $ 1 - \frac{C}{n}$, for all $j,k$,
\[
\lambda_n >  | \frac{1}{n} \hat{r}^\tran \Delta_{k,j}|
\]
And therefore, any solution to the full regression (\ref{opt:alternate_opt}), with boundedness constraint, is zero on $S^c$. 
\end{theorem}



\begin{theorem} (Controlling False Negatives)\\
\label{thm:false_negative}
Suppose assumptions A1', A2', A3 hold. Let $\ds \hat{f} = \{ \hat{d}_k, \hat{c}_k\}_{k\in S}$ be any solution to the \emph{restricted regression} with both the $B$-boundedness and $L$-Lipschitz constraint. Let $c,C$ be absolute constants.

Suppose $L \max \left(\lambda_n, b (B+\sigma)B\sigma \sqrt{\frac{1}{n^{4/5}} s^5 \log sn} \right) \rightarrow 0$.\\

Then, for large enough $n$, with probability at least $1-\frac{C}{n}$, $\hat{f}_k = (\hat{d}_k, \hat{c}_k) \neq 0$ for all $k \in S$. 
\end{theorem}

Combining Theorem~\ref{thm:false_positive} and ~\ref{thm:false_negative} and ignoring dependencies on $b,B,L,\sigma$, we come to the following conclusion.
\begin{corollary}
Assume A1', A2', A3. Let $\lambda_n = \Theta\left( \sqrt{ \frac{1}{n} s^3 \log n \log (pn)} \right)$. 

Suppose $\lambda_n \rightarrow 0$ and $\sqrt{\frac{1}{n^{4/5}} s^5 \log sn} \rightarrow 0$. 

Let $\hat{f}$ be solution to (\ref{opt:alternate_opt}) with $B$-boundedness and $L$-Lipschitz constraints. Then for all large enough $n$, $P( \trm{supp}(\hat{f}) = \trm{supp}(f_0) ) \geq 1 - O(\frac{1}{n})$.
\end{corollary}


