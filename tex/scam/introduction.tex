\section{Introduction}


Shape restrictions such as monotonicity, convexity, and concavity
provide a natural way of limiting the complexity of many statistical
estimation problem.  Shape-constrained estimation is not as well
understood as more traditional nonparametric estimation involving
smoothness constraints.  For instance, as of this writing, the minimax
rate of convergence for multivariate convex regression has yet to be
rigorously established in full generality.  Even the one-dimensional
case is challenging, and has been of recent interest
\citep{guntusen:13}.

In this paper we study the problem of variable selection in convex
regression.  Assuming that the regression function is convex and
sparse, our goal is to identify the relevant variables.  We show that
it suffices to estimate a sum of $p$ one-dimensional convex functions,
leading to significant computational and statistical advantages.  This
is in contrast to general nonparametric regression, where fitting an
additive model can result in false negatives.  Our approach is based
on a two-stage quadratic programming procedure.  In the first stage,
we fit an convex additive model, imposing a sparsity penalty.  In the
second stage, we fit a concave function on the residual for each
variable.  As we demonstrate, this perhaps non-intuity second stage is
in general necessary.  Our first result is that this procedure is
faithful in the population setting, meaning that it results in no
false negatives, under mild assumptions on the density of the
covariates.  Our second result is a finite-sample statistical analysis
of the procedure, where we upper bound the statistical rate of
convergence.  Each stage of the method requires quadratic programming.
An additional contribution is to show how these quadratic programs can
be forumlated to be more scalable.  Simulations demonstrate the method
performs in a manner that is consistent with our analysis.

Variable selection for nonparametric
regression under smoothness constraints is difficult without
making additional strong assumptions. 
\cite{lafferty2008rodeo} show that computationally
efficient, near minimax-optimal estimation is possible, but in ambient
dimensions that scale only as $p = O(\log n)$; see also \cite{BertinLecue}.  This is in contrast
to the $p=O\bigl(e^{n^c}\bigr)$ scaling enjoyed by sparse linear
models \cite{Wain:09a}. \citet{dalalyan:12} achieve exponential scaling $p=O(e^n)$
under certain Fourier smoothness conditions, but require
that the number of relevant variables $s$ must be less than $\log n$.

Approximating the regression function by a sum of one-dimensional
functions, known as sparse additive models \citep{Ravikumar:09}, is a
practical alternative to fully nonparametric function estimation.  
Sparse additive models have been further studied in by \cite{Meier09},
and by \cite{Raskutti:12}.  But 
the additive assumption is limited.  In particular, the natural idea
of first selecting the single variable effects, then the pairwise
effects, and so on, does not in general lead to consistent variable
selection.  In other words, the general nonparametric model is not
additively faithful.  Remarkably, the additional assumption of
convexity does lead to consistent variable selection, as we show
here. In addition, we show that the high dimensional scaling $n =
O\big(\textrm{poly}(s) \log p\big)$ is achievable for sparse convex
additive models. Thus, with respect to variable selection, the
geometric convexity constraint is quite different from the smoothness
constraints imposed in traditional nonparametric regression.

A key to our approach is the observation that least squares
nonparametric estimation under convexity constraints is equivalent to
a finite dimensional quadratic program.  Specifically, the infinite
dimensional optimization 
\begin{align}
\begin{split}
\text{minimize} & \quad \sum_{i=1}^n (Y_i - m(x_i))^2 \\
\text{subject to} &  \quad m:\reals^p\rightarrow\reals\ \text{is
  convex}
\end{split}
\end{align}
is precisely equivalent to the finite dimensional quadratic
program 
\begin{align}
\begin{split}
\label{eq:outer}
\text{minimize}_{h, \beta} & \;\; \sum_{i=1}^n (Y_i - h_i)^2 \\
\text{subject to} & \;\; h_j \geq h_i + \beta_i^T (x_j-x_i),\; \text{for
    all $i,j$}.
\end{split}
\end{align}
%\end{equation}
%See \cite{Boyd04}, Section 6.5.5.
Here $h_i$ is the estimated function value $m(x_i)$, and the vectors
$\beta_i \in \reals^d$ represent supporting hyperplanes to the
epigraph of $m$.  Importantly, this finite dimensional quadratic program does
not have tuning parameters for smoothing the function. Such parameters are the bane
of nonparametric estimation.

Estimation of convex functions arises naturally in several
applications.  Examples include geometric programming \cite{Boyd04},
computed tomography \cite{Prince:90}, target reconstruction
\cite{Lele:92}, image analysis \cite{Golden:06} and circuit design
\cite{Hannah:12}.  Other applications include queuing theory
\cite{Chen:01} and economics, where it is of interest to estimate
concave utility functions \cite{Pratt:68}.  See \cite{Lim:12} for
other applications.  

Beyond cases where the assumption of convexity is
natural, we offer that the convexity assumption is attractive as a
tractable, nonparamametric relaxation of the linear model.  In
addition to the lack of tuning parameters, other than the
regularization parameter $\lambda$ to control the level of sparsity,
the global convexity assumption leads to effective, scalable algorithms.  We
demonstrate use of our approach on experiments with standard
regression data sets, in a comparison with sparse linear models
(lasso).

%In the following section we give a high-level summary of our technical
%results, including additive faithfulness, variable selection 
%consistency, and high dimensional scaling.  In Section~X...

\def\mathbf#1{\mbox{\boldmath $#1$}} 



% DO NOT CHANGE; RefTex variables -minx

%%% Local Variables: ***
%%% mode:latex ***
%%% TeX-master: "paper.tex" ***
%%% End: ***