\section{Introduction}


Shape restrictions such as monotonicity, convexity, and concavity
provide a natural way of limiting the complexity of many statistical
estimation problems.  Shape-constrained estimation is not as well
understood as more traditional nonparametric estimation involving
smoothness constraints.  
%For instance, the minimax rate of convergence
%for multivariate convex regression has yet to be rigorously
%established in full generality.  
Even the one-dimensional case is
interesting and challenging, and has been of recent interest \citep{guntusen:13}.

In this paper we study the problem of variable selection in
multivariate convex regression.  Assuming that the regression function
is convex and sparse, our goal is to identify the relevant variables.
We show that it suffices to estimate a sum of one-dimensional convex
functions, leading to significant computational and statistical
advantages.  This is in contrast to general nonparametric regression,
where fitting an additive model can result in false negatives.  Our
approach is based on a two-stage quadratic programming procedure.  In
the first stage, we fit an convex additive model, imposing a sparsity
penalty.  In the second stage, we fit a concave function on the
residual for each variable.  As we show, this non-intuitive second
stage is in general necessary.  Our first result is that this
procedure is faithful in the population setting, meaning that it
results in no false negatives, under mild assumptions on the density
of the covariates.  Our second result is a finite sample statistical
analysis of the procedure, where we upper bound the statistical rate
of variable screening consistency.  An additional contribution is to show how the
required quadratic programs can be formulated to be more scalable.  We
give simulations to illustrate our method, showing that it performs in
a manner that is consistent with our analysis.

Estimation of convex functions arises naturally in several
applications.  Examples include geometric programming \citep{Boyd04},
computed tomography \citep{Prince:90}, target reconstruction
\citep{Lele:92}, image analysis \citep{Golden:06} and circuit design
\citep{Hannah:12}.  Other applications include queuing theory
\citep{Chen:01} and economics, where it is of interest to estimate
concave utility functions \citep{Pratt:68}.  See \cite{Lim:12} for
other applications.  
Beyond cases where the assumption of convexity is
natural, convexity can be attractive as a
tractable, nonparamametric relaxation of the linear model.  

Recently, there has been increased research activity on shape-constrained estimation. \cite{guntusen:13} analyze univariate convex regression and show surprisingly that the risk of the MLE is adaptive to the complexity of the true function. \cite{seijo2011nonparametric} and \cite{Lim:12} study maximum likelihood estimation of multivariate convex regression and independently establish its consistency. \cite{Cule:10} and \cite{kim2014global} analyze log-concave density estimation and prove consistency of the MLE; the latter further show that log-concave density estimation has minimax risk lower bounded by $n^{-2/(d+1)}$ for $d \geq 2$, refuting a common notion that the condition of convexity is equivalent, in estimation difficulty, to the condition of having two bounded derivatives. Additive shape-constrained estimation has also been studied; \cite{pya2014shape} propose a penalized B-spline estimator while \cite{chen2014generalised} show the consistency of the MLE. To the best of our knowledge however, there has been no work on variable selection and estimation of high-dimensional convex functions.


Variable selection in general nonparametric regression or function
estimation is a notoriously difficult problem. \citet{lafferty2008rodeo} develop a greedy procedure for
adjusting bandwidths in a local linear regression estimator,
and show that the procedure achieves the minimax rate
as if the relevant variables were isolated in advance.
But the method only provably scales to dimensions $p$ that 
grow logarithmically in the sample size $n$, i.e., $p = O(\log n)$.  This
is in contrast to the high dimensional scaling behavior
known to hold for sparsity selection in linear models
using $\ell_1$ penalization, where $n$
is logarithmic in the dimension $p$. \citet{bertin:08}
develop an optimization-based approach in
the nonparametric setting, applying the lasso
in a local linear model at each test point.  Here again,
however, the method only scales as $p = O(\log n)$,
the low-dimensional regime.
An approximation theory approach to the same
problem is presented in \cite{devore:11}, 
using techniques based on hierarchical hashing schemes,
similar to those used for ``junta'' problems \citep{mossel:04}.
Here it is shown that the sample complexity scales as $n > \log p$ 
if one adaptively selects the points on
which the high-dimensional function is evaluated.

\citet{dalalyan:12} show that the exponential scaling $n=O(\log p)$ is
achievable if the underlying function is assumed to be smooth with
respect to a Fourier basis. They also give support for the intrinsic
difficulty of variable selection in nonparametric regression, giving
lower bounds showing that consistent variable selection is not
possible if $n < \log p$ or if $n < \exp s$, where $s$ is the number
of relevant variables.  Variable selection over kernel classes is
studied by \citet{Kolch:10}.

Perhaps more closely related to the present work is the framework
studied by \cite{Raskutti:12} for sparse additive models, where sparse
regression is considered under an additive assumption, with each
component function belonging to an RKHS.  An advantage of working over
an RKHS is that nonparametric regression with a sparsity-inducing
regularization penalty can be formulated as a finite dimensional
convex cone optimization.  On the other hand, smoothing parameters for
the component Hilbert spaces must be chosen, leading to extra tuning
parameters that are difficult to select in practice. There has also been
work on estimating sparse additive models over a spline basis, for 
instance the work of \cite{huang2010variable}, but these approaches also
require the tuning of smoothing parameters. 

While nonparametric, the convex regression problem is naturally
formulated using finite dimensional convex optimization, with no
additional tuning parameters. The convex additive model can be used
for convenience, without assuming it to actually hold, for the purpose
of variable selection. As we show, our method scales to high
dimensions, with a dependence on the intrinsic dimension $s$ that
scales polynomially, rather than exponentially as in the general case
analyzed in \cite{dalalyan:12}. 



%* Laetitia Comminges, Arnak S. Dalalyan 
%Tight conditions for consistency of variable selection 
%In the context of high dimensionality. 
%Ann. Statist, 40(5), 2667-2696, 2012 
%
%Comment: They estimate the coefficient of the regression function with respect to a Fourier basis and then threshold the estimated coefficients. They prove that sparsistency is achievable so long as $n > log p$ and $n > exp(s)$. They assume that the functions are smooth with respect to the Fourier basis. The fourier coefficients however seem difficult to estimate; it appears that the knowledge of the true density is required. They also provide 
%
%
%* Bertin, K. and Lecu´e, G. (2008). Selection of variables 
%and dimension reduction in high-dimensional non-parametric 
%regression. Electron. J. Stat. 2 1224–1241 
%
%Comment: They assume Holder smoothness and use multiple L1 regularized local polynomial smoothing to detect the sparsity pattern. They require that $p < log n$.
%
%
%* "Approximation of Functions of Few Variables in High Dimensions" 
%by DeVore, Petrova and Wojtaszczyk. 
%
%Comment: They show that variable selection under a scaling of
%


%Variable selection for nonparametric
%regression under smoothness constraints is difficult without
%making additional strong assumptions. 
%\cite{lafferty2008rodeo} show that computationally
%efficient, near minimax-optimal estimation is possible, but in ambient
%dimensions that scale only as $p = O(\log n)$; see also \cite{BertinLecue}.  This is in contrast
%to the $p=O\bigl(e^{n^c}\bigr)$ scaling enjoyed by sparse linear
%models \cite{Wain:09a}. \citet{dalalyan:12} achieve exponential scaling $p=O(e^n)$
%under certain Fourier smoothness conditions, but require
%that the number of relevant variables $s$ must be less than $\log n$.
%
%Approximating the regression function by a sum of one-dimensional
%functions, known as sparse additive models \citep{Ravikumar:09}, is a
%practical alternative to fully nonparametric function estimation.  
%Sparse additive models have been further studied in by \cite{Meier09},
%and by \cite{Raskutti:12}.  But 
%the additive assumption is limited.  In particular, the natural idea
%of first selecting the single variable effects, then the pairwise
%effects, and so on, does not in general lead to consistent variable
%selection.  In other words, the general nonparametric model is not
%additively faithful.  Remarkably, the additional assumption of
%convexity does lead to consistent variable selection, as we show
%here. In addition, we show that the high dimensional scaling $n =
%O\big(\textrm{poly}(s) \log p\big)$ is achievable for sparse convex
%additive models. Thus, with respect to variable selection, the
%geometric convexity constraint is quite different from the smoothness
%constraints imposed in traditional nonparametric regression.
%

In the following section we give a high-level summary of our technical
results, including additive faithfulness, variable selection 
consistency, and high dimensional scaling.  In
Section~\ref{sec:additivefaithful} we give a detailed account
of our method and the conditions under which we can guarantee
consistent variable selection.  In Section~\ref{sec:optimization}
we show how the required quadratic programs can be reformulated
to be more efficient and scalable.  In Section~\ref{sec:finitesample}
we give the details of our finite sample analysis, showing
that a sample size growing as $n = O\big(\textrm{poly}(s) \log p\big)$
is sufficient for variable selection.  In Section~\ref{sec:thesims}
we report the results of simulations that illustrate our methods
and theory.  The full proofs are
given in a technical appendix.




% DO NOT CHANGE; RefTex variables -minx

%%% Local Variables: ***
%%% mode:latex ***
%%% TeX-master: "paper.tex" ***
%%% End: ***
