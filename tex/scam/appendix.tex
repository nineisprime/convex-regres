\section{Appendix}
 
 
 \subsection{Proof of the Deterministic Condition for Sparsistency}
 \label{sec:deterministic_proof}
 
We restate Theorem~\ref{thm:deterministic} first for convenience. 
 
\begin{theorem} 
The following holds regardless of whether we impose the Lipschitz condition in optimization~\ref{opt:alternate_opt} and optimization~\ref{opt:alternate_opt_concave}.

Let $\{\hat{d}_k \}_{k \in S}$ be a minimizer of the restricted regression, that is, the solution to optimization (\ref{opt:alternate_opt}) where we restrict $k \in S$. 
Let $\hat{r} \coloneqq Y - \sum_{k \in S} \bar{\Delta}_k \hat{d}_k$ be the restricted regression residual. \\

Suppose for all $k\in S^c$, for all $i=1,...,n$, $\lambda_n > | \frac{1}{2n}
\hat{r}^\tran \mathbf{1}_{(i:n)}|$ where $\mathbf{1}_{(i:n)}$ is 1 on the coordinates of the $i$-th largest to the $n$-th largest entries of $X_k$ and 0 elsewhere.\\

Then the following are true:
\begin{enumerate}
\item Let $\hat{d}_k = 0$ for $k \in S^c$, then \{$\hat{d}_k\}_{k=1,...,p}$ is an optimal solution to optimization~\ref{opt:alternate_opt}. Furthermore, any solution to the optimization program \ref{opt:alternate_opt} must be zero on $S^c$.
\item For all $k \in S^c$, the solution to optimization~\ref{opt:alternate_opt_concave} must be 0 and must be unique.
\end{enumerate}
\end{theorem}


\begin{proof} 
We will omit the Lipschitz constraint in our proof here. It is easy to add that in and check that the result of the theorem still holds.\\

We first consider the first item in the conclusion of the theorem.

We will show that with $\{\hat{d}_k\}_{k=1,..,p}$ as constructed, we can set the dual variables to satisfy complementary slackness and stationary conditions: $\nabla_{d_k} \mathcal{L}(\hat{d})  = 0$ for all $k$.\\ 

The Lagrangian is
\begin{equation}
\label{eqn:full_lagrange}
\mathcal{L}( \{ d_k \}, \nu) = 
  \frac{1}{2n} \Big\| 
    Y - \sum_{k=1}^p  \bar{\Delta}_k d_k  \Big\|_2^2 + 
    \lambda \sum_{k=1}^p \| \bar{\Delta}_k d_k \|_\infty -
    \sum_{k=1}^p \sum_{i=2}^{n-1} \nu_{ki} d_{ki} 
\end{equation}
with the constraint that $\nu_{ki} \geq 0$ for all $k,i$.

Because $\{\hat{d}_k\}_{k \in S}$ is by definition the optimal solution of the restricted regression, it is a consequence that stationarity holds for $k \in S$, that is, $\partial_{ \{ d_k \}_{k \in S} } \mathcal{L}(d) = 0$, and that the dual variables $\nu_k$ for $k \in S$ satisfy complementary slackness.

We now verify that stationarity holds also for $k \in S^c$. We fix one dimension $k \in S^c$ and let $\hat{r} = Y - \sum_{k' \in S} \bar{\Delta}_{k'} \hat{d}_{k'}$. 

The Lagrangian form of the optimization, in term of just $d_k$, is
\[
\mathcal{L}(d_k, \nu_k) =
  \frac{1}{2n} \big\| Y - \sum_{k' \in S} \bar{\Delta}_{k'} d_{k'} 
  -  \bar{\Delta}_k d_k \big\|_2^2 
   + \lambda \| \bar{\Delta}_k d_k\|_\infty
  - \sum_{i=2}^{n-1} \nu_{ki} d_{ki}
\]
with the constraint that $v_i \geq 0$ for all $i$. 

The derivative of the Lagrangian is:
\begin{align*}
\partial_{d_k} \mathcal{L}(d_k) =  -\frac{1}{n} \bar{\Delta}_k^\tran ( Y - \sum_{k'\in S} \bar{\Delta}_{k'} d_{k'}  - \bar{\Delta}_k d_k )
        + \lambda \bar{\Delta}_k^\tran \mathbf{u}
      - \nu_k
\end{align*}
where $\mathbf{u}$ is the subgradient of $\| \bar{\Delta}_k d_k \|_\infty$, an $n$-vector such that $\| \mathbf{u}_T \|_1 = 1$ where $T = \{ i \,:\,  (\bar{\Delta}_k d_k)_i = \| \bar{\Delta}_k d_k \|_\infty \}$.

We now substitute in $d_{k'} = \hat{d}_{k'}$ for $k' \in S$, $d_k = 0$ for $k \in S$, and $r = \hat{r}$ and show that the duals can be set in a way to ensure that the derivatives are equal to 0.

\begin{align*}
\partial_{d_k} \mathcal{L}(\hat{d}_k) = -\frac{1}{n} \bar{\Delta}_k^\tran\hat{r} + \lambda \bar{\Delta}_k^\tran \mathbf{u}
           - \nu_k = 0 
\end{align*}
where $\| \mathbf{u} \| \leq 1$ and $\nu_k \geq 0$. It clear that to show stationarity, we only need to show that $-\frac{1}{n} \bar{\Delta}_k^\tran \hat{r} + \lambda \bar{\Delta}_k^\tran \mathbf{u} \geq 0$ hwere the inequality is element-wise.

Let us reorder the samples so that the $i$-th sample is the $i$-smallest sample. \\

We will construct $\gamma = 0$, and $\mathbf{u} = (-a, 0, ..., a)$ for some $0 < a < 1/2$. (coordinates of $\mathbf{u}$ correspond to the new sample ordering) We then just need to show that
\begin{align*}
- \frac{1}{n} \bar{\Delta}_k^\tran \hat{r} + \lambda \bar{\Delta}_k^\tran \mathbf{u} &\geq 0 \quad \Leftrightarrow \\
- \frac{1}{n} \Delta_k^\tran \hat{r} + \lambda \Delta_k^\tran \mathbf{u} &\geq 0 \quad \Leftrightarrow\\
- \frac{1}{n} \sum_{i > j} (X_{ki} - X_{kj}) \hat{r}_i + \lambda (X_{kn} - X_{kj})a &\geq 0 \quad 
   \textrm{for each } j \\
- \frac{1}{n} \sum_{i > j} \sum_{j < i' \leq i} \mathsf{gap}_{i'} \hat{r}_i 
    + \lambda (X_{kn} - X_{kj})a &\geq 0 \\
- \frac{1}{n} \sum_{i' > j} \mathsf{gap}_{i'} \sum_{i \geq i'} \hat{r}_i 
    + \lambda (X_{kn} - X_{kj})a &\geq 0 \\
- \frac{1}{n} \sum_{i' > j} \mathsf{gap}_{i'} \mathbf{1}_{(i':n)}^\tran \hat{r} 
   + \lambda (X_{kn} - X_{kj})a &\geq 0 
\end{align*}
where $\mathsf{gap}_i = X_{ki} - X_{k,i-1}$. If $\frac{1}{2n} |\mathbf{1}_{(i:n)}^\tran \hat{r}| \leq \lambda a$ for all $i=1,...,n$, then we have that:

\begin{align*}
- \frac{1}{n} \sum_{i' > j} \mathsf{gap}_{i'} \mathbf{1}_{i':n}^\tran \hat{r}
   + \lambda (X_{kn} - X_{kj})a &\geq 0 \quad \Leftrightarrow \\
- \sum_{i' > j} \mathsf{gap}_{i'} \lambda a + \lambda (X_{kn} - X_{kj}) a &\geq 0\\
- (X_{kn} - X_{kj}) \lambda a + \lambda (X_{kn} - X_{kj}) a &\geq 0
\end{align*}

We have thus proven that there exist one solution $\{ \hat{d}_k \}_{k=1,...,p}$ such that $\hat{d}_k = 0$ for all $k \in S^c$. Furthermore, we have shown that the subgradient variables $\mathbf{u}_k$ of the solution $\{ \hat{d}_k \}$ can be chosen such that $\| \mathbf{u}_k \|_1 < 1$ for all $k \in S^c$.  We now prove that if $\{ \hat{d}'_k \}_{k = 1,..., p}$ is another solution, then it must be that $\hat{d}'_k = 0$ for all $k \in S^c$ as well.  \\

%% prove uniqueness of sparsity pattern.

We first claim that $\sum_{k=1}^p \bar{\Delta}_k \hat{d}_k = \sum_{k=1}^p \bar{\Delta}_k \hat{d}'_k$. If this were not true, then a convex combination of $\hat{d}_k, \hat{d}'_k$ would achieve a strictly lower objective on the quadratic term. More precisely, let $\zeta \in [0,1]$. If $\sum_{k=1}^p \bar{\Delta}_k \hat{d}'_k \neq \sum_{k=1}^p \bar{\Delta}_k \hat{d}_k$, then $\| Y - \sum_{k=1}^p \bar{\Delta}_k \big( \hat{d}_k + \zeta ( \hat{d}'_k - \hat{d}_k) \big) \|_2^2$ is strongly convex as a function of $\nu$. Thus, it cannot be that $\hat{d}_k$ and $\hat{d}'_k$ both achieve optimal objective and we have reached a contradiction.\\

Now, we look at the stationarity condition for both $\{ \hat{d}_k \}$ and $\{ \hat{d}'_k \}$. Let $\mathbf{u}_k \in \partial \| \bar{\Delta}_k \hat{d}_k \|_\infty$ and let $\mathbf{u}'_k \in \partial \| \bar{\Delta}_k \hat{d}'_k \|_\infty$ be the two sets of subgradients. Let $\{ \nu_{ki} \}_{k=1,..,p,\, i=1,...n-1}$ and $\{ \nu'_{ki} \}$ be the two sets of positivity dual variables. \footnote{since there is no positivity constraint on $d_{k1}$, we let $\nu_{k1} = 0$ always.}

Let us define $\bar{\Delta}$, a $n \times p(n-1)$ matrix, to denote the column-wise concatenation of $\{ \bar{\Delta}_k \}_k$ and $\hat{d}$, a $p(n-1)$ dimensional vector, to denote the concatenation of $\{ \hat{d}_k \}_k$. With this notation, we can express $\sum_{k=1}^p \bar{\Delta}_k \hat{d}_k = \bar{\Delta} \hat{d}$.

Since both solutions $(\hat{d}, \mathbf{u}, \nu)$ and $(\hat{d}', \mathbf{u}', \nu')$ must satisfy the stationarity condition, we have that:
\[
\bar{\Delta}^\tran ( Y - \bar{\Delta} \hat{d} ) 
   + \lambda \sum_{k=1}^p \bar{\Delta}_k^\tran \mathbf{u}_k - \nu = 
\bar{\Delta}^\tran ( Y - \bar{\Delta} \hat{d}' ) 
   + \lambda \sum_{k=1}^p \bar{\Delta}_k^\tran \mathbf{u}'_k - \nu' = 0
\] 
We multiply both sides of the above equation by $\hat{d}'$:
\[
\hat{d}'^{\tran}  \bar{\Delta}^\tran ( Y - \bar{\Delta} \hat{d} ) 
    + \lambda \sum_{k=1}^p \hat{d}'^\tran_k \bar{\Delta}_k^\tran \mathbf{u}_k - \hat{d}'^\tran \nu = \hat{d}'^{\tran}  \bar{\Delta}^\tran ( Y - \bar{\Delta} \hat{d}' ) 
    + \lambda \sum_{k=1}^p \hat{d}'^\tran_k \bar{\Delta}_k^\tran \mathbf{u}'_k - \hat{d}'^\tran \nu'
\]
Since $\bar{\Delta} \hat{d}_k = \bar{\Delta} \hat{d}$, $\hat{d}'^\tran \nu' = 0$ (complementary slackness), and $\hat{d}'^\tran_k \bar{\Delta}_k^\tran \mathbf{u}'_k  = \| \hat{f}'_k \|_\infty$ (where $\hat{f}'_k = \bar{\Delta}_k \hat{d}'_k$), we have that:
\[
\lambda \sum_{k=1}^p \hat{d}'^\tran_k \bar{\Delta}_k^\tran \mathbf{u}_k - \hat{d}'^\tran \nu = \lambda \sum_{k=1}^p \| \hat{f}'_k \|_\infty
\]
On one hand, $\hat{d}'$ is a feasible solution so $\hat{d}'^\tran \nu \geq 0$ and so 
\[
\sum_{k=1}^p \hat{d}'^\tran_k \bar{\Delta}_k^\tran \mathbf{u}_k \geq \sum_{k=1}^p \| \hat{f}'_k \|_\infty .
\]

On the other hand, by Holder's inequality:
\begin{align*}
\sum_{k=1}^p \hat{d}'^\tran_k \bar{\Delta}_k^\tran \mathbf{u}_k &\leq 
   \sum_{k=1}^p \| \hat{f}'_k \|_\infty \|\mathbf{u}_k \|_1 
\end{align*}

Since $\mathbf{u}_k$ can be chosen so that $\| \mathbf{u}_k \|_1 < 1$ for all $k \in S^c$, we would get a contradiction if $\| \hat{f}'_k \|_\infty > 0$ for some $k \in S^c$. We thus conclude that $\hat{d}'$ must follow the same sparsity pattern.\\


The second item in the theorem concerning optimization~\ref{opt:alternate_opt_concave} is proven in exactly the same way. 

The Lagrangian of optimization~\ref{opt:alternate_opt_concave} is:
\[
\mathcal{L}_{\trm{cave}}(d_k, \nu_k) = 
  \frac{1}{2n} \big\| \hat{r} - \bar{\Delta}_k d_k \big \|_2^2 + 
  \lambda \| \bar{\Delta}_k d_k \|_\infty + \sum_{k=1}^p \sum_{i=2}^{n-1} \nu_{ki} d_{ki}
\]

The exact same reasoning applies to show that $\hat{d}_k = 0$ satisfies KKT conditions sufficient for optimality.

\end{proof}
 
 
 
 
 
 
 \subsection{Proof of False Positive Control}
 \label{sec:false_positive_proof}
 
\textbf{Note:} the symbols $c,C$ represent absolute constants. We will often abuse notation and ``absorb'' new absolute constants into $c, C$; the actual value of $c, C$ could thus vary from line to line.

 We first restate the theorem for convenience. 
 

\begin{theorem} 
Suppose assumptions A1-A4 hold. 

Suppose $\lambda_n \geq c s Lb \sigma  \sqrt{ \frac{1}{n} \log^2 np}$, then with probability at least $ 1 - \frac{C}{n}$, for all $k \in S^c$, and for all $i'=1,...,n$:
\[
\lambda_n > \Big| \frac{1}{2n}\hat{r}^\tran \mathbf{1}_{(i':n)} \Big|
\]

And therefore，for all $k \in S^c$, both the AC solution $\hat{f}_k$, from optimization~\ref{opt:alternate_opt}, and the DC solution $\hat{g}_k$, from optimization~\ref{opt:alternate_opt_concave} are zero. 
\end{theorem}

\begin{proof}
The key is to note that $\hat{r}$ and $\Delta_{k,j}$ are independent for all $k \in S^c,j=1,...,n$ because $\hat{r}$ is only dependent on $X_{S}$.

Fix $j$ and $i$. $\hat{r}^\tran \mathbf{1}_{(i':n)}$ is then the sum of $n-i'+1$ random coordinates of $\hat{r}$. We will then use Serfling's theorem on the concentration of measure of sampling without replacement. (corollary~\ref{cor:serfling}) We must first bound $\| \hat{r} \|_\infty$ and $\frac{1}{n} \sum_{i=1}^n \hat{r}_i$ before we can use Serfling's results however.\\

\textbf{Step 1}: Bounding $\| \hat{r} \|_\infty$. 

$\hat{r}_i = f_0(x_i) + w_i - \hat{f}(x_i)$ where $\hat{f}(x_i) = \sum_{k \in S} \bar{\Delta}_k \hat{d}_k$ is the convex additive function outputted by the restricted regression.

Both $f_0(x_i)$ and $\hat{f}(x_i)$ are coordinate-wise $L$-Lipschitz and therefore are bounded by $2 sLb$. 

Because $w_i$ is subgaussian, $|w_i| \leq c \sigma \sqrt{\log \frac{2}{\delta}}$ with probability at most $1-\delta$. By union bound across $i=1,...,n$, we have that $\| w\|_\infty \leq c \sigma \sqrt{ \log \frac{2}{\delta}}$ with probability at most $1 - n \delta$.

We now put this together and take another union bound across all $j$ and all $i'$:
\begin{align*}
\| \hat{r} \|_\infty &\leq c (sLb + \sigma \sqrt{ \log \frac{2}{\delta}}) \\
      &\leq c sLb \sigma \sqrt{\log \frac{2}{\delta}}
\end{align*}
with probability at least $1 - n^2 p \delta$. We supposed that both $sLb \geq 2$ and $\sigma\sqrt{\log \frac{2}{\delta}} \geq 2$.

\textbf{Step 2}: Bounding $| \frac{1}{n} \hat{r}^\tran \mathbf{1} |$. 

\begin{align*}
\frac{1}{n} \hat{r}^\tran \mathbf{1} &= 
    \frac{1}{n} \sum_{i=1}^n f_0(x_i) + w_i - \hat{f}(x_i) \\
  &= \frac{1}{n} \sum_{i=1}^n f_0(x_i) + w_i \quad \trm{ ($\hat{f}$ is centered)}
\end{align*}

Because $|f_0(x_i)| \leq sLb$, the first term $| \frac{1}{n} \sum_{i=1}^n f_0(x_i)|$ is at most $2 sLb \sqrt{\frac{1}{n} \log \frac{2}{\delta}}$ with probability at most $1-\delta$ by Hoeffding Inequality.

Because $w_i$ is subgaussian, the second term $|\frac{1}{n} \sum_{i=1}^n w_i|$ is at most $2 \sigma \sqrt{ \frac{1}{n} \log \frac{2}{\delta}}$ with probability at most $1-\delta$.

Taking an union bound, we have that 

\begin{align*}
| \frac{1}{n} \hat{r}^\tran \mathbf{1}| &\leq 2 sLb \sqrt{\frac{1}{n} \log \frac{2}{\delta}} + 2 \sigma \sqrt{\frac{1}{n} \log \frac{2}{\delta}} \\
  &\leq c sLb \sigma \sqrt{\frac{1}{n} \log \frac{2}{\delta}} 
\end{align*}
with probability at least $1-2\delta$.\\

\textbf{Step 3}: We now apply Serfling's theorem.

Serfling's theorem states that with probability at least $1 - \delta$:
\[
\Big
|\frac{1}{n} \hat{r}^\tran \mathbf{1}_{(i':n)}\Big| \leq
   2\| \hat{r} \|_\infty \sqrt{ \frac{1}{n} \log \frac{2}{\delta}} + 
   \Big|\frac{1}{n} \hat{r}^\tran \mathbf{1} \Big|
\]

Taking an union bound across previous events, we have that with probability at least $1 - 3n^2 p \delta$, for all $j \in S^c$, for all $i'=1,...,n$:
\begin{align*}
\Big|\frac{1}{n} \hat{r}^\tran \mathbf{1}_{(i':n)} \Big| &\leq
  c sLb \sigma \sqrt{\frac{1}{n} \log \frac{2}{\delta}} 
\end{align*}
Setting $\delta = \frac{1}{n^3 p}$ gives the desired result.

\end{proof}

 
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%
%%
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
 
 \subsection{Proof of False Negative Control}
 \label{sec:false_negative_proof}
 
\textbf{Note:} the symbols $c,C$ represent absolute constants. We will often abuse notation and ``absorb'' new absolute constants into $c, C$; the actual value of $c, C$ could thus vary from line to line.

 We first introduce notations.
 \subsubsection{Notation} 
\label{sec:false_negative_proof_notations}

Let $f : \mathbb{R}^s \rightarrow \R$, we denote $\| f \|_P \equiv \E f(X)^2$. \\
Given samples $X_1,...,X_n$, we denote $\| f \|_n \equiv \frac{1}{n} \sum_{i=1}^n f(X_i)^2$ and $\langle f, g \rangle_n \equiv \frac{1}{n} \sum_{i=1}^n f(X_i) g(X_i)$. \\

Let $\mathcal{C}^1$ denote the set of univariate convex functions supported on $[-b,b]$. Let $\mathcal{C}^1_L \equiv \{ f \in \mathcal{C}^1 \,:\, \| \partial f \|_\infty \leq L \}$ denote the set of $L$-Lipschitz univariate convex functions. \\

Define $\mathcal{C}^s$ as the set of convex additive functions and $\mathcal{C}^s_L$ likewise as the set of $L$-Lipschitz convex additive functions.
\begin{align*}
\mathcal{C}^s &\equiv \{ f \,:\, f = \sum_{k=1}^s f_k, \,
   f_k \in \mathcal{C}^1 \} \\
\mathcal{C}^s_L &\equiv \{ f \in \mathcal{C}^s \,:\, 
f = \sum_{k=1}^s f_k, \, \| \partial f_k \|_\infty \leq L \}
\end{align*}

Let $f^*(x) = \sum_{k=1}^s f^*_k(x_k)$ be the population risk minimizer:
\[
f^* = \arg\min_{f \in \mathcal{C}^s} \| f_0 - f^* \|_P^2
\]

We let $L$ be an upper bound on $\| \partial_{x_k} f_0 \|_\infty$ and $\| \partial f^*_k \|_\infty$. Since $f_0, f^*$ are supported on $[-b, b]^s$, it follows that $\| f_0 \|_\infty, \|f^* \|_\infty \leq s L b$.

We define $\hat{f}$ as the empirical risk minimizer:
\[
\hat{f} = \arg\min \Big \{ \| y - f \|_n^2 + \lambda \sum_{k=1}^s \| f_k \|_\infty 
    \,:\, f \in \mathcal{C}^s_L, \mathbf{1}_n^\tran f_k = 0 \Big \}
\]

For $k \in \{1,...,s\}$, define $g^*_k$ to be decoupled concave population risk minimizer
\[
g^*_k \equiv \argmin_{g_k \in \mh \mathcal{C}^1} \| f_0 - f^* - g_k \|_P^2 
\]
In our proof, we will analyze $g^*_k$ for $k$'s such that $f^*_k = 0$. Likewise, we define the empirical version:
\[
\hat{g}_k \equiv \argmin \Big\{ \| f_0 - \hat{f} - g_k \|_n^2 \,:\, g_k \in \mh \mathcal{C}^1_L \,, \mathbf{1}_n^\tran g_k = 0 \Big\}
\]
By the definition of the ACDC procedure, $\hat{g}_k$ exist only for $k$ that have zero in their convex additive approximation.


\subsubsection{Proof}
 
By additive faithfulness of the ACDC procedure, it is necessary that $f^*_k \neq 0$ or $g^*_k \neq 0$ for all $k \in S$. \\


Intuitively, we would like to show the following:
\begin{align*}
\| f_0 - \hat{f} \|_P & \approx \| f_0 - f^* \|_P \\
\| f_0 - f^* - \hat{g}_k \|_P & \approx \| f_0 - f^* - g^*_k \|_P 
       \quad \trm{for all $k \in S$ where $f^*_k = 0$}
\end{align*}
where the estimation error is a term that decreases with $n$.

Suppose $\hat{f}_k = 0$ and $f^*_k \neq 0$, then, when $n$ is large enough, there must exist a contradiction because the population risk of $f^*$, $\| f_0 - f^* \|_P$, is strictly larger than the population risk of the best approximation whose $k$-th component is constrained to be zero. 

Suppose $f^*_k = 0$, then $g^*_k \neq 0$. When $n$ is large enough, $\hat{g}_k$ must not be zero or we would have another contradiction. \\


\begin{theorem}
\label{thm:convex_consistent}
Let $\hat{f}$ be the minimizer of the restricted regression with $\lambda \leq sB \sqrt{ \frac{1}{n} \log^2 np}$.

Suppose $n \geq c_1 \max(\sqrt{sB}, B)$ and $\sigma \geq c_2$ for some constant $c_1, c_2 > 0$.

Then, with probability at least $1-\delta$,
\begin{align}
\|f_0 - \hat{f} \|_P^2 - \| f_0 - f^* \|_P^2 
&\leq c B^3\sigma \sqrt{ \frac{s^5}{n^{4/5}} \log^2 \frac{Cnp}{\delta}}
\end{align}
Where $c, C$ are constants, dependent on $c_1, c_2$.

\end{theorem}


\begin{proof}

\textbf{Step 1.} We start from the definition. 

\begin{align*}
\| y - \hat{f} \|_n^2 + \lambda \sum_{k=1}^s \| \hat{f}_k \|_\infty &\leq
  \| y - f^* + \bar{f}^* \|_n^2 + \lambda \sum_{k=1}^s \| f^*_k - \bar{f}^* \|_\infty 
\end{align*}
We plug in $y = f_0 + w$:
\begin{align*}
\| f_0 + w - \hat{f} \|_n^2 + \lambda \sum_{k=1}^s \Big( \| \hat{f}_k \|_\infty - 
    \| f^*_k - \bar{f}^*_k \|_\infty \Big) &\leq \|f_0 + w - f^* + \bar{f}^* \|_n^2 \\
\| f_0 - \hat{f} \|_n^2 + 2\langle w, f_0 - \hat{f} \rangle_n 
     +\lambda \sum_{k=1}^s \Big( \| \hat{f}_k \|_\infty - \|f^*_k -\bar{f}^*_k\|_\infty \Big) 
    &\leq \| f_0 - f^* + \bar{f}^* \|_n^2 + 
    2 \langle w, f_0 - f^* + \bar{f}^* \rangle \\
\|f_0 - \hat{f} \|_n^2 - \| f_0 - f^* + \bar{f}^* \|_n^2 + 
    \lambda \sum_{k=1}^s \Big( \| \hat{f}_k \|_\infty - 
 \| f^*_k - \bar{f}^*_k \|_\infty \Big) &\leq 2 \langle w, \hat{f} - f^* + \bar{f}^* \rangle
\end{align*}

The middle term can be bounded with the fact that $\|f^*_k - \bar{f}^*_k \|_\infty \leq 4B$.

\begin{align*}
\|f_0 - \hat{f} \|_n^2 - \| f_0 - f^* + \bar{f}^* \|_n^2 
   &\leq 2 \langle w, \hat{f} - f^* + \bar{f}^* \rangle + \lambda 4 s B 
\end{align*}

Using Lemma~\ref{lem:remove_centering}, we can remove $\bar{f}^*$ from the LHS. With probability at least $1 - \delta$:
\begin{align}
\label{eqn:first_step_inequality}
\|f_0 - \hat{f} \|_n^2 - \| f_0 - f^* \|_n^2 
   &\leq 2 \langle w, \hat{f} - f^* + \bar{f}^* \rangle + \lambda 4 s B + c(sB)^2 \frac{1}{n} \log \frac{2}{\delta}
\end{align}

%[TOOD: at this point, we still have to choose a value for $\lambda$]\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% Step 2
%%
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Step 2.} We upper bound $2 \langle w, \hat{f} - f^* + \bar{f}^* \rangle$ with bracketing entropy.

Define $\mathcal{G}$ as $\{ f - f^* + \bar{f}^* \,:\, f \in \mathcal{C}^s\}$ as the set of convex additive functions centered around the function $f^* - \bar{f}^*$. 

By Corollary~\ref{prop:convexbracket_lp}, there is an $\epsilon$-bracketing of $\mathcal{G}$ whose size is bounded by $\log N_{[]}( 2\epsilon, \mathcal{G}, L_1(P)) \leq sK^{**} \left( \frac{2sbB}{\epsilon} \right)^{1/2}$, for all $\epsilon \in (0, sB \epsilon_3]$.

By Corollary~\ref{cor:convexbracket_ln}, with probability at least $1-\delta$, each bracketing pair $(h_U, h_L)$ is close in $L_1(P_n)$ norm, i.e., for all $(h_U, h_L)$, 
$\frac{1}{n} \sum_{i=1}^n | h_U(X_i) - h_L(X_i) | \leq 2 \epsilon + sB \sqrt{ \frac{sK^{**}(2sbB
)^{1/2} + \log \frac{1}{\delta}}{2\epsilon^{1/2} n}}$.

For each $h \in \mathcal{G}$, there exists a pair $(h_U, h_L)$ such that $h_U(X_i) - h_L(X_i) \geq h(X_i) - h_L(X_i) \geq 0$. Therefore, with probability at least $1-\delta$, uniformly for all $h \in \mathcal{G}$:

$$
\frac{1}{n} \sum_{i=1}^n |h(X_i) - h_L(X_i)| \leq \frac{1}{n} \sum_{i=1}^n | h_U(X_i) - h_L(X_i)| \leq 2\epsilon +  (sB) \sqrt{ \frac{sK^{**}(2sbB)^{1/2} \log \frac{1}{\delta}}{2\epsilon^{1/2} n}}
$$.
Where we assumed that condition~\ref{cond:simplify_covering_number} holds. 

We denote $\epsilon_{n,\delta} \equiv (sB) \sqrt{ \frac{sK^{**}(2sbBc_u)^{1/2} \log \frac{1}{\delta}}{2\epsilon^{1/2} n}}$. 
Therefore,  
\[
|\langle w, h - h_L\rangle_n| \leq \| w \|_\infty \frac{1}{n} \sum_{i=1}^n |h(X_i) - h_L(X_i)| \leq
  \sigma \sqrt{\log \frac{n}{\delta}} \left( \epsilon + \epsilon_{n,\delta} \right)
\]
For the last inequality, we used the fact that $w$ is a vector of independent subgaussian($\sigma$) random variables and hence, by union bound, with probability at least $1-\delta$, $\| w \|_\infty \leq \sigma \sqrt{\log \frac{n}{\delta}}$. 

By another property of subgaussian random variables, with probability at least $1-\delta$, $|\langle w, h_L \rangle_n | \leq \| h_L \|_n \sigma \sqrt{ \frac{1}{cn} \log \frac{1}{\delta} }$. Applying an union bound, we have that 
$\sup_{h_L} |\langle w, h_L \rangle| \leq sB \sigma \sqrt{ \frac{\log N_{[]}}{n} \log \frac{1}{\delta}}$.

Putting this together, we have that
\begin{align*}
|\langle w, h \rangle_n | &\leq | \langle w, h_L\rangle_n| + |\langle w, h - h_L\rangle_n|\\
|\sup_{h \in \mathcal{G}} \langle w, h \rangle_n| &\leq 
     | \sup_{h^L} \langle w, h^L \rangle_n | + \sigma \sqrt{\log \frac{n}{\delta}} (\epsilon + \epsilon_{n, \delta}) \\
   &\leq   sB \sigma \sqrt{ \frac{ \log N_{[]} + \log \frac{1}{\delta}}{cn}} + \sigma \sqrt{\log \frac{n}{\delta}} (\epsilon + \epsilon_{n, \delta}) \\
   &\leq  sB \sigma \sqrt{ \frac{sK^{**} (2sbB)^{1/2} \log \frac{1}{\delta}}{cn \epsilon^{1/2}}} +
   \sigma \sqrt{ \log \frac{n}{\delta}} (\epsilon + \epsilon_{n, \delta}) \\
   &\leq sB \sigma \sqrt{ \frac{sK^{**} (2sbB)^{1/2} \log \frac{1}{\delta}}{cn \epsilon^{1/2}}} +
   \sigma\sqrt{\log \frac{n}{\delta}} \epsilon + sB \sigma \sqrt{ \frac{sK^{**} (2sbB)^{1/2} + \log \frac{1}{\delta}}{cn \epsilon^{1/2}} \log \frac{n}{\delta}} \\
   &\leq \sigma\sqrt{\log \frac{n}{\delta}} \epsilon + sB \sigma \sqrt{ \frac{sK^{**} (2sbB)^{1/2} \log^2 \frac{n}{\delta}}{cn \epsilon^{1/2}}} \\
\end{align*}

The two terms are balanced when one sets $\epsilon = sB \sqrt{ \frac{(s K^{**} (sBb)^{1/2})^{4/5}}{n^{4/5}} }$.  

This is only valid if $n$ is large enough so that $\epsilon \in (0, sB \epsilon_3]$.

Also, to ensure that conditions~\ref{cond:simplify_covering_number} hold, we need that $\log n \geq 2$ and $n \geq c B$ where $c$ is some constant dependent only on $K^{**}$ and $b$.

We upper bound some terms to simplify the presentations again and end up with the following result:

\[
|\sup_{h \in \mathcal{G}} \langle w, h \rangle | \leq sB \sigma \sqrt{ 
   \frac{s b^{1/2} \log^2 \frac{n}{\delta}}{c n^{4/5}}}
\]

% Then, $\| h \|_n \leq \| h \|_\infty \leq 4sLb$ for all $h \in \mathcal{G}$.\\

% According to theorem~\ref{thm:chaining}, for all $\epsilon > \frac{1}{\sqrt{n}}\sigma c \int_0^R \sqrt{ \log N_2(t, \mathcal{G}) }dt \vee R$,
% \[
% P\Big( \sup_{h \in \mathcal{G}} \langle w, h \rangle_n \geq \epsilon \Big) \leq
%   4 \exp \Big( - \frac{ n \epsilon^2}{ c R^2 \sigma^2} \Big)
% \]
% where $R = 4sLb$ for our purpose.

% Restated, we have that, with probablity at least $1-\delta$,
% \[
% \sup_{h \in \mathcal{G}} | \langle w, h \rangle_n | \leq 
%    c R \sigma \sqrt{ \frac{1}{n} \log \frac{4}{\delta}} + 
%       \Big( \int_0^R \sqrt{\log N_2(t, \mathcal{G})}dt \vee R \Big)\, 
%        c \sigma \sqrt{\frac{1}{n}} 
% \]

% Now we evaluate the integral. Since $N_{\|\cdot\|_n}(t, \mathcal{G}) \leq N_\infty(t, \mathcal{G})$, we know that $\sqrt{\log N_{\|\cdot\|_n}(t, \mathcal{G})} \leq \sqrt{C s^{1.5} b L} t^{-1/4}$.

% \begin{align*}
% \int_0^R \sqrt{\log N_{\|\cdot\|_n}(t, \mathcal{G})} dt &\leq 
%       \sqrt{C s^{1.5} b L} \int_0^R t^{-1/4} dt \\ 
%  &= \sqrt{C s^{1.5} b L} \, \frac{4}{3} R^{3/4} \\
%  &= \sqrt{C s^{1.5} b L} \, c (sLb)^{3/4} \\
%  &\leq c (s b L)^2
% \end{align*}

% Coming back, we have, with probability at least $1-\delta$,
% \begin{align*}
% \sup_{h \in \mathcal{G}} | \langle w, h \rangle | &\leq 
%    c sLb \sigma \sqrt{ \frac{1}{n} \log \frac{4}{\delta} } + 
%     c (sLb)^2 \sigma \sqrt{ \frac{1}{n} } \\
%  &\leq c (sLb)^2\sigma \sqrt{ \frac{1}{n} \log \frac{4}{\delta} }
% \end{align*}


Plugging this result into equation~\ref{eqn:first_step_inequality} and using an union bound, we get, with probability at least $1 - 2\delta$:
\begin{align}
\|f_0 - \hat{f} \|_n^2 - \| f_0 - f^* \|_n^2 
   &\leq c sB \sigma \sqrt{ 
   \frac{s b^{1/2} \log^2 \frac{n}{\delta}}{n^{4/5}}}
   + \lambda 4 s B + c (sB)^2 \frac{1}{n} \log \frac{2}{\delta} \nonumber\\
\|f_0 - \hat{f} \|_n^2 - \| f_0 - f^* \|_n^2 
   &\leq c sB \sigma \sqrt{ 
   \frac{s b^{1/2} \log^2 \frac{nC}{\delta}}{n^{4/5}}}
   + \lambda 4 s B \nonumber\\   
   &\leq c B \sigma 
    \sqrt{ \frac{s^3 b^{1/2}}{n^{4/5}} \log^2 \frac{n}{\delta}} + \lambda 4 sB
\label{eqn:second_step_inequality}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Step 3.
%%
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Step 3.} We continue from equation~\ref{eqn:second_step_inequality}, use lemma~\ref{lem:uniform_convergence}, use another union bound, with probability at least $1-3\delta$,

\begin{align}
\|f_0 - \hat{f} \|_P^2 - \| f_0 - f^* \|_P^2 
   &\leq cB^2 \sigma 
    \sqrt{ \frac{s^3 b^{2/5}}{n^{4/5}} \log^2 \frac{Cn}{\delta}}
 +\lambda 4 s B + c B^3 \sqrt{ \frac{s^5}{n^{4/5}} \log \frac{2}{\delta}}
    \nonumber \\
&\leq c B^3 \sigma \sqrt{ \frac{s^5 b^{2/5}}{n^{4/5}} \log^2 \frac{Cn}{\delta}} + \lambda 4 sB \nonumber
\end{align}
Where for the second inequality, we used the assumption that there is some large constant $c$ such that $B \geq \frac{\sigma}{c\sigma + 1}$.

Substituting in $\lambda \leq c sB \sqrt{\frac{1}{n} \log^2 np}$ and we get the desired result.

\end{proof}
 
%% End of "No False Negative" proof for the f's






\begin{theorem}
\label{thm:concave_consistent}
Let $\hat{g}_k$ denote the minimizer of the concave postprocessing with $\lambda \leq c sB \sqrt{\frac{1}{n} \log^2 np}$.\\

Suppose $n$ is large enough such that $B^3 \sigma \sqrt{ \frac{s^5}{n^{4/5}} \log^2 \frac{4np}{\delta} } \leq 1$.\\

Then, with probability at least $1- s\delta$, for all $k=1,...,s$:\\
\[
\| f_0 - f^* - \hat{g}_k \|_P^2 - \| f_0 - f^* - g^*_k \|_P^2 \leq  c (Lb)^{2.5} \sigma^{0.5} \sqrt[4]{ \frac{s^5}{n^{4/5}} \log^2 \frac{4np}{\delta}} 
\]
\end{theorem}

\begin{proof}
The proof proceeds almost identically to that of theorem~\ref{thm:convex_consistent} because convex and concave functions have the same bracketing number.

\textbf{Step 1.} We start from the definition of $\hat{g}_k$:
\begin{align*}
\| y - \hat{f} - \hat{g}_k \|_n^2 + \lambda \| \hat{g} \|_\infty &\leq
   \| y - \hat{f} - g^*_k \|_n^2 + \lambda \| g^* \|_\infty \\
\| y - \hat{f} - \hat{g}_k \|_n^2 &\leq \| y - \hat{f} - g^*_k \|_n^2 + \lambda 2B \\
 &\\
\| f_0 - \hat{f} - \hat{g}_k + w\|_n^2 & \leq \| f_0 - \hat{f} - g^*_k + w \|_n^2 
   +\lambda 2 B \\
\| f_0 - \hat{f} - \hat{g}_k \|_n^2 - \|f_0 -\hat{f} - g^*_k\|_n^2 &\leq
   2 \langle w, \hat{g}_k - g^*_k \rangle_n + \lambda 2B
\end{align*}

Using identical analysis as Step 2 of the proof of Theorem~\ref{thm:convex_consistent} but setting $s=1$, we have, with probability at least $1-\delta$,
\begin{align*}
\| f_0 - \hat{f} - \hat{g}_k \|_n^2 - \|f_0 - \hat{f} - g^*_k \|_n^2 &\leq
  c B^2 \sigma \sqrt{ \frac{b^{1/2}}{n^{4/5}} \log \frac{C}{\delta} }+ \lambda 2 B
\end{align*}

Using the uniform convergence result (lemma~\ref{lem:uniform_convergence}), we have, with probability at least $1-2\delta$:
\begin{align*}
\| f_0 - \hat{f} - \hat{g}_k \|_P^2 - \|f_0 - \hat{f} - g^*_k \|_P^2 &\leq
  c B^2 \sigma \sqrt{ \frac{1}{n} \log \frac{4}{\delta} }+ \lambda 2 Lb +
  c B^3 \sqrt{\frac{s^5}{n^{4/5}} \log \frac{2}{\delta} } \\
 &\leq c (Lb)^3\sigma \sqrt{\frac{s^5}{n^{4/5}} \log \frac{4}{\delta}}+ \lambda 2 Lb
\end{align*}

Plugging in $\lambda \leq \sqrt{ \frac{1}{n} \log^2 np}$:
\begin{align*}
\| f_0 - \hat{f} - \hat{g}_k \|_P^2 - \|f_0 - \hat{f} - g^*_k \|_P^2 &
\leq c B^3\sigma \sqrt{\frac{s^5}{n^{4/5}} \log \frac{4}{\delta}}+ 
    2Lb \sqrt{\frac{1}{n} \log^2 np}\\
\| f_0 - \hat{f} - \hat{g}_k \|_P^2 - \|f_0 - \hat{f} - g^*_k \|_P^2 &
\leq c B^3\sigma \sqrt{\frac{s^5}{n^{4/5}} \log^2 \frac{4np}{\delta}} \\
\end{align*}

\textbf{Step 2.} The goal is to bound the quality of approximation between $\| f_0 - \hat{f} - g^*_k \|_P^2$ and $\| f_0 - f^* - g^*_k \|_P^2$ and likewise for $\hat{g}_k$.

\begin{align*}
\| f_0 - \hat{f} - g^*_k \|_P^2 - \| f_0 - f^* - g^*_k\|_P^2 &\leq 
    \| f_0 - \hat{f} \|_P^2 - \|f_0 - f^*\|_P^2 - 2\langle f_0 - \hat{f}, g^*_k \rangle
   + 2 \langle f_0 - f^*, g^*_k \rangle \\
 &\leq c B^3 \sigma \sqrt{ \frac{s^5}{n^{4/5}} \log^2 \frac{4np}{\delta}} + 
    2 | \langle \hat{f} - f^*, g^*_k \rangle |  \\
 &\leq  c B^3 \sigma \sqrt{ \frac{s^5}{n^{4/5}} \log^2 \frac{4np}{\delta}} +
    2 \| \hat{f} - f^* \|_P \| g^*_k \|_P \\
&\leq  c B^3 \sigma \sqrt{ \frac{s^5}{n^{4/5}} \log^2 \frac{4np}{\delta}} +
   c B \sqrt{B^3\sigma \sqrt{ 
                   \frac{s^5}{n^{4/5}} \log^2 \frac{4np}{\delta}} }\\
&\leq  cB \sqrt{B^3\sigma \sqrt{ 
                   \frac{s^5}{n^{4/5}} \log^2 \frac{4np}{\delta}} }\\
\end{align*}

The same bound likewise holds for $\hat{g}_k$.

\textbf{Step 3.} Collecting the results from Step 1 and Step 2, we have, with probability at least $1-2\delta$:
\begin{align*}
\| f_0 - f^* - \hat{g}_k \|_P^2 - \|f_0 - f^* - g^*_k \|_P^2 \leq
   c B^{2.5} \sigma^{0.5} 
     \sqrt[4]{ \frac{s^5}{n^{4/5}} \log^2 \frac{4np}{\delta}} 
\end{align*}

Taking an union bound across $k=1,...,s$ dimensions completes the result.

\end{proof}








\subsubsection{Support Lemmas}

%%%%%%%%%%
%% Uniform convergence lemma
%%
%%
%%
%%
%%%%%%%%%%%%%%%%%%%%

\begin{lemma}
\label{lem:uniform_convergence}
With probability at least $1-\delta$:
\begin{align*}
\sup_{f \in \mathcal{C}^s_L} \Big| \| f_0 - f \|^2_n - \|f_0 - f \|^2_P\Big| \leq
   c B^2 \sqrt{ \frac{s^5b^{1/2}}{n^{4/5}} \log \frac{2}{\delta}}
\end{align*}
\end{lemma}

\begin{proof}
Let $\mathcal{G}$ denote the off-centered set of convex functions, that is, $\mathcal{G} \equiv \mathcal{C}^s - f_0$. Note that if $h \in \mathcal{G}$, then $\| h \|_\infty = \| f_0 - f \|_\infty \leq 4 s B$.\\

There exists an $\epsilon$-bracketing of $\mathcal{G}$. 

%For every $h \in \mathcal{G}$, there exists $h^L$ in the bracketing such that $\| h^U - h^L \|_{L_1(P)} \leq \epsilon$. 

By Corollary~\ref{cor:convexadditive_lp}, the bracketing has size at most $\log N_{[]}(2\epsilon, \mathcal{C}^s, L_1(P)) \leq s K^{**}\left( \frac{2sbB}{\epsilon} \right)^{1/2}$. By Corollary~\ref{cor:convexbracket_ln}, we know that with probability at least $1-\delta$, we have that $\|h_U - h_L\|_{L_1(P_n)} \leq \epsilon + \epsilon_{n,\delta}$ for all pairs $(h_U, h_L)$ in the bracketing, where $\epsilon_{n,\delta} = sB \sqrt{ \frac{K^{**} (2sbB)^{1/2} \log \frac{2}{\delta}}{2 \epsilon^{1/2} n}}$.

For a particular function $h \in \mathcal{G}$, we can construct $\psi_L \equiv \min( |h_U|, |h_L|)$ and $\psi_U \equiv \max( |h_U|, |h_L| )$ so that
\[
\psi_L^2 \leq h^2 \leq \psi_U^2
\]

We can then bound the $L_1(P)$ norm of $\psi_U^2 - \psi_L^2$.
\begin{align*}
\int (\psi_U^2(x) - \psi_L^2(x)) p(x)dx  &\leq  \int | h_U^2(x) - h_L^2(x)| p(x) dx \\
   &\leq \int | h_U(x) - h_L(x) | \, |h_U(x) + h_L(x)| p(x) dx \\
   &\leq 2sB \epsilon
\end{align*}

% And, with probability at least $1-\delta$, for all $\psi_U, \psi_L$:
% \begin{align*}
% \frac{1}{n} \sum_{i=1}^n ( \psi_U^2(X_i) - \psi_L^2(X_i)) &\leq 
%          \frac{1}{n} \sum_{i=1}^n | h_U^2(X_i) - h_L^2(X_i)| \\
%   &\leq \frac{1}{n}\sum_{i=1}^n | h_U(X_i) - h_L(X_i)| \, |h_U(X_i) + h_L(X_i)| \\
%   &\leq 2sB(\epsilon + \epsilon_{n,\delta})
% \end{align*}

Now we can bound $\| h \|_n^2 - \| h \|_P^2$.

\begin{align}
\frac{1}{n} \sum_{i=1}^n \psi_L(X_i)^2 - \E \psi_U(X)^2  \leq
    \| h \|_n^2 - \| h \|_P^2 \leq
  \frac{1}{n} \sum_{i=1}^n \psi_U(X_i)^2 - \E \psi_L(X)^2  \label{eqn:hpsi_bound}
\end{align}

$\psi_L(X_i)^2$ and $\psi_U(X_i)^2$ are bounded random variables with upper bound $sB$. By Hoeffding's inequality and union bound, we have that, with probability at least $1-\delta$,, for all $\psi_L$ (and likewise $\psi_U$):
\[
\left| \frac{1}{n} \sum_{i=1}^n \psi_L(X_i)^2 - 
   \E \psi_L(X)^2 \right| \leq (sB)^2 \sqrt{ \frac{ sK^{**} (sBb)^{1/2} \log \frac{2}{\delta}}{ \epsilon^{1/2} n} }
\]

Plugging this into equation~\ref{eqn:hpsi_bound} above, we have that:
\begin{align*}
& \E \psi_L(X)^2 - \E \psi_U(X)^2 - 
(sB)^2 \sqrt{ \frac{ sK^{**} (sBb)^{1/2} \log \frac{2}{\delta}}{ \epsilon^{1/2} n} } \\
 & \leq 
 \| h \|_n^2 - \| h \|_P^2 \leq
\E \psi_U(X)^2 - \E \psi_L(X)^2 + 
(sB)^2 \sqrt{ \frac{ sK^{**} (sBb)^{1/2} \log \frac{2}{\delta}}{ \epsilon^{1/2} n} }
\end{align*}

Using the $L_1(P)$ norm of $\psi_U^2 - \psi_L^2$ result, we have:
\begin{align*}
-sB\epsilon - 
(sB)^2 \sqrt{ \frac{ sK^{**} (sBb)^{1/2} \log \frac{2}{\delta}}{ \epsilon^{1/2} n} } \leq 
 \| h \|_n^2 - \| h \|_P^2 \leq
sB\epsilon + 
(sB)^2 \sqrt{ \frac{ sK^{**} (sBb)^{1/2} \log \frac{2}{\delta}}{ \epsilon^{1/2} n} }
\end{align*}

We balance the terms by choosing $\epsilon = \left( \frac{ (sB)^2 sK^{**} (sBb)^{1/2}}{n} \right)^{2/5}$.

We have then that, with probability at least $1-\delta$,
\begin{align*}
\big| \| h \|_n^2 - \| h \|_P^2  \big| \leq
  B^2 \sqrt{ \frac{s^5 b^{1/2} \log \frac{2}{\delta}}{n^{4/5}}}
\end{align*}

%For a $h \in \mathcal{G}$, let $h_\epsilon$ denote a function in the $\epsilon$-bracketing of $\mathcal{G}$ closest to $h$. It obviously must be that $\| h - h_\epsilon \|_n \leq \| h - h_\epsilon \|_\infty \leq \epsilon$.\\

% Because $\| h \|_n = \| h - h^L + h^L \|_n$, we have that
% \begin{align*}
% \|h^L \|_n - \| h - h^L \|_n &\leq 
%     \| h \|_n \leq \|h^L \|_n + \| h - h^L \|_n \\
% \|h^L \|_n - \epsilon - \epsilon_{n,\delta} &\leq 
%     \| h \|_n \leq \|h^L \|_n + \epsilon + \epsilon_{n,\delta}  \\
% \| h^L \|^2_n - 8 \epsilon (sB)  &\leq \| h  \|^2_n  \leq
%    \| h^L \|_n^2 + 8 \epsilon (sB) 
% \end{align*}
% where we used the fact that $\| h - h^L \|_n \leq \| h - h^L \|_\infty \leq \epsilon$ and $ \| h^L \|_n \leq \| h^L \|_\infty \leq 4 s L b$.


% And likewise:
% \begin{align*}
% \| h_\epsilon \|^2_P - 8 \epsilon (s L b)  &\leq \| h  \|^2_P \leq
%    \| h_\epsilon \|_P^2 + 8 \epsilon (s L b)
% \end{align*} 

% Therefore, 
% \begin{align*}
% \sup_{h \in \mathcal{G}}  \Big| \|h\|^2_n - \|h\|^2_P \Big| \leq 
% \sup_{h_\epsilon} \Big| \|h_\epsilon\|^2_n - \|h_\epsilon\|^2_P \Big| 
%         + \epsilon (16 sLb)
% \end{align*}

% Since $\| h_\epsilon \|_n^2 = \frac{1}{n} \sum_{i=1}^n h_\epsilon(X_i)^2$ is an average of bounded random variables, we have by Union Bound and Hoeffding Inequality that, with probability at most $1 - \delta$,
% \begin{align*}
% \sup_{h_\epsilon} \Big| \|h_\epsilon\|^2_n - \|h_\epsilon\|^2_P \Big| &\leq
%   (8sLb)^2 \sqrt{ \frac{1}{cn} \big(\log \frac{2}{\delta} 
%         + \log N_\infty(\epsilon, \mathcal{C}_L^s) \big) } \\
% &\leq (8sLb)^2 \sqrt{ \frac{1}{cn} \big(\log \frac{2}{\delta} 
%         + C s^{1.5} Lb \epsilon^{-1/2}   \big) }
% \end{align*}

% We will set $\epsilon = \frac{1}{n^{2/5}} (C s^{0.5} L b)^2$. 
% Therefore:
% \begin{align*}
% \sup_{h_\epsilon} \Big|  \|h_\epsilon\|^2_n - \|h_\epsilon\|^2_P \Big| &\leq
%    (8sLb)^2 \sqrt{ \frac{1}{cn} \big(\log \frac{2}{\delta} 
%         + s n^{1/5}  \big) } \\
%   &\leq (8Lb)^2 \sqrt{ \frac{s^5}{cn^{4/5}} \log \frac{2}{\delta} }
% \end{align*}
% And
% \begin{align*}
% \sup_{h \in \mathcal{G}}  \Big| \|h\|^2_n - \|h\|^2_P \Big| &\leq 
% \sup_{h_\epsilon} \Big| \|h_\epsilon\|^2_n - \|h_\epsilon\|^2_P \Big| 
%         + \frac{1}{n^{2/5}} C^2 s^2 (Lb)^3 \\
%   &\leq (8Lb)^2 \sqrt{ \frac{s^5}{c n^{4/5}} \log \frac{2}{\delta}}
%      + (CLb)^2 \sqrt{ \frac{s^4}{n^{4/5}} } \\
%   &\leq c (Lb)^3 \sqrt{ \frac{s^5}{n^{4/5}} \log \frac{2}{\delta}}
% \end{align*}

\end{proof}



\begin{lemma}
\label{lem:remove_centering}

Let $f_0, f^*$ be defined as in section~\ref{sec:false_negative_proof_notations}. Define $\bar{f}^* = \frac{1}{n} \sum_{i=1}^n f^*(X_i)$.

Then, with probability at least $1 - 2\delta$,
\[
\Big | \| f_0 - f^* \|_n^2 - \| f_0 - f^* + \bar{f}^* \|_n^2 \Big| \leq
    c (sLb)^2 \frac{1}{n} \log \frac{4}{\delta}
\]
\end{lemma}

\begin{proof} (of lemma~\ref{lem:remove_centering})
\begin{align*}
\| f_0 - f^* + \bar{f}^* \|_n^2 &= \| f_0 - f^* \|_n^2 
    + 2 \langle f_0 - f^*, \bar{f}^* \rangle + \bar{f}^{*2} \\
  &= \| f_0 - f^* \|_n^2 + 2 \bar{f}^* \langle f_0 - f^*, \mathbf{1} \rangle_n + 
    \bar{f}^{*2} \\
  &= \| f_0 - f^* \|_n^2 + 2 \bar{f}^* \bar{f}_0 - \bar{f}^{*2}
\end{align*}


$\bar{f}^* = \frac{1}{n} \sum_{i=1}^n f^*(X_i)$ is the average of $n$ bounded mean-zero random variables and therefore, with probability at least $1-\delta$, $| \bar{f}^* | \leq 4 s L b \sqrt{ \frac{1}{n} \log \frac{2}{\delta} }$.

The same reasoning likewise applies to $\bar{f}_0 = \frac{1}{n} \sum_{i=1}^n f_0(X_i)$.

Taking a union bound and we have that, with probability at least $1- 2\delta$, 

\begin{align*}
| \bar{f}^* | | \bar{f}_0 | &\leq c (sB)^2 \frac{1}{n} \log \frac{2}{\delta} \\
\bar{f}^{*2} &\leq c (sB)^2 \frac{1}{n} \log \frac{2}{\delta}
\end{align*}

Therefore, with probability at least $1 - 2\delta$,
\[
\|f_0 - f^*\|_n^2 - c (sB)^2 \frac{1}{n} \log \frac{2}{\delta} \leq
    \| f_0 - f^* + \bar{f}^* \|_n^2 \leq 
\|f_0 - f^*\|_n^2 + c (sB)^2 \frac{1}{n} \log \frac{2}{\delta}
\]

\end{proof}




%%%%%%%%%%%%%
%% Technical Material
%%
%%
%%
%%
%%%%%%%%%%%%%%%%%%%% 

 \subsection{Supporting Technical Material}
 
 \subsubsection{Concentration of Measure}

\textbf{Sub-Exponential} random variable is the square of a subgaussian random variable\cite{vershynin2010introduction}.

\begin{proposition} (Subexponential Concentration \cite{vershynin2010introduction})
Let $X_1,...,X_n$ be zero-mean independent subexponential random variables with subexponential scale $K$. 
\[
P( | \frac{1}{n} \sum_{i=1}^n X_i | \geq \epsilon) \leq
	2 \exp \left[ -c n \min\left( \frac{\epsilon^2}{K^2}, \frac{\epsilon}{K} \right) \right]
\]
where $c > 0$ is an absolute constant.
\end{proposition}

For uncentered subexponential random variables, we can use the following fact. If $X_i$ subexponential with scale $K$, then $X_i - \E[X_i]$ is also subexponential with scale at most $2K$.

\textbf{Restating}. We can set
\[
c \min\left( \frac{\epsilon^2}{K^2}, \frac{\epsilon}{K} \right) = \frac{1}{n} \log \frac{1}{\delta}.
\]
Thus, with probability at least $1-\delta$, the deviation at most
\[
K \max\left( \sqrt{\frac{1}{cn} \log \frac{C}{\delta}},  \frac{1}{cn} \log \frac{C}{\delta} \right)
\]


\begin{corollary}
Let $w_1,...,w_n$ be $n$ independent subgaussian random variables with subgaussian scale $\sigma$. 

Then, for all $n > n_0$, with probability at least $1- \frac{1}{n}$,
\[
\frac{1}{n} \sum_{i=1}^n w_i^2 \leq c \sigma^2 
\]
\end{corollary}

\begin{proof}
Using the subexponential concentration inequality, we know that, with probability at least $1-\frac{1}{n}$, 

\[
| \frac{1}{n} \sum_{i=1}^n w_i^2 - \E w^2 | \leq \sigma^2 \max\left( \sqrt{\frac{1}{cn} \log \frac{C}{\delta}}, \frac{1}{cn}\log \frac{C}{\delta} \right)
\]

First, let $\delta = \frac{1}{n}$. Suppose $n$ is large enough such that $ \frac{1}{cn} \log Cn < 1$. Then, we have, with probability at least $1-\frac{1}{n}$,
\begin{align*}
 \frac{1}{n} \sum_{i=1}^n w_i^2 &\leq c\sigma^2 \Big(1+\sqrt{\frac{1}{cn} \log Cn}\Big) \\
		&\leq 2 c \sigma^2
 \end{align*}
 
\end{proof}



\subsubsection{Sampling Without Replacement}

\begin{lemma} (Serfling \cite{serfling1974probability}) 
Let $x_1,..., x_N$ be a finite list, $\bar{x} = \mu$. Let $X_1,...,X_n$ be sampled from $x$ without replacement. 

Let $b = \max_i x_i$ and $a = \min_i x_i$. Let $r_n = 1- \frac{n-1}{N}$. Let $S_n = \sum_i X_i$.
Then we have that
\[
P( S_n - n \mu \geq n \epsilon) \leq \exp( - 2 n \epsilon^2 \frac{1}{r_n (b-a)^2})
\]
\end{lemma}

\begin{corollary}
\label{cor:serfling}
Suppose $\mu = 0$. 
\[
P( \frac{1}{N} S_n \geq \epsilon) \leq \exp( -2 N \epsilon^2 \frac{1}{(b-a)^2})
\]

And, by union bound, we have that
\[
P( | \frac{1}{N} S_n| \geq \epsilon) \leq 2 \exp( -2 N \epsilon^2 \frac{1}{(b-a)^2})
\]

\end{corollary}

A simple restatement. With probability at least $1- \delta$, the deviation $| \frac{1}{N} S_n|$ is at most $ (b-a) \sqrt{ \frac{1}{2N} \log \frac{2}{\delta}}$.

\begin{proof}
\[
P( \frac{1}{N} S_n \geq \epsilon) = P( S_n \geq \frac{N}{n} n \epsilon) \leq \exp( - 2 n \frac{N^2}{n^2} \epsilon^2 \frac{1}{r_n (b-a)^2} ) 
\]

We note that $r_n \leq 1$ always, and $n \leq N$ always. 
\[
\exp( - 2 n \frac{N^2}{n^2} \epsilon^2 \frac{1}{r_n (b-a)^2} )  \leq \exp( - 2 N \epsilon^2 \frac{1}{(b-a)^2})
\]
This completes the proof.

\end{proof}

\subsubsection{Bracketing Number for Convex Functions}

\begin{definition}
% upper, lower, epsilon room
% metric
% function space
Let $\mathcal{C}$ be a set of functions. For a given $\epsilon$ and metric $\rho$ (which we take to be $L_2$ or $L_2(P)$), we define a \textbf{bracketing} of $\mathcal{C}$ to be a set of pairs of functions $\{ (f_L, f_U) \}$ satisfying (1) $\rho( f_L, f_U) \leq \epsilon$ and (2) for any $f \in \mathcal{C}$, there exist a pair $(f_L, f_U)$ where $f^U \geq f \geq f^L$. 

We let $N_{[]}(\epsilon, \mathbf{C}, \rho)$ denote the size of the smallest bracketing of $\mathcal{C}$
\end{definition}

\begin{proposition} (Proposition 16 in \cite{kim2014global})\\
\label{prop:convexbracket}
Let $\mathcal{C}$ be the set of convex functions supported on $[-b, b]^d$ and uniformly bounded by $B$. Then there exist constants $\epsilon_3$ and $K^{**}$, dependent on $d$, such that
\[
\log N_{[]} (2\epsilon, \mathcal{C}, L_2) \leq K^{**} \left( \frac{2bB}{\epsilon} \right)^{d/2}
\]
for all $\epsilon \in (0, B \epsilon_3]$.
\end{proposition}

It is trivial to extend Kim and Samworth's result to $L_2(P)$ norm for an absolutely continuous distribution $P$.

\begin{proposition}
\label{prop:convexbracket_lp}
Let $P$ be a distribution with a density $p$. Let $\mathcal{C}, b, B, \epsilon_3, K^{**}$ be defined as in Proposition~\ref{prop:convexbracket}. Then,
\[
\log N_{[]} (2\epsilon, \mathcal{C}, L_1(P)) \leq K^{**} \left( \frac{2bB}{\epsilon} \right)^{d/2}
\]
For all $\epsilon \in (0, B\epsilon_3]$.
\end{proposition}

\begin{proof}
Let $\mathcal{C}_\epsilon$ be the bracketing the satisfies the size bound in Proposition~\ref{prop:convexbracket_lp}. 

Let $(f_L, f_U) \in \mathcal{C}_\epsilon$. Then we have that:
\begin{align*}
\| f_L - f_U \|_{L_1(P)} &= \int | f_L(x) - f_U(x)| p(x) dx \\
   &\leq \left( \int | f_L(x) - f_U(x) |^2 dx \right)^{1/2}
      \left( \int p(x)^2 dx \right)^{1/2} \\
  &\leq \left( \int | f_L(x) - f_U(x)|^2 dx \right)^{1/2}\\
 &\leq \| f_L - f_U \|_{L_2} \leq \epsilon
\end{align*}
On the third line, we used the fact that $\int p(x)^2 dx \leq \left( \int p(x) dx \right)^2 \leq 1$.
\end{proof}

It is also simple to extend the bracketing number result to additive convex functions. As before, let $\mathcal{C}^s$ be the set of additive convex functions with $s$ components.

\begin{corollary}
\label{cor:convexadditive_lp}
Let $P$ be a distribution with a density $p$ upper bounded by $c_u$. Let $b, B, \epsilon_3, K^{**}$ be defined as in Proposition~\ref{prop:convexbracket}. Then,
\[
\log N_{[]}(2\epsilon, \mathcal{C}^s, L_1(P)) \leq s K^{**} 
    \left( \frac{2sbB}{\epsilon} \right)^{1/2}
\]
For all $\epsilon \in (0, s B \epsilon_3]$.
\end{corollary}

\begin{proof}
Let $f \in \mathcal{C}^s$. We can construct an $\epsilon$-bracketing for $f$ through $\epsilon/s$-bracketings for each of the components $\{ f_k \}_{k=1,...,s}$:
\[f_U = \sum_{k=1}^s f_{Uk}  \qquad f_L = \sum_{k=1}^s f_{Lk} \]
It is clear that $f_U \geq f \geq f_L$. It is also clear that $\| f_U - f_L \|_{L_1(P)} \leq \sum_{k=1}^s \| f_{Uk} - f_{Lk} \|_{L_1(P)} \leq \epsilon$.
\end{proof}


The following result follows from Corollary~\ref{prop:convexbracket_lp} directly by an union bound. 

\begin{corollary}
\label{cor:convexbracket_ln}
Let $X_1,...,X_n$ be random samples from a distribution $P$. Let $1 > \delta > 0$. Let $\mathcal{C}^s_\epsilon$ be an $\epsilon$-bracketing of $\mathcal{C}^s$ with respect to the $L_1(P)$-norm whose size is at most $N_{[]}( 2\epsilon, \mathcal{C}^s, L_1(P))$. Let $\epsilon \in (0, s B \epsilon_3]$.

Then, with probability at least $1-\delta$, for all pairs $(f_L, f_U) \in \mathcal{C}^s_\epsilon$, we have that

\begin{align*}
\frac{1}{n} \sum_{i=1}^n |f_L(X_i) - f_U(X_i)| &\leq \epsilon + \epsilon_{n, \delta}
\end{align*}

where 
$\epsilon_{n,\delta} \equiv 
sB \sqrt{ \frac{ \log N_{[]}(2\epsilon, \mathcal{C}^s, L_2(P)) + \log \frac{1}{\delta}}{2n}} 
= \sqrt{ \frac{ sK^{**}(sBb)^{1/2}}{2\epsilon^{1/2}n} + \frac{1}{2n} \log \frac{1}{\delta}}$
\end{corollary}

\begin{proof}
$|f_L(X_i) - f_U(X_i)|$ is at most $sB$. There are $N_{[]}(2\epsilon, \mathcal{C}^s, L_1(P))$ pairs $(f_L, f_U)$. The inequality follows from a direct application of union bound and Hoeffding Inequality.\\
\end{proof}

To make the expression in this corollary easier to work with, we derive an upper bound for $\epsilon_{n, \delta}$. Suppose 
\begin{align}
\epsilon^{1/2} \leq 2sK^{**} (sBb)^{1/2} \quad \trm{and} \quad \log \frac{1}{\delta} \geq 2. \label{cond:simplify_covering_number}
\end{align}

Then we have that
\begin{align*}
\epsilon_n \leq sB \sqrt{ \frac{ sK^{**} (sBb)^{1/2} \log \frac{1}{\delta}}{\epsilon^{1/2}n}}
\end{align*}

% \subsubsection{Covering Number for Lipschitz Convex Functions}

% \begin{definition}
% $\{ f_1,..., f_N\} \subset \mathcal{C}[b,B,L]$ is an $\epsilon$-covering of $\mathcal{C}[b,B,L]$ if for all $f \in \mathcal{C}[b,B,L]$, there exist $f_i$ such that $\| f - f_i \|_\infty \leq \epsilon$.

% We define $N_\infty( \epsilon, \mathcal{C}[b,B,L])$ as the size of the minimum covering.
% \end{definition}

% \begin{lemma} (Bronshtein 1974)
% \[
% \log N_\infty (\epsilon, \mathcal{C}[b,B,L]) \leq C\left( \frac{bBL}{\epsilon} \right)^{1/2}
% \]
% For some absolute constant $C$.
% \end{lemma}

% \begin{lemma}
% \[
% \log N_\infty( \epsilon, \mathcal{C}^s[b,B,L])  \leq C s \left(\frac{bBLs}{\epsilon}\right)^{1/2}
% \]
% For some absolute constant $C$.
% \end{lemma}

% \begin{proof}
% Let $f = \sum_{k=1}^s f_k$ be a convex additive function. Let $\{ f'_k \}_{k=1,..,s}$ be $k$ functions from a $\frac{\epsilon}{s}$ $L_\infty$ covering of $\mathcal{C}[b,B,L]$. 

% Let $f' \coloneqq \sum_{k=1}^s f'_k$, then 
% \[
% \| f' - f \|_{\infty} \leq \sum_{k=1}^s \| f_k - f'_k \|_\infty \leq s \frac{\epsilon}{s} \leq \epsilon
% \]

% Therefore, a product of $s$ $\frac{\epsilon}{s}$-coverings of univariate functions induces an $\epsilon$-covering of the additive functions.
% \end{proof}

% In our proofs, we will use the Dudley's chaining: [TODO:cite van de geer]

% \begin{theorem} (Dudley's Chaining) \\
% \label{thm:chaining}
% Let $\mathcal{G} = \{ g : \| g \|_n \leq R \}$. Let $M( \epsilon, R)$ be the size of the minimal $\epsilon$-covering of $\mathcal{G}$ with respect to the $\| \cdot \|_n$ norm. Suppose $w = (w_1, ..., w_n)$ is a vector of i.i.d. subgaussian random variables with scale $\sigma$.\\

% Suppose $\delta > 0$ is such that
% \[
% \sqrt{n} \delta \geq \sigma \Big( 
%    14 \sum_{s=0}^\infty 2^{-s} \sqrt{ \log M( 2^{-s} R, \mathcal{G})} 
%       \Big)\vee 70 \log 2 R
% \]
% Then we have that 
% \[
% P\Big( \sup_{g \in \mathcal{G}} \langle w, g \rangle_n \geq \delta \Big) \leq
%   4 \exp \Big( - \frac{n \delta^2}{(70R)^2\sigma^2} \Big)
% \]
% \end{theorem}

% For convenience, we can upper bound the metric-entropy sum with an integral: \\
% $\ds \sum_{s=0}^\infty 2^{-s} \sqrt{ \log M( 2^{-s} R, \mathcal{G})} 
%   \leq \int_0^R \sqrt{\log M(t, \mathcal{G}) } dt
% $




% DO NOT CHANGE; RefTex variables -minx
 
%%% Local Variables: ***
%%% mode:latex ***
%%% TeX-master: "faithful.tex" ***
%%% End: ***

