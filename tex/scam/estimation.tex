\def\C{\mathcal{C}}

\subsection{Estimation Procedure}
\label{sec:acdc}

Theorem~\ref{thm:acdc_faithful} naturally suggests 
a two-stage screening procedure for variable selection in the population setting. In the first stage, we fit a convex additive model. 
\begin{equation}
\label{eqn:scam2_pop}
f^*_1, ..., f^*_p = \argmin_{f_1,...,f_p \in \C^1_0, \mu} 
   \E \Big( f(X) - \mu - \sum_{k=1}^p f_k(X_k) \Big)^2 
\end{equation}
where we denote $\C^1_0$ ($\mh{}\C^1_0$) as the set of one-dimensional convex (resp. concave) functions with population mean zero. In the second stage, for every variable marked as irrelevant in the first stage, we fit a univariate \emph{concave} function separately on the residual for that variable.
 For each $k$ such that $ f^*_k = 0$:
\begin{equation}
\label{eqn:dc2}
g^*_k = \argmin_{g_k \in \mh{}\C^1_0} 
   \E \Big( f(X) - \mu^* - \sum_{k'} f^*_{k'}(X_{k'}) 
    - g_k(X_{k})\Big)^2 
\end{equation}
We screen out $S^C$, any variable $k$ that is zero after the second stage, and output $S$.
\begin{equation}
\label{eqn:acdc_vars_pop}
S^c = \bigl\{k : f^*_k =
0 \; \mathrm{and}\; g^*_k =0\bigr\}.
\end{equation}

We refer to this procedure as AC/DC (additive convex/decoupled
concave). Theorem~\ref{thm:acdc_faithful} guarantees that the true set
of relevant variables $S_0$ must be a subset of $S$.

% More precisely, given samples  
% $(\mathbf{x}_1, y_1), ..., (\mathbf{x}_n, y_n)$, 
% we perform the following steps.
% \begin{enumerate}
% \item {\it AC Stage}: Estimate an additive convex model
% \begin{equation}
% \label{eqn:scam}
% \hat{f}_1, ..., \hat{f}_p, \hat \mu  = \argmin_{f_1,...,f_p \in
%   \C^1_0, \,\mu\in\reals} 
%    \frac{1}{n} \sum_{i=1}^n \Big(y_i - \mu - \sum_{k=1}^p f_k(x_{ik}) \Big)^2 
%        + \lambda \sum_{k=1}^p \| f_k \|_\infty.
% \end{equation}
% \item {\it DC Stage}: For each $k$ such that $\| \hat{f}_k \|_\infty = 0$, estimate
%   a concave function on the residual:
% \begin{equation}
% \label{eqn:dc}
% \hat{g}_k = \argmin_{g_k \in \mh{}\C^1_0} 
%    \frac{1}{n} \sum_{i=1}^n \Big( y_i - \hat \mu - \sum_{k'} \hat{f}_{k'}(x_{ik'}) 
%     - g_k(x_{ik})\Big)^2 
%       + \lambda \| g_k \|_\infty.
% \end{equation}
% \item Output as the set of relevant variables
% $\hat S = \{ k \,:\, \| \hat{f}_k \|_\infty > 0 
%   \textrm{ or } \| \hat{g}_k \|_\infty > 0 \}$. 
% \end{enumerate}

%For identifiability, we impose the constraint $\sum_{i=1}^n f_k(x_{ik}) =
%0$ for each $k$.  
It is straightforward to construct a finite sample variable screening procedure, which we describe in Figure~\ref{fig:backfitting:algo}.
We use an $\ell_\infty/\ell_1$ penalty in equation~\eqref{eqn:scam2}
and an $\ell_\infty$ penalty in equation~\eqref{eqn:dc2} to encourage
sparsity.  Other penalties can also produce
sparse estimates, such as a penalty on the derivative of each of the
component functions.  The $\|\cdot\|_\infty$ norm is convenient for both
theoretical analysis and implementation.

After selecting the variable set $\hat{S}$, one can refit a
low-dimensional non-additive convex function to build the best
predictive model. If refitting is undesirable for whatever reason, the
AC/DC outputs can also be used for prediction. Given a new sample
$\mathbf{x}$, we let $\hat{y} = \sum_k \hat{f}_k(\mathbf{x}_k) +
\sum_k \hat{g}_k(\mathbf{x}_k)$. Note that $\hat{g}_k = 0$ for $k$
such that $\hat{f}_k \neq 0$ in AC/DC. The next section describes how
to compute this function evaluation.

The optimization in \eqref{eqn:scam2} appears to be infinite
dimensional, but it is equivalent to a finite dimensional quadratic
program.  In the following section, we give the details
of this optimization, and show how it can be reformulated
to be more computationally efficient.

%%For a simple proof of this fact, see \cite{Boyd04}, Section 6.5.5.
%%In more detail, we first set $\hat\mu = \frac{1}{n}\sum_{i=1}^n y_i$.  
%%For identifiability, we impose the constraint $\sum_{i=1}^n f_{ik} =
%%0$ for each $k$, where $f_{ik} = f_k(x_{ik})$ are the function values,
%%which are the program variables.  Then, we write the optimization as
%%\begin{align}
%%\label{eqn:scamqp}
%%\min_{f, \beta} \quad
%% &  \frac{1}{n} \sum_{i=1}^n \Big(y_i - \hat\mu - \sum_{k=1}^p f_{ik} \Big)^2 
%%       + \lambda \sum_{k=1}^p \| f_k \|_\infty \\
%%\nonumber
%%\text{such that} \quad & \textrm{for all $k=1,\ldots, p$:} \\
%% & \mathbf{1}^T f_k = 0, \; k=1,\ldots, p \\
%% & f_{i'k} \geq f_{ik} + \beta_{ik}(x_{i'k} - x_{ik}), \; \textrm{for
%%   all $i', i = 1,\ldots, n$}.
%%\end{align}
%%Likewise, the optimization in \eqref{eqn:dc} is implemented as
%%\begin{align}
%%\label{eqn:dcqp}
%%\min_{g_k, \gamma_k} \quad
%% &  \frac{1}{n} \sum_{i=1}^n \Big(y_i - \hat\mu - \sum_{k'=1}^p
%% f_{ik'} - g_{ik}\Big)^2 
%%       + \lambda \sum_{k=1}^p \| g_k \|_\infty \\
%%\nonumber
%%\text{such that} \quad &  \mathbf{1}^T g_k = 0, \\
%% & g_{i'k} \leq g_{ik} + \gamma_{ik}(x_{i'k} - x_{ik}), \; \textrm{for
%%   all $i', i = 1,\ldots, n$}.
%%\end{align}


\begin{figure}[t]
{\sc AC/DC Algorithm for Variable Selection in Convex Regression\hfill}
\vskip5pt
\begin{center}
\hrule
\vskip7pt
\normalsize
\begin{enumerate}
\item[] \textit{Input}:  $(\mathbf{x}_1, y_1), ..., (\mathbf{x}_n, y_n)$, regularization parameter $\lambda$.
\vskip5pt
\item[] \textit{AC Stage}:  Estimate a sparse additive convex model:
\begin{equation}
\label{eqn:scam2}
\hat{f}_1, ..., \hat{f}_p, \hat\mu = \argmin_{f_1,...,f_p \in \C^1_0} 
   \frac{1}{n} \sum_{i=1}^n \Big(y_i - \mu-\sum_{k=1}^p f_k(x_{ik}) \Big)^2 
       + \lambda \sum_{k=1}^p \| f_k \|_\infty
\end{equation}
\vskip5pt
\item[] \textit{DC Stage}:  Estimate concave functions
 for each $k$ such that $\| \hat{f}_k \|_\infty = 0$:
\begin{equation}
\label{eqn:dc22}
\hat{g}_k = \argmin_{g_k \in \mh{}\C^1_0} 
   \frac{1}{n} \sum_{i=1}^n \Big( y_i - \hat \mu - \sum_{k'} \hat{f}_{k'}(x_{ik'}) 
    - g_k(x_{ik})\Big)^2 
      + \lambda \| g_k \|_\infty
\end{equation}
\item[] \textit{Output}: Component functions $\{\hat f_k\}$ and 
relevant variables $\hat S$ where
\begin{equation}
\hat S^c = \bigl\{k : \| \hat{f}_k \| =
0 \; \mathrm{and}\; \|\hat{g}_k \|=0\bigr\}.
\end{equation}
\end{enumerate}
\vskip3pt
\hrule
\end{center}
\vskip0pt
\caption{The AC/DC algorithm for variable selection in convex
  regression.  The AC stage fits a sparse additive convex regression
  model, using a quadratic program that imposes an group sparsity
  penalty for each component function.  The DC stage fits
  decoupled concave functions on the residuals, for each 
  component that is zeroed out in the AC stage.}
\label{fig:backfitting:algo}
\end{figure}


% DO NOT CHANGE; RefTex variables -minx
 
%%% Local Variables: ***
%%% mode:latex ***
%%% TeX-master: "paper.tex" ***
%%% End: ***
