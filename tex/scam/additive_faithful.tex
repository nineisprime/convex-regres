
\section{Additive Faithfulness}

For general regression, additive approximation may result in a
relevant variable being incorrectly marked as irrelevant. Such
mistakes are inherent to the approximation and may persist even with
infinite samples.  In this section we give
examples of this phenomenon, and then show how the convexity
assumption
changes the behavior of the additive approximation. We begin
with a lemma that characterizes the components of the additive approximation under mild conditions.


% OLD lemma, requires product distribution
% \begin{lemma}
% \label{lem:general_int_reduction}
% Let $F$ be a product distribution on $\mathbf{C}=[0,1]^s$ with density function $p$ which is positive on $\mathbf{C}$. Let
% $X=(X_1,...,X_s) \sim F$. Let $f: \mathbf{C} \rightarrow \R$ be an
% integrable function.
% Let 
% \begin{equation}
% f_k^*, \mu^* \coloneqq \argmin_{f_1,\ldots, f_s, \mu} \Bigl\{ \E \bigl( f(X)  -
%  \sum_{k=1}^s f_k(X_k) -\mu \bigr)^2 
%         \,:\, \E f_k(X_k) = 0\Bigr\}.
% \end{equation}
% Then $\mu^* = \E f(X)$ and $f^*_k(x_k) = \E[ f(X) \given x_k] - \E f(X)$ and this solution is unique.
% \end{lemma}


\begin{lemma}
\label{lem:general_int_reduction}
Let $F$ be a distribution on $C=[0,1]^s$ with a positive density function $p$. Let $f: C \rightarrow \R$ be an integrable function. 

Let $\ds f^*_1,...,f^*_s, \mu^* \coloneqq 
\arg\min \{ \E \big( f(X) - \sum_{k=1}^s f_k(X_k) - \mu \big)^2 \,:\, \forall k, \E f_k(X_k) = 0\}$ 

Then 
$$ f^*_k(x_k) = \E[ f(X) - \sum_{k' \neq k} f^*_{k'}(X_{k'}) \given x_k] - \E f(X) $$
 and $\mu^* = \E f(X)$ and this solution is unique.
\end{lemma}


Lemma~\ref{lem:general_int_reduction} follows from the stationarity
conditions of the optimal solution. 

\begin{proof}
Let $f^*_1,...,f^*_s, \mu^*$ be the minimizers as defined. 

We first show that the optimal $\mu^* = \E f(X)$ for any $f_1, ..., f_k$ such that $\E f_k(X_k) = 0$. This follows from the stationarity condition, which states that $\mu^* = \E[ f(X) - \sum_k f_k(X_k)] = \E[ f(X) ]$. Uniqueness is apparent because the second derivative is strictly larger than 0 and strong convexity is guaranteed.

We now turn our attention toward the $f^*_k$'s. 

It must be that $f^*_k$ minimizes $\Big\{ \E \big( f(X) - \mu^* - \sum_{k' \neq k} f^*_{k'} (X_{k'}) - f_k (X_k) \big)^2 \,:\, \E f_k(X_k) = 0 \Big\}$.

Fix $x_k$, we will show that the value 
$\E[ f(X) - \sum_{k' \neq k} f_{k'}(X_{k'}) \given x_k] - \mu^*$, for all $x_k$,  
uniquely minimizes 
\[
\min_{ f_k(x_k) } \int_{\mathbf{x}_{-k}} p(\mathbf{x}) 
         \Big( f(\mathbf{x}) - \sum_{k' \neq k} f^*_{k'} (x_{k'}) - f_k (x_k) -\mu^*\Big)^2 
                 d \mathbf{x}_{-k}.
\]
It easily follows then that the function $x_k \mapsto \E[ f(X) - \sum_{k'} f_{k'} (X_{k'}) \given x_k] - \mu^*$ is the unique $f^*_k$ that minimizes the expected square error.We focus our attention on $f^*_K$, and fix $x_k$. 

The first-order optimality condition gives us:

\begin{align*}
\int_{\mathbf{x}_{-k}} p(\mathbf{x}) f_k(x_k) d \mathbf{x}_{-k} &= 
  \int_{\mathbf{x}_{-k}} p(\mathbf{x}) 
      ( f(\mathbf{x})-\sum_{k' \neq k} f^*_{k'}(x_{k'})-\mu^*) d \mathbf{x}_{-k} \\  
p(x_k) f_k(x_k) &= \int_{\mathbf{x}_{-k}} p(x_k)
     p(\mathbf{x}_{-k} \given x_k ) 
     ( f(\mathbf{x}) - \sum_{k' \neq k} f^*_{k'} (x_{k'})-\mu^*) 
              d \mathbf{x}_{-k} \\
f_k(x_k) &= \int_{\mathbf{x}_{-k}} 
       p(\mathbf{x}_{-k} \given x_k ) 
     (f(\mathbf{x}) - \sum_{k'\neq k} f_{k'} (x_{k'})  -\mu^*) d \mathbf{x}_{-k} 
 \end{align*}

The square error objective is strongly convex. The second derivative with respect to $f_k(x_k)$ is $2 p(x_k)$, which is always positive under the assumption that $p$ is positive. Therefore, the solution $f^*_k(x_k) = \E[ f(X) \given x_k ] - \E f(X)$ is unique.

Now, we note that as a function of $x_k$, $\E[ f(X) -\sum_{k'\neq k} f_{k'}(X_{k'}) | x_k] - \E f(X)$ has mean zero and we thus finish the proof.
\end{proof}

In the case that the distribution in Lemma~\ref{lem:general_int_reduction} is a product distribution, we get particularly clean expressions for the additive components.

\begin{corollary}
\label{cor:product_int_reduction}
Let $F$ be a product distribution on $\mathbf{C}=[0,1]^s$ with density function $p$ which is positive on $\mathbf{C}$. Let $\mu^*, f^*_k(x_k)$ be defined as in Lemma~\ref{lem:general_int_reduction}.
Then $\mu^* = \E f(X)$ and $f^*_k(x_k) = \E[ f(X) \given x_k] - \E f(X)$ and this solution is unique.
\end{corollary}


If $F$ is the uniform distribution,
then $f^*_k(x_k) = \int f(x_k, \mathbf{x}_{-k})
d\mathbf{x}_{-k}$.

\begin{example} Using Corollary~\ref{cor:product_int_reduction}, we give two examples of \emph{additive unfaithfulness} under
  the uniform distribution, that is, examples where relevant variables are erroneously marked as irrelevant under an additive approximation. First, consider the following function:
\[
\trm{(egg carton)} \quad f(x_1, x_2) = \sin( 2\pi x_1) \sin( 2 \pi x_2)
\]
defined for $(x_1, x_2) \in [0,1]^2$.  Then
$\int_{x_2} f(x_1, x_2) d x_2 = 0$ and
$\int_{x_1} f(x_1, x_2) d x_1 = 0$ for each $x_1$ and $x_2$. An additive approximation
would set $f_1 = 0$ and $f_2 = 0$.  Next, consider the function
\[
\trm{(tilting slope)} \quad f(x_1, x_2) = x_1 x_2
\]
defined for $x_1 \in [-1,1],\; x_2 \in [0,1]$.  In this case
$\int_{x_1} f(x_1, x_2) d x_1 = 0$ for each $x_2$; therefore, we expect $f_2 = 0$ under the additive approximation. This function, for every fixed $x_2$, is a zero-intercept linear function of $x_1$ with slope $x_2$.
\end{example}

\begin{figure*}[htp]
\vskip-10pt
	\centering
	\subfigure[egg carton]{
		\centering
		{\includegraphics[width=0.4\textwidth]{figs/sine_wave_funct3}}
	}
	\subfigure[tilting slope]{
		\centering
		{\includegraphics[width=0.4\textwidth]{figs/tilting_slope_funct3}}
	}
\caption{Two additively unfaithful functions. Relevant variables are
  zeroed out under an additive approximation because every ``slice''
  of the function integrates to zero.}
\vskip-10pt
\end{figure*}

In order to exploit additive models, it is important to understand when the
additive approximation accurately captures all of the relevant variables.
We call this property \textbf{additive faithfulness}. We first formalize the intuitive notion that a multivariate function $f$ \emph{depends on} a coordinate $x_k$.

\begin{definition}
  Let $F$ be a distribution on $\mathbf{C}=[0,1]^s$, and $f:\mathbf{C}\rightarrow \R$. 
  
We say that $f$ \textbf{depends on} coordinate $k$ if, for all $x_k \in [0,1]$, the set 
$\big\{ x'_k \in [0, 1] \,:\, f(x_k, \mathbf{x}_{-k}) = f(x'_k, \mathbf{x}_{-k}) 
\trm{ for almost all  $\mathbf{x}_{-k}$} \big\}$ 
has probability strictly less than 1.\\

Suppose we have the additive approximation:
\begin{equation}
f_k^*, \mu^* \coloneqq \argmin_{f_1,\ldots, f_s, \mu} \Bigl\{ 
             \E ( f(X) - \sum_{k=1}^s f_k(X_k) -\mu )^2 
         \,:\, \E f_k(X_k) = 0 \Bigr\}.
\end{equation}

We say that $f$ is \textbf{additively faithful} under $F$ in case $f^*_k = 0 \Rightarrow \trm{$f$ does not depend on coordinate $k$}$. 

\end{definition}
% We can define the support $\trm{supp}(f) \coloneqq \{ k \,:\,
% \trm{$k$ is relevant to $f$}\}$. Let $f^* = \sum_{k=1}^s$, then $f$
% is additively faith if $\trm{supp}(f) = \trm{supp}(f^*)$.

Additive faithfulness is an attractive property because it implies that, in the population setting, the additive approximation yields consistent variable selection. 

\subsection{Additive Faithfulness of Convex Functions}

Remarkably, under a general class of distributions which we characterize below, convex multivariate functions are additively faithful.

\begin{definition}
\label{defn:three-point}
Let $p(\mathbf{x})$ be a density supported on $[0,1]^s$, $p$ satisfies the \emph{three-points property} if, for all $j$, and for all $\mathbf{x}_{-j}$:
\begin{packed_enum}
\item $\frac{\partial p(\mathbf{x}_{-j} \given x_j)}{\partial x_j}  = 0$ at $x_k = 0, x_k = 1$ and a third point $x_k^M \in (0,1)$.
\item $\frac{\partial^2 p(\mathbf{x}_{-j} \given x_j)}{\partial x_j^2} = 0$ at $x_k = 0, x_k = 1$.
\end{packed_enum}
\end{definition}



The three-points property is a weak condition. For instance, it is satisfied when the density is flat at the boundary of support and has at least one peak in the interior of the support. More precisely, when the \emph{joint density} satisfies the properties that $\frac{\partial p(x_j,\mathbf{x}_{-j})}{\partial x_j} = 0$ at points $x_j = 0, x_j= 1$ and a third point $x_j^M \in (0,1)$ and $\frac{\partial^2 p(x_j, \mathbf{x}_{-j})}{\partial x_j^2} = 0$ at points $x_j = 0, x_j=1$. The three-points property is also trivially satisfied when $p$ is the density of any product distributions.

The following theorem is the main result of this section.

\begin{theorem}
\label{thm:convex_faithful}
Let $p$ be a positive density supported on $C=[0,1]^s$ that satisfies the three-points property (definition~\ref{defn:three-point}). If $f$ is convex and twice differentiable, then $f$ is \emph{additively faithful} under $p$.
\end{theorem}

% We give the full proof in Section~\ref{sec:faithful_proof} of the
% Appendix, but pause here to provide some intuition. 

We pause to give some intuition before we present the full proof: 
suppose the underlying distribution is a product distribution for a second, 
then we know from lemma~\ref{lem:general_int_reduction} that the
additive approximation zeroes out $k$ when, fixing $x_k$, every
``slice'' of $f$ integrates to zero. We prove
Theorem~\ref{thm:convex_faithful} by showing that ``slices'' of convex
functions that integrate to zero cannot be ``glued'' together while
still maintaining convexity.


\begin{proof} (of Theorem~\ref{thm:convex_faithful})\\
Fix $k$. Using the result of Lemma~\ref{lem:general_int_reduction}, we need only show that for all $x_k$, $ \E[ f(X) - \sum_{k'} f_{k'}(X_{k'}) \given x_k] - \E f(X) = 0 $ implies that $f$ does not depend on coordinate $k$.\\

Let us then use the shorthand notation that $r(\mathbf{x}_{-k}) = \sum_{k' \neq k} f_{k'}(x_{k'})$ and assume without loss of generality that $\mu = 0$. We then assume that for all $x_k$, 

\[
 \E[ f(X) - r(X_{-k})  \given x_k] \equiv 
 \int_{\mathbf{x}_{-k}}  p(\mathbf{x}_{-k} \given x_k ) 
 \big(f(\mathbf{x}) - r(\mathbf{x}_{-k}) \big) = 0
\]

We let $p'(\mathbf{x}_{-k} \given x_k)$ denote 
$\frac{\partial p(\mathbf{x}_{-k} \given x_k)}{\partial x_k}$ and 
$p''(\mathbf{x}_{-k} \given x_k)$ denote 
$\frac{\partial^2 p(\mathbf{x}_{-k} \given x_k)}{\partial x_k^2}$ and likewise for $f'(x_k, \mathbf{x}_{-k})$ and $f''(x_k, \mathbf{x}_{-k})$. We then differentiate under the integral, which is valid because all functions are bounded.

\begin{align}
& \int_{\mathbf{x}_{-k}} p'(\mathbf{x}_{-k} \given x_k) 
\big( f(\mathbf{x}) - r(\mathbf{x}_{-k}) \big) + 
p(\mathbf{x}_{-k} \given x_k) f'(x_k, \mathbf{x}_{-k}) d \mathbf{x}_{-k}  = 0 
\label{eqn:integral1a} \\
& \int_{\mathbf{x}_{-k}} p''(\mathbf{x}_{-k} \given x_k) 
\big( f(\mathbf{x}) - r(\mathbf{x}_{-k}) \big)  + 
2 p'(\mathbf{x}_{-k} \given x_k) f'(x_k, \mathbf{x}_{-k}) +
p(\mathbf{x}_{-k} \given x_k) f''(x_k, \mathbf{x}_{-k}) d\mathbf{x}_{-k}  = 0 
\end{align}

By the three-points property, we have that $p''(\mathbf{x}_{-k} \given x_k)$ and $p'(\mathbf{x}_{-k} \given x_k)$ are zero at $x_k = x_k^0 \equiv 0$. The integral equations reduce to the following then:
\begin{align}
& \int_{\mathbf{x}_{-k}} p(\mathbf{x}_{-k} \given x^0_k) f'(x^0_k, \mathbf{x}_{-k}) d \mathbf{x}_{-k}= 0 \label{eqn:integral1b} \\
& \int_{\mathbf{x}_{-k}} p(\mathbf{x}_{-k} \given x^0_k) f''(x^0_k, \mathbf{x}_{-k}) d\mathbf{x}_{-k} = 0
\end{align}

Because $f$ is convex, $f(x_k, \mathbf{x}_{-k})$ must be a convex function of $x_k$ for all $\mathbf{x}_{-k}$. Therefore, for all $\mathbf{x}_{-k}$, $f''(x^0_k, \mathbf{x}_{-k}) \geq 0$. Since $p(\mathbf{x}_{-k} \given x^0_k) > 0$ by assumption that $p$ is a positive density, we have that $\forall \mathbf{x}_{-k}, f''(x^0_k, \mathbf{x}_{-k}) = 0$ necessarily.\\

The Hessian of $f$ at $(x^0_k, \mathbf{x}_{-k})$ then has a zero at the $k$-th main diagonal entry. A positive semidefinite matrix with a zero on the $k$-th main diagonal entry must have only zeros on the $k$-th row and column \footnote{ See proposition 7.1.10 of \citet{HJ90}}, which means that \emph{at all $\mathbf{x}_{-k}$, the gradient of $f'(x^0_k, \mathbf{x}_{-k})$ with respect to $\mathbf{x}_{-k}$ must be zero}.

Therefore, $f'(x_k^0, \mathbf{x}_{-k})$ must be a constant. By equation~\ref{eqn:integral1b}, we conclude then that $f'(x_k^0, \mathbf{x}_{-k}) = 0$ for all $\mathbf{x}_{-k}$. We can use the same reasoning for the case where $x_k = x_k^1$ and deduce that $f'(x^1_k, \mathbf{x}_{-k}) = 0$ for all $\mathbf{x}_{-k}$. \\

Now we apply the first-order condition of convex functions to the two points $(x^0_k, \mathbf{x}_{-k})$ and $(x^1_k, \mathbf{x}_{-k})$:

\begin{align*}
\forall \mathbf{x}_{-k}, f(x^1_k, \mathbf{x}_{-k}) & \leq f(x^0_k, \mathbf{x}_{-k}) 
  + f'(x^0_k, \mathbf{x}_{-k}) ( x^1_k - x^0_k) \\ 
f(x^1_k, \mathbf{x}_{-k}) &\leq f(x^0_k, \mathbf{x}_{-k}) \\
\forall \mathbf{x}_{-k}, f(x^0_k, \mathbf{x}_{-k}) & \leq f(x^1_k, \mathbf{x}_{-k}) 
  + f'(x^1_k, \mathbf{x}_{-k}) ( x^0_k - x^1_k) \\ 
f(x^0_k, \mathbf{x}_{-k}) &\leq f(x^1_k, \mathbf{x}_{-k})
\end{align*}

We thus have that $f(x^0_k, \mathbf{x}_{-k}) = f(x^1_k, \mathbf{x}_{-k})$ for all $\mathbf{x}_{-k}$. By convexity of $f$, we know that for all $x_k \in (0,1)$, it must be that for all $\mathbf{x}_{-k}$, $f(x_k, \mathbf{x}_{-k}) \leq f(x^0_k, \mathbf{x}_k)$.\\

We now use the middle point condition.

At $x_k = x^M_k$, equation~\ref{eqn:integral1a} becomes:
\begin{align}
\int_{\mathbf{x}_{-k}} p(\mathbf{x}_{-k} \given x^M_k) 
  f'(x^M_k, \mathbf{x}_{-k}) d \mathbf{x}_{-k} = 0 \label{eqn:integral1c}
\end{align}

Using the first order condition, we have that:
\begin{align*}
f(x^0_k, \mathbf{x}_{-k}) &\leq f(x^M_k, \mathbf{x}_{-k}) 
+ f'(x^M_k, \mathbf{x}_{-k})(x^0_k - x^M_k)  \quad \Rightarrow \\
\int_{\mathbf{x}_{-k}} p(\mathbf{x}_k \given x^M_0) 
f(x^0_k, \mathbf{x}_{-k}) d\mathbf{x}_{-k}  
&\leq \int_{\mathbf{x}_{-k}} p(\mathbf{x}_{-k} \given x^M_k)
\big( f(x^M_k, \mathbf{x}_{-k}) 
+ f'(x^M_k, \mathbf{x}_{-k})(x^0_k - x^M_k) \big) d\mathbf{x}_{-k} 
\end{align*}

By equation~\ref{eqn:integral1c} then, we have that $
\int_{\mathbf{x}_{-k}} p(\mathbf{x}_{-k} \given x_k) \big(f(x^0_k, \mathbf{x}_{-k}) -
 f(x^M_k, \mathbf{x}_{-k}) \big) d\mathbf{x}_{-k} \leq 0$

However, we have previously argued that for all $\mathbf{x}_{-k}$, $f(x_k, \mathbf{x}_{-k}) \leq f(x^0_k, \mathbf{x}_{-k})$ for all $x_k \in (0,1)$. Since $p(\mathbf{x}_{-k} \given x_k) > 0$, it must be that $f(x^0_k, \mathbf{x}_{-k}) = f(x^M_k, \mathbf{x}_{-k})$ for all $\mathbf{x}_{-k}$.\\

Thus, we have shown that for all $\mathbf{x}_{-k}$, 
\[
f(x^0_k, \mathbf{x}_{-k}) = f(x^M_k, \mathbf{x}_{-k}) = f(x^1_k, \mathbf{x}_{-k})
\]
This implies that $f(x_k, \mathbf{x}_{-k})$, as a convex function of $x_k$, must be a constant. This proves that $f$ does not depend on $x_k$. 
\end{proof}

Theorem~\ref{thm:convex_faithful} plays an important role in our
sparsistency analysis, where we show that the additive
approximation is variable selection consistent (or ``sparsistent''), even when the true function is not
additive.

\begin{remark}
  We assume twice differentiability in
  Theorems~\ref{thm:convex_faithful} to simplify the proof. 
  We believe
  this smoothness condition is not necessary because every non-smooth
  convex function can be approximated arbitrarily well by a smooth
  one.  
\end{remark}

\begin{remark} 
It is difficult to prove the opposite direction of additive faithfulness, that is, if $f$ does not depend on coordinate $k$, then $f_k^*$ will be zero in the additive approximation. Consider as a conceptual example a 3D distribution over $(X_1, X_2, X_3)$; suppose $X_1, X_2$ are independent, and $f$ is only a function of $X_1, X_2$. We can then let $X_3 = f(X_1, X_2) - f^*_1(X_1) - f^*_2(X_2)$, that is, we let $X_3$ exactly capture the additive approximation error, then the best additive approximation of $f$ would have a component $f^*_3(X_3) = X_3$ even though $f$ does not depend on $X_3$. This is, of course, unlikely to occur in practice and we leave as future work analysis under reasonable assumptions that rules out such psychotic phenomenon. 
\end{remark}

% \begin{remark}
% Without restrictions on the distribution, a convex
%   function may not be additively faithful. Intuitively, an arbitrarily shaped
%   density $p$
%   may ``undo'' the convexity of $f$ so that the product
%   $p(\mathbf{x}) \, f(\mathbf{x})$ resembles an egg carton or a
%   tilting slope.  With appropriate conditions on the density $p$,
%   however, it is possible to relax the independence assumption.  We leave this to
%   future work.
% \end{remark}


% DO NOT CHANGE; RefTex variables -minx
 
%%% Local Variables: ***
%%% mode:latex ***
%%% TeX-master: "paper.tex" ***
%%% End: ***

