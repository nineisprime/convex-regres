
\documentclass[pdftex,12pt]{article}

\usepackage{hyperref}
\usepackage{fullpage}
\usepackage{enumerate}
\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage{graphicx,subfigure}
\usepackage[pdftex]{color}
\usepackage{epic,eepic,eepicemu}
\usepackage{epsf}
\usepackage{epsfig}
\usepackage{fancyhdr}
\usepackage{graphics}
\usepackage{psfrag,latexsym}
\usepackage{times}
\newcommand{\thetamin}{\ensuremath{\theta^*_{min}}}
\newcommand{\mutinc}{\ensuremath{\alpha}}
\bibliographystyle{abbrv}


\setlength{\textwidth}{\paperwidth}
\addtolength{\textwidth}{-6cm}
\setlength{\textheight}{\paperheight}
\addtolength{\textheight}{-4cm}
\addtolength{\textheight}{-1.1\headheight}
\addtolength{\textheight}{-\headsep}
\addtolength{\textheight}{-\footskip}
\setlength{\oddsidemargin}{0.5cm}
\setlength{\evensidemargin}{0.5cm}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\degmax{d}
\def\pdim{p}
\def\numobs{n}
\let\hat\widehat
\definecolor{blue}{rgb}{0.01,0.01,0.75}
\def\reviewercomment#1{{\it\textcolor{blue}{#1}}\smallskip}
\parindent0pt
\parskip12pt

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\vspace*{5pt}

We thank the reviewers for their helpful comments regarding our
submission. We have carefully addressed all these comments. The
following describes the revisions made to the manuscript in response
to the reviews.

\subsection*{Associate Editor}

\reviewercomment{Below I recapitulate a few
point of the reviewers', along with some questions/comments of my own.
These (as well as other points in the reviews) need to be thoroughly
addressed in the revised manuscript.}

\begin{enumerate}
\item \reviewercomment{Boundary flatness condition: much of the paper's analysis hinges on
this condition.  Reviewer 3 raises some concerns about when it holds,
especially for functions that are not necessarily supported on the
unit hypercube.  See also the point below.}

We have added commentary on this here and here.

\item \reviewercomment{Intuitively, variable selection is very closely related to
estimating a function (or possibly its partial derivatives) in the
sup-norm.  It is well-known that accurate estimation of functions
(whether in $L^2$ or the sup-norm) with bounded smoothness in dimension
$s$ requires a sample size that grows exponentially in $s$.  For this
reason, the past result of Comminges and Dalalyan (suggesting an
exponential growth for variable selection) is to be expected.
However, in terms of metric entropy, the class of convex functions
behaves roughly like the class of two-smooth functions (e.g., Dryanov
(2009)).  So at first glance, this makes the result of this paper
rather surprising---namely, that one can somehow side-step any
exponential growth in $s$, even though the metric entropy is still
scaling as badly as the case of 2-smooth functions.  I wondered if the
authors could comment and shed insight on this fact, which seems a
little counterintuitive.  What I suspect that it could mean is that
the additive faithfulness (AF) condition, or one of the sufficient
conditions for AF (namely boundary flatness) are somehow very
restrictive.  Could the authors comment on these issues?}

The boundary flatness condition does not artificially make the variable selection problem easy. Comminges and Dalalyan showed that variable selection for general two-smooth functions is difficult even if the underlying distribution is a product distribution, and boundary flat distributions are much more general than product distributions. 

\item \reviewercomment{Writing and proofs need to be tidied up in places to make it easier
to follow.  See various comments of referees, particularly the
detailed ones of R2.  Also, in Theorem 3, please remind the author of
when/where the AC/DC estimators were defined.  (I was forced to dig
back through.)}

We have tidied up.

\end{enumerate}

\subsection*{Reviewer 1}

\reviewercomment{Conceptually, I have a few questions for the authors to address:}

\begin{enumerate}[(1)]
\item \reviewercomment{If the true regression function $f_0$ can actually be written as a
sum of univariate convex functions (plus noise), are there natural
conditions that one can provide for when additive faithfulness will
hold? Similarly, is there a simpler version of Theorem 3 that can be
stated when $f_0$ is actually a sum of univariate convex functions? This
would seem to depend on an appropriate statement for Assumption (A3).}

If the true regression function is a convex additive function, then additive faithfulness holds trivially--the best additive approximation to an additive function is the function itself. The estimation procedure becomes simpler--the DC stage is no longer necessary and can be safely removed. 

In the finite sample analysis, the assumptions would not change except that A3 would be $\| f^*_k \|_\infty \leq B$ for all $k$ since $f_0 = f^*$ in this case. The signal strength definition can be made simpler: $\alpha^+ = \min_k \mathbb{E}( f^*_k(X_k)^2 )$. The rates would not improve.


\item \reviewercomment{If we actually wish to fit smooth convex univariate components, is
it easy to add this condition as a regularization term and incorporate
it into the analysis?}

Smoothness can be incorporated by using B-spline basis with uniformly spaced knots. The details are described in a paper by Pya and Wood which we cited. We chose not to discuss estimating smooth convex functions because it would introduce an additional tuning parameter, which negates some of the advantages of doing shape-constrained estimation.

\item \reviewercomment{Assuming model selection consistency, can one derive prediction
error bounds using the fitted $f_k$'s and $g_k$'s, as well? Prediction error
bounds seem quite relevant due to possible model misspecification when
$f_0$ is not actually a sum of univariate convex functions.}

\item \reviewercomment{In order to improve clarity, the authors should define the convex
additive model at the top of page 5: $f(x_i) = \sum_{k=1}^p f_k(x_{ik})$, where the
$f_k$'s are convex.  Otherwise, the notation in equation (2.3) is
initially a bit confusing.}

\item \reviewercomment{Another notational comment: the use of the
distribution function $F$ seems unnecessary (e.g., Lemma 3.1 and
Corollary 3.1), since everything is already defined in terms of the
joint density function $p(x)$. (An additional suggestion would be to use
a different letter to represent the joint density function, since $p$ is
already used to represent the dimensionality of the distribution.)}

\item \reviewercomment{I would suggest including the dependence on the positive-valued lower
bound on $p(x)$ (as assumed in (A5)) in the statement of Theorem 3. This
is a point worth commenting upon, since the earlier theorems only
require $p(x)$ to be strictly positive. Assumption (A5) seems like it
could also be interpreted as a sort of minimum signal strength
requirement for successful variable selection.}

\item \reviewercomment{It would be good to point to Assumption (A3) at the beginning of
Section 5 when discussing the $B$-boundedness condition. In the third
paragraph, the authors might note that the $B$-boundedness condition
arises from the analysis of Theorem 5.2. To clarify, the penultimate
paragraph on page 22 should state that Theorem 5.1 controls false
positives alone, whereas Theorem 5.3 (to follow) provides
finite-sample results controlling the false negative rate.}

\item \reviewercomment{At the top
of page 23, the remark regarding mutual incoherence conditions is a
bit confusing. Aren’t the conditions imposed in Theorem 5.1 on the
covariates essentially analogous to the mutual incoherence conditions
in this case? }

\item \reviewercomment{In the simulation section, the authors might consider comparing the
set of variables selected after the AC step only with the set of
variables selected via the entire AC/DC algorithm. This would help
illustrate the necessity of the concave fitting step in addition to
the convex additive fit alone.}

Minor typos:
\begin{enumerate}[(i)]
\item \reviewercomment{In Section 2.3, it would be good to actually state the regularized
objective function being analyzed, rather than waiting until Section
4.}
\item \reviewercomment{Assumption (A3) on page 7 is missing the subscript $\infty$.}
\item \reviewercomment{The parameter $\sigma$ is never defined in Section 2.3. Reading ahead,
it seems to be the sub-Gaussian parameter of the additive noise.}
\item \reviewercomment{Perhaps Section 3 should be titled something other than ``Additive
faithfulness'' (maybe ``Population- level results''?).}
\item \reviewercomment{The first paragraph of Section 5 should read
  ``Figure 3'' rather than ``Algorithm 3.''}

\item \reviewercomment{The second paragraph of Section 5 has $m\hat{}u$ instead of $\hat\mu$.}
\item \reviewercomment{The first sentence of Section 5.1 should read optimization (4.8).}
\end{enumerate}

\end{enumerate}

\subsection*{Reviewer 2}

We are grateful to the reviewer for the detailed reading. We have corrected the mistakes found and further proof-read the paper. We discuss some of the issues raised. 

\reviewercomment{I found many (unimportant) mistakes in the proofs (I
  did not mention them all in the minor comments), which need to be
  read again carefully.}

\begin{enumerate}
\item \reviewercomment{I have a problem with the assertion of
  uniqueness in Lemma 3.1 and Therorem 3.2. First, in Lemma 3.1, it is
  clear that $f^∗$ is unique. But $(f_1^∗, \ldots, f_p^∗)$ is not. What is
  shown is that, for $k$ fixed and given $(f_j^*)_{j\neq k}$, $f_k^*$ is unique. Next,
  in Theorem 3.2, the proof of uniqueness (bottom of page 16 and top
  of page 17) is not at all correct.
   Now, to see why I think that, with the assumtions made on $X$, there is
   not uniqueness, simply suppose for instance that $p = 2$ and $X_2 = g(X_1)$
   for a certain function $g$. Then if $(f_1^∗, f_2^∗)$ is a solution,
   $(f_1^∗ + f_2^ \circ g, 0)$ is also a solution.}

We have corrected the proof of Lemma 3.1 and we have given a more rigorous proof that the components $f^*_k$'s are unique in a new Lemma 8.3 (old Lemma 8.3 was removed). Our result requires that the density $p(\mathbf{x}) > 0$ and thus, the $X_2 = g(X_1)$ example raised by the reviewer is ruled out. 

\item \reviewercomment{On page 11, after Deônition 3.2, it is written ``when the joint
density satisfies the property that
$\frac{\partial(x_{-j},x_j)}{\partial x_j} = \frac{\partial^2 p(x_{-j},
  x_j)}{\partial x_j^2} = 0$ at boundary points''.
I think the condition ``$p(x_{−j}, x_j ) = 0$ at the boundary points'' is
missing.}

It is not necessary that $p(\mathbf{x}_{-j}, x_j) = 0$ at $x_j = 0,1$; it is necessary that $p(\mathbf{x}_{-j}, x_j) > 0$. We have however, added an additional condition that $p(\mathbf{x})$ has a bounded second derivative. This condition ensures that the marginal density $p(x_j)$ is twice differentiable and $p'(x_j) = p''(x_j) = 0$ at $x_j = 0,1$. With this, it is valid to apply the quotient rule for differentiation to show that boundary flatness holds. Please see the second item in Example 3.2 for the relevant modifications. 


\item \reviewercomment{On page 12, I did not understand why the functions 
$\frac{\partial^2 p(x_{-j} |  x_j)}{\partial x_j^2}$
and adn 
$\frac{\partial^2 f(x_{-j} |  x_j)}{\partial x_j^2}$
are bounded (they are not supposed to be continuous)}

We have modified the definition of boundary flatness so that $p(\mathbf{x_{-j}} \,|\, x_j)$ is bounded and has a bounded first and second derivative for all $x_j$ in some open set around the boundary points and for all $\mathbf{x}_{-j}$. Dominated convergenc theorem holds without problems because we only exchange the derivative and the integral at $x_j = 0,1$. Just to make sure, we have verified in detail that the domination condition holds in the newly added Section 8.3.4.

%\item \reviewercomment{On page 13, on line 23, I do not think $\Sigma$ has been defined. Besides it
%is not speciôed that $Var(X_1) = 1$.}

%\item \reviewercomment{On page 13, on line 29, ``additive'' should be replaced by
%``additively''.}

%\item \reviewercomment{On page 13, the assumption $\E f(X) = 0$ should be made, or else the
%parameter $\mu$ should be introduced as before.}

%\item \reviewercomment{On page 16, in the expresson of $c^∗$, the
%  denominator should be $\E X_k^2 −m_k^2$.}

\item \reviewercomment{On page 16, in the proof of Theorem 3.2, Lemma 8.3 is used. This
lemma supposes that $\phi$ is continuous. Here I do not see why $\phi =
\sum_{k'\neq k} f_{k'}*$ is continuous (it is convex then continuous on the interior of
$[0,1]^p$. ).}

We have modified the conditions and the proof of Theorem 3.2 so that $\phi$ need only be bounded. The new proof does not require that $h^*_k$ is differentiable, only that it is in $L^2$. The old Lemma 8.3 has been removed. 

\item \reviewercomment{On page 17, same problem as before with the absence of parameter $\mu$.}
\item \reviewercomment{On page 17, on line 16 ``for that variable. for each''}
\item \reviewercomment{On page 18, at the bottom of the page :
  ``$\beta_{\pi_k(i+1)k} 
\geq \beta_{\pi_k(i)k}$ for $i=1,\ldots, n-1$'' , $n−1$ must be replaced by $n−2$.}
\item \reviewercomment{On page 19, $\sum_{i} f_{ik}$ instead of $\sum_i
  f_{ki}$.}
\item \reviewercomment{On page 20, in the third paragraph, on the second, third and fourth
lines, $j$ and $j − 1$ must be replaced by $j + 1$ and $j$ respectively in
$x_{\pi_k(j)k}$ and $x_{\pi_k(j-1)k}$.}

\end{enumerate}

\subsection*{Reviewer 3}


Major comments:

\begin{enumerate}
\item \reviewercomment{The basis of all the results seem to be the boundary flatness
condition. This condition seems to be an artifact of the analysis and
based on my reading, it is unclear how restrctive this assumption
is. The authors claim it is weak just after stating it and then
present several straightforward examples in which it doesn't hold
(e.g. $f(x_1,x_2)=x_1 x_2$) and multivariate Gaussian examples. Hence it is unclear to me how
likely a given function is likely to fit into this framework.}

\item \reviewercomment{Following on from the previous point, the simulation study also does
not add any further insight to this issue since the simulations are
for multivariate Gaussians where other parametric approaches that deal
with covariance seem more suitable. It is puzzling to me that the
authors did not consider non-parametric examples where the additive
model framework seems more suitable.}

We consider only densities with support in a hypercube. We have modified old experiments and added new experiments to use boundary flat distributions instead of the correlated multivariate Gaussian distribution. 


\end{enumerate}

Minor comments:

\begin{enumerate}
\item \reviewercomment{I didn't think there was any need for Section 2
  since it feels very repetitive and slightly confusing that parts of
  the text are repeated almost verbatim. I would suggest removing this section.}

\item \reviewercomment{Page 2, second last paragraph `and and.'}
\end{enumerate}


\vspace*{10pt}

Sincerely, 


Min Xu, Minhua Chen, and John Lafferty\\[1pt]
\today{}

\bibliography{local}
\end{document}
