
\documentclass[pdftex,12pt]{article}

\usepackage{hyperref}
\usepackage{fullpage}
\usepackage{enumerate}
\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage{graphicx,subfigure}
\usepackage[pdftex]{color}
\usepackage{epic,eepic,eepicemu}
\usepackage{epsf}
\usepackage{epsfig}
\usepackage{fancyhdr}
\usepackage{graphics}
\usepackage{psfrag,latexsym}
\usepackage{times}
\newcommand{\thetamin}{\ensuremath{\theta^*_{min}}}
\newcommand{\mutinc}{\ensuremath{\alpha}}
\bibliographystyle{abbrv}


\setlength{\textwidth}{\paperwidth}
\addtolength{\textwidth}{-6cm}
\setlength{\textheight}{\paperheight}
\addtolength{\textheight}{-4cm}
\addtolength{\textheight}{-1.1\headheight}
\addtolength{\textheight}{-\headsep}
\addtolength{\textheight}{-\footskip}
\setlength{\oddsidemargin}{0.5cm}
\setlength{\evensidemargin}{0.5cm}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\degmax{d}
\def\pdim{p}
\def\numobs{n}
\let\hat\widehat
\definecolor{blue}{rgb}{0.01,0.01,0.75}
\def\rc#1{{\it\textcolor{blue}{#1}}\smallskip}
\parindent0pt
\parskip12pt

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\vspace*{5pt}

We thank the reviewers for their helpful comments regarding our
submission. We have carefully addressed all these comments. The
following describes the revisions made to the manuscript in response
to the reviews.

\subsection*{Associate Editor}

\rc{Below I recapitulate a few
point of the reviewers', along with some questions/comments of my own.
These (as well as other points in the reviews) need to be thoroughly
addressed in the revised manuscript.}

\begin{enumerate}
\item \rc{Boundary flatness condition: much of the paper's analysis hinges on
this condition.  Reviewer 3 raises some concerns about when it holds,
especially for functions that are not necessarily supported on the
unit hypercube.  See also the point below.}

We have added commentary on this here and here.

\item \rc{Intuitively, variable selection is very closely related to
estimating a function (or possibly its partial derivatives) in the
sup-norm.  It is well-known that accurate estimation of functions
(whether in $L^2$ or the sup-norm) with bounded smoothness in dimension
$s$ requires a sample size that grows exponentially in $s$.  For this
reason, the past result of Comminges and Dalalyan (suggesting an
exponential growth for variable selection) is to be expected.
However, in terms of metric entropy, the class of convex functions
behaves roughly like the class of two-smooth functions (e.g., Dryanov
(2009)).  So at first glance, this makes the result of this paper
rather surprising---namely, that one can somehow side-step any
exponential growth in $s$, even though the metric entropy is still
scaling as badly as the case of 2-smooth functions.  I wondered if the
authors could comment and shed insight on this fact, which seems a
little counterintuitive.  What I suspect that it could mean is that
the additive faithfulness (AF) condition, or one of the sufficient
conditions for AF (namely boundary flatness) are somehow very
restrictive.  Could the authors comment on these issues?}

The boundary flatness condition does not artificially make the variable selection problem easy. Comminges and Dalalyan showed that variable selection for general two-smooth functions is difficult even if the underlying distribution is a product distribution, and boundary flat distributions are much more general than product distributions. 

\item \rc{Writing and proofs need to be tidied up in places to make it easier
to follow.  See various comments of referees, particularly the
detailed ones of R2.  Also, in Theorem 3, please remind the author of
when/where the AC/DC estimators were defined.  (I was forced to dig
back through.)}

We have tidied up.

\end{enumerate}

\subsection*{Reviewer 1}

\rc{Conceptually, I have a few questions for the authors to address:}

\begin{enumerate}[(1)]
\item \rc{If the true regression function $f_0$ can actually be written as a
sum of univariate convex functions (plus noise), are there natural
conditions that one can provide for when additive faithfulness will
hold? Similarly, is there a simpler version of Theorem 3 that can be
stated when $f_0$ is actually a sum of univariate convex functions? This
would seem to depend on an appropriate statement for Assumption (A3).}

If the true regression function is a convex additive function, then additive faithfulness holds trivially--the best additive approximation to an additive function is the function itself. The estimation procedure becomes simpler--the DC stage is no longer necessary and can be safely removed. 

In the finite sample analysis, the assumptions would not change except that A3 would be $\| f^*_k \|_\infty \leq B$ for all $k$ since $f_0 = f^*$ in this case. The signal strength definition can be made simpler: $\alpha^+ = \min_k \mathbb{E}( f^*_k(X_k)^2 )$. The rates would not improve.


\item \rc{If we actually wish to fit smooth convex univariate components, is
it easy to add this condition as a regularization term and incorporate
it into the analysis?}

Smoothness can be incorporated by using B-spline basis with uniformly spaced knots. The details are described in a paper by Pya and Wood which we cited. We chose not to discuss estimating smooth convex functions because it would introduce an additional tuning parameter, which negates some of the advantages of doing shape-constrained estimation.

\item \rc{Assuming model selection consistency, can one derive prediction
error bounds using the fitted $f_k$'s and $g_k$'s, as well? Prediction error
bounds seem quite relevant due to possible model misspecification when
$f_0$ is not actually a sum of univariate convex functions.}

\item \rc{In order to improve clarity, the authors should define the convex
additive model at the top of page 5: $f(x_i) = \sum_{k=1}^p f_k(x_{ik})$, where the
$f_k$'s are convex.  Otherwise, the notation in equation (2.3) is
initially a bit confusing.}

\item \rc{Another notational comment: the use of the
distribution function $F$ seems unnecessary (e.g., Lemma 3.1 and
Corollary 3.1), since everything is already defined in terms of the
joint density function $p(x)$. (An additional suggestion would be to use
a different letter to represent the joint density function, since $p$ is
already used to represent the dimensionality of the distribution.)}

\item \rc{I would suggest including the dependence on the positive-valued lower
bound on $p(x)$ (as assumed in (A5)) in the statement of Theorem 3. This
is a point worth commenting upon, since the earlier theorems only
require $p(x)$ to be strictly positive. Assumption (A5) seems like it
could also be interpreted as a sort of minimum signal strength
requirement for successful variable selection.}

\item \rc{It would be good to point to Assumption (A3) at the beginning of
Section 5 when discussing the $B$-boundedness condition. In the third
paragraph, the authors might note that the $B$-boundedness condition
arises from the analysis of Theorem 5.2. To clarify, the penultimate
paragraph on page 22 should state that Theorem 5.1 controls false
positives alone, whereas Theorem 5.3 (to follow) provides
finite-sample results controlling the false negative rate.}

\item \rc{At the top
of page 23, the remark regarding mutual incoherence conditions is a
bit confusing. Aren’t the conditions imposed in Theorem 5.1 on the
covariates essentially analogous to the mutual incoherence conditions
in this case? }

\item \rc{In the simulation section, the authors might consider comparing the
set of variables selected after the AC step only with the set of
variables selected via the entire AC/DC algorithm. This would help
illustrate the necessity of the concave fitting step in addition to
the convex additive fit alone.}

Minor typos:
\begin{enumerate}[(i)]
\item \rc{In Section 2.3, it would be good to actually state the regularized
objective function being analyzed, rather than waiting until Section
4.}
\item \rc{Assumption (A3) on page 7 is missing the subscript $\infty$.}
\item \rc{The parameter $\sigma$ is never defined in Section 2.3. Reading ahead,
it seems to be the sub-Gaussian parameter of the additive noise.}
\item \rc{Perhaps Section 3 should be titled something other than ``Additive
faithfulness'' (maybe ``Population- level results''?).}
\item \rc{The first paragraph of Section 5 should read
  ``Figure 3'' rather than ``Algorithm 3.''}

\item \rc{The second paragraph of Section 5 has $m\hat{}u$ instead of $\hat\mu$.}
\item \rc{The first sentence of Section 5.1 should read optimization (4.8).}
\end{enumerate}

\end{enumerate}

\subsection*{Reviewer 2}

\rc{I found many (unimportant) mistakes in the proofs (I
  did not mention them all in the minor comments), which need to be
  read again carefully.}

\begin{enumerate}
\item \rc{I have a problem with the assertion of
  uniqueness in Lemma 3.1 and Therorem 3.2. First, in Lemma 3.1, it is
  clear that $f^∗$ is unique. But $(f_1^∗, \ldots, f_p^∗)$ is not. What is
  shown is that, for $k$ fixed and given $(f_j^*)_{j\neq k}$, $f_k^*$ is unique. Next,
  in Theorem 3.2, the proof of uniqueness (bottom of page 16 and top
  of page 17) is not at all correct.
   Now, to see why I think that, with the assumtions made on $X$, there is
   not uniqueness, simply suppose for instance that $p = 2$ and $X_2 = g(X_1)$
   for a certain function $g$. Then if $(f_1^∗, f_2^∗)$ is a solution,
   $(f_1^∗ + f_2^* \circ g, 0)$ is also a solution.}

\item \rc{In the definition of $g_k^∗$ on pages 6 and 15, it would be more precise to
write $g_k(X_k)$ in the sum instead of $g_k$.}

\item \rc{On page 9, in the proof of Lemma 3.1, the star has been forgotten
twice on $f_{k'}$ in (3.2) and (3.7).}

\item \rc{On page 10, on line 3, ``therefore the solution $f_k^∗(x_k) = \E[f(X)|x_k]
− \E f(X)$ is unique'', the term $−\sum_{k'\neq k} f_{k'}(X_{k'})$ has been forgotten.}

\item \rc{On page 10, on line 14, ``In particular, if $F$ is the uniform
distribution etc'', the term $− \int f$ has been forgotten.}

\item \rc{On page 10, on line 16, ``additively'' should be replaced by
``additive''.}

\item \rc{On page 11, at the bottom of the page, ``before we presenting''}

\item \rc{On page 11, after Deônition 3.2, it is written ``when the joint
density satisfies the property that
$\frac{\partial(x_{-j},x_j)}{\partial x_j} = \frac{\partial^2 p(x_{-j},
  x_j)}{\partial x_j^2} = 0$ at boundary points''.
I think the condition ``$p(x_{−j}, x_j ) = 0$ at the boundary points'' is
missing.}

\item \rc{On page 12, I did not understand why the functions 
$\frac{\partial^2 p(x_{-j} |  x_j)}{\partial x_j^2}$
and adn 
$\frac{\partial^2 f(x_{-j} |  x_j)}{\partial x_j^2}$
are bounded (they are not supposed to be continuous)}

\item \rc{On page 13, on line 6, ``this this''.}

\item \rc{On page 13, on line 23, I do not think $\Sigma$ has been defined. Besides it
is not speciôed that $Var(X_1) = 1$.}
\item \rc{On page 13, on line 29, ``additive'' should be replaced by
``additively''.}
\item \rc{On page 13, the assumption $\E f(X) = 0$ should be made, or else the
parameter $\mu$ should be introduced as before.}
\item \rc{On page 16, in the expresson of $c^∗$, the
  denominator should be $\E X_k^2 −m_k^2$.}
\item \rc{On page 16, in the proof of Theorem 3.2, Lemma 8.3 is used. This
lemma supposes that $\phi$ is continuous. Here I do not see why $\phi =
\sum_{k'\neq k} f_{k'}*$ is continuous (it is convex then continuous on the interior of
$[0,1]^p$. ).}
\item \rc{On page 17, same problem as before with the absence of parameter $\mu$.}
\item \rc{On page 17, on line 16 ``for that variable. for each''}
\item \rc{On page 18, at the bottom of the page :
  ``$\beta_{\pi_k(i+1)k} 
\geq \beta_{\pi_k(i)k}$ for $i=1,\ldots, n-1$'' , $n−1$ must be replaced by $n−2$.}
\item \rc{On page 19, $\sum_{i} f_{ik}$ instead of $\sum_i
  f_{ki}$.}
\item \rc{On page 20, in the third paragraph, on the second, third and fourth
lines, $j$ and $j − 1$ must be replaced by $j + 1$ and $j$ respectively in
$x_{\pi_k(j)k}$ and $x_{\pi_k(j-1)k}$.}

\item \rc{On page 20, In the central matrix equation, the
  first vector should be
  $\left[\begin{matrix}f_k(x_{1k}\\ f_k(x_{2k}\\ \vdots\\f_k(x_{nk}\end{matrix}\right]$. Next
  in ``$\mu_k = -\frac{1}{n} 1_n^T \Delta_k d_k$``, $1_n$ is missing.}
\item \rc{On page 21, in Section 5, at the end of the
  second paragraph, ``$m\hat{}u$ from
our estimation procedure''.}

\item \rc{On page 22, in Deônition 5.1, the inequalities
  about $d_k$ must be replaced by the same inequalities as in Proposition 4.1.}

\item \rc{On page 22 (and on page 31), in the statement of Theorem
  5.1, in\\ $\max_{i=1,\ldots, n} \frac{X_{k\pi_k(i+1)}-
    X_{k\pi_k(i)}}{\text{range}_k} \geq \frac{1}{16}$ the
sign $\geq$ must be replaced by $\leq$ . Besides I think it
is a bad idea to change notation : $X_{k\pi_k(i)}$ should
be replaced by $X_{\pi_k(i)k}$ as before (page 19 for instance).}

\item \rc{On page 23, in equation (5.1), in ``$f^∗(X)$'', the star must be
removed.}

\item \rc{On page 24, ``$\alpha_{+} > 0$ since $f$ is the unique risk minimizer'' : I do
not understand this statement (there is an infimum, not a minimum in
the definition of $\alpha_{+}$.)}
\item \rc{On page 24, in Remark 5.1, ``In important direction''.}
\item \rc{On page 25, in the statement of Theorem 5.3, the square must be
removed on $\alpha_-$.}

\item \rc{On page 25, after the statement of Corollary 5.1 : I do not understand
why ``$p = O(exp(n^c)$'' is written instead of ``$p = O(exp(cn)$.}
\item \rc{On page 26, in the first paragraph ``the relevant variable set'' and
in the third paragraph, ``seting''.}
\item \rc{I really did not understand what was represented in figures 5(a) and
5(b) (axis?). Besides it does not seem to correspond with what is
described at the bottom of page 26 and at the top of page 27. The
notation $\|\cdot\|_{\infty,1}$ is not defined.}
\item \rc{On page 31, at the top of the page, ``see discussion at beginning of
Section 5'' : I think it should be ``at the beginning''. In (8.1),
same problem as before with the indices : ``$\nu_{ki} d_{ki}$ `` must be replaced
with ``$\nu_{ik}d_{ik}$'' (see for instance page 20). At the bottom of the page,
$v_{ki}$ must be replaced by $\nu_{ki}$.}
\item \rc{On page 32, on the first line of the second paragraph, ``$d_k = 0 \mbox{for $k
\in S$}$'', $S$ must be replaced by $S^c$ and $\|\mu\|$ by $\|\mu\|_1$ and ``It clear
that''. Next, the third paragraph (``to ease notational ....'') should
be placed at the beginning of the proof (it is used from the beginning
of the proof, cf conditions on $d_k$).}
\item \rc{On page 33, I find that $[\lambda\Delta_k^Tu]_1 = \lambda((X_{kn} −X_{k1})\kappa$. ( If $\kappa$ is be defined
in the following way : $\kappa =
\frac{1}{\lambda_n(X_{kn}-X_{k1})}[\Delta_k^T u]_1$, then everything
goes well).}

\item \rc{On page 33, on the second line, ``other rows of stationarity
condition holds'' and in the third paragraph : ``following our strategy, We''.}
\item \rc{On page 34, at the bottom of the page, the parentheses in the
denominator of the fraction.}
\item \rc{On page 35, in the third paragraph, ``as a function of $\nu$'' : it is
$\zeta$, not $\nu$. In the fourth paragraph, $\nu$ is not defined. In the fifth
paragraph, in the equation, remove the sum $\sum_{k=1}^p
\bar\Delta_k^T u_k$  and replace
it by the vector ̄ˆ with componen$(\bar\Delta_k^T u_k)_{k=1,\ldots, p}$ In the sixth paragraph,
in $\bar\Delta \hat d_k = \bar\Delta\hat d$ replace $\hat d_k$ by
$\hat d'$.}
\item \rc{On page 36, maybe it would be better to replace $d$ by $c$ here. Add
``for $k\in S^c$ after''to show that $\hat c_k =0$''.}
\item \rc{On page 37, In the first paragraph, I do not understand why $p$ appears
here. In the last paragraph, I find 4 instead of 2 on the first term of
the second line (and then 12 instead of 8 on the third line) .}
\item \rc{on page 38, In the sixth paragraph, in the inequality, the sign
should be $\leq$. In the eighth paragraph, $c''$ must be replaced by $c$. In
the ninth paragraph, ``Taking an union''.}
\item \rc{On page 40, on the fifth line, ``empricial''. In the last paragraph,
``whose size is bounded'' : it is the log of the size.}

\item \rc{I had a problem with the definition of an ε-bracketing. First, in the
definition 8.1, on page 49, `` we define a bracketing'' should be
replaced by `` we define a $\epsilon$-bracketing''. Then it is said that $\rho(f_L ,
f_U ) \leq \epsilon$ and that the corresponding size is $N(\epsilon,C,\rho)$. But then, in
Proposition 8.3 (and almost everywhere else), when $N(2\epsilon,L_2(P),\rho)$ is
used, it is written $\|f_L − f_U \|_{L_2} \leq \epsilon$ (instead of $2\epsilon$. ) (same thing in
Corollary 8.4 for instance : $\epsilon$-bracketing with a size $N(2\epsilon,...)$)}
\item \rc{On page 41, problem with $\epsilon$ (same as the previous item).}
\item \rc{On page 40, I think the functions in $G$ are bounded by $2sB$ (not $sB$.).}
\item \rc{On page 49, in Definition 8.1, $f^U$ and $f^L$ must be replaced by $f_U$ and
$f_L$. On the last line of the page, ``additive convex functions with $s$
components'' : add ``components bounded by $B$''.}
\item \rc{On page 49, ``$\int p(x)^2dx \leq (\int p(x)dx)^2$ ``??}
\item \rc{On page 50, a constant 2 is missing in $\epsilon_n$.}
\item \rc{On page 41, in the second paragraph, I think the bound on $\sup_{h_L} |⟨w,h_L⟩_n|$ is
wrong. I would have used the fact that the variables $h_L(X_i) W_i$ are
independent, centered and sub-Gaussian with a scale smaller than
$2\sigma s B$. It gives a bound of order $sB\sigma \sqrt{\frac{\log\frac{2}{\delta}}{n}}$.}
\item \rc{The term $\epsilon$ supposed to balance the two terms is not correct (for
instance it does not contain $\delta$). There are many small mistakes in the
calculations. In particular, the exponent in $s$ is not the right one.}
\item \rc{In the proof of Theorem 8.4 : too many small mistakes in
calculations, but the final result is OK.}
\item \rc{On page 45, the term $\epsilon$ supposed to balance the terms is not correct
(same remark about $\delta$). In the last equations, on the first line (bottom
of the page) $n$ is missing in one of the scalar products.}
\item \rc{On page 46: ``taking a union bound and we have that''}
\item \rc{In Lemma 8.3, I do not see why the functions are continuous : the
functions $x_{-k} \rightarrow p(x_{-k} | x_k)$ are not supposed to be continuous .}
\item \rc{On page 47, in the proof of Lemma 8.4 : notation $\phi_{-j}$
  has not been defined.
In the definition of $A_+$, the sign should be $>$, not $\geq$. Remove ``both'' .}
\item \rc{On page 47, ``a sub-exponential random is''.}

\end{enumerate}

\subsection*{Reviewer 3}


Major comments:

\begin{enumerate}
\item \rc{The basis of all the results seem to be the boundary flatness
condition. This condition seems to be an artifact of the analysis and
based on my reading, it is unclear how restrctive this assumption
is. The authors claim it is weak just after stating it and then
present several straightforward examples in which it doesn't hold
(e.g. $f(x_1,x_2)=x_1 x_2$) and multivariate Gaussian examples. Hence it is unclear to me how
likely a given function is likely to fit into this framework.}

\item \rc{Following on from the previous point, the simulation study also does
not add any further insight to this issue since the simulations are
for multivariate Gaussians where other parametric approaches that deal
with covariance seem more suitable. It is puzzling to me that the
authors did not consider non-parametric examples where the additive
model framework seems more suitable.}

We consider only densities with support in a hypercube. We have modified old experiments and added new experiments to use boundary flat distributions instead of the correlated multivariate Gaussian distribution. 


\end{enumerate}

Minor comments:

\begin{enumerate}
\item \rc{I didn't think there was any need for Section 2
  since it feels very repetitive and slightly confusing that parts of
  the text are repeated almost verbatim. I would suggest removing this section.}

\item \rc{Page 2, second last paragraph `and and.'}
\end{enumerate}


\vspace*{10pt}

Sincerely, 


Min Xu, Minhua Chen, and John Lafferty\\[1pt]
\today{}

\bibliography{local}
\end{document}
