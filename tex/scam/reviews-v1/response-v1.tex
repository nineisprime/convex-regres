
\documentclass[pdftex,12pt]{article}

\usepackage{hyperref}
\usepackage{fullpage}
\usepackage{enumerate}
\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage{graphicx,subfigure}
\usepackage[pdftex]{color}
\usepackage{epic,eepic,eepicemu}
\usepackage{epsf}
\usepackage{epsfig}
\usepackage{fancyhdr}
\usepackage{graphics}
\usepackage{psfrag,latexsym}
\usepackage{times}
\newcommand{\thetamin}{\ensuremath{\theta^*_{min}}}
\newcommand{\mutinc}{\ensuremath{\alpha}}
\bibliographystyle{abbrv}


\setlength{\textwidth}{\paperwidth}
\addtolength{\textwidth}{-6cm}
\setlength{\textheight}{\paperheight}
\addtolength{\textheight}{-4cm}
\addtolength{\textheight}{-1.1\headheight}
\addtolength{\textheight}{-\headsep}
\addtolength{\textheight}{-\footskip}
\setlength{\oddsidemargin}{0.5cm}
\setlength{\evensidemargin}{0.5cm}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\degmax{d}
\def\pdim{p}
\def\numobs{n}
\let\hat\widehat
\definecolor{blue}{rgb}{0.01,0.01,0.75}
\def\rc#1{{\it\textcolor{blue}{#1}}\smallskip}
\parindent0pt
\parskip12pt

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\vspace*{5pt}

We thank the reviewers for their helpful comments regarding our
submission. We have carefully addressed all these comments. The
following describes the revisions made to the manuscript in response
to the reviews.

\subsection*{Associate Editor}

\rc{Below I recapitulate a few
point of the reviewers', along with some questions/comments of my own.
These (as well as other points in the reviews) need to be thoroughly
addressed in the revised manuscript.}

\begin{enumerate}
\item \rc{Boundary flatness condition: much of the paper's analysis hinges on
this condition.  Reviewer 3 raises some concerns about when it holds,
especially for functions that are not necessarily supported on the
unit hypercube.  See also the point below.}

We have added commentary on this here and here.

\item \rc{Intuitively, variable selection is very closely related to
estimating a function (or possibly its partial derivatives) in the
sup-norm.  It is well-known that accurate estimation of functions
(whether in $L^2$ or the sup-norm) with bounded smoothness in dimension
$s$ requires a sample size that grows exponentially in $s$.  For this
reason, the past result of Comminges and Dalalyan (suggesting an
exponential growth for variable selection) is to be expected.
However, in terms of metric entropy, the class of convex functions
behaves roughly like the class of two-smooth functions (e.g., Dryanov
(2009)).  So at first glance, this makes the result of this paper
rather surprising---namely, that one can somehow side-step any
exponential growth in $s$, even though the metric entropy is still
scaling as badly as the case of 2-smooth functions.  I wondered if the
authors could comment and shed insight on this fact, which seems a
little counterintuitive.  What I suspect that it could mean is that
the additive faithfulness (AF) condition, or one of the sufficient
conditions for AF (namely boundary flatness) are somehow very
restrictive.  Could the authors comment on these issues?}

The boundary flatness doesn't artificially make the variable selection easy. Boundary flatness is much more general than the product density and Comminges and Dalalyan showed that, even under a product density, variable selection for general two-smooth functions is hard. Any bounded density with compact support can be approximated arbitrary well by a boundary flat density (see the newly added Example 3.2).

The intuition behind our result is the following: start with the fact that convex functions are additively faithful under a product density (see paragraphs below Theorem 3.1 for an intuition of this). The behavior of a convex function everywhere is constrained by its behavior at the boundary. Therefore, we need only that the underlying density resembles a product density at the boundary, which is exactly the notion that boundary flatness formalizes.

What we think is that, for functions with shape-constraints such as convexity, the metric entropy alone does not determine the hardness of variable selection; the shape-constraint too plays a direct role. 

\item \rc{Writing and proofs need to be tidied up in places to make it easier
to follow.  See various comments of referees, particularly the
detailed ones of R2.  Also, in Theorem 3, please remind the author of
when/where the AC/DC estimators were defined.  (I was forced to dig
back through.)}

We have tidied up.

\end{enumerate}

\subsection*{Reviewer 1}

\rc{Conceptually, I have a few questions for the authors to address:}

\begin{enumerate}[(1)]
\item \rc{If the true regression function $f_0$ can actually be written as a
sum of univariate convex functions (plus noise), are there natural
conditions that one can provide for when additive faithfulness will
hold? Similarly, is there a simpler version of Theorem 3 that can be
stated when $f_0$ is actually a sum of univariate convex functions? This
would seem to depend on an appropriate statement for Assumption (A3).}

If the true regression function is a convex additive function, then additive faithfulness holds trivially--the best additive approximation to an additive function is the function itself. The estimation procedure becomes simpler--the DC stage is no longer necessary and can be safely removed. 

In the finite sample analysis, the assumptions would not change except that A3 would be $\| f^*_k \|_\infty \leq B$ for all $k$ since $f_0 = f^*$ in this case. The signal strength definition can be made simpler: $\alpha^+ = \min_k \mathbb{E}( f^*_k(X_k)^2 )$. The rates would not improve.


\item \rc{If we actually wish to fit smooth convex univariate components, is
it easy to add this condition as a regularization term and incorporate
it into the analysis?}

Smoothness can be incorporated by using B-spline basis with uniformly spaced knots. The details are described in a 2014 paper by Pya and Wood which we have cited in the introduction. We chose not to discuss estimating smooth convex functions because it would introduce an additional tuning parameter.

The analysis may require much more work if one uses a B-spline basis in the estimation. B-splines may significantly complicate the false positive analysis, which rely on the particular form of the optimization. The false negative analysis may not be affected much because it mainly uses bracketing entropy.

\item \rc{Assuming model selection consistency, can one derive prediction
error bounds using the fitted $f_k$'s and $g_k$'s, as well? Prediction error
bounds seem quite relevant due to possible model misspecification when
$f_0$ is not actually a sum of univariate convex functions.}

To minimize predictive error, we can refit a non-additive convex function after selecting a sparse model. Comparing the predictive accuracy of $f^*_k$'s and $g^*_k$'s against the non-additive true regression function $f_0$ is beyond the scope of our paper, but we do compare the finite sample estimate $\hat{f}_k$ and $\hat{g}_k$ against the population level $f^*_k$ and $g^*_k$. The analysis we do to control false negatives also shows that $\mathbb{E}\left(\sum_{k=1}^p (f^*_k(X_k) - \hat{f}_k(X_k))\right)^2 \rightarrow 0$ and likewise for the DC stage.

\item \rc{In order to improve clarity, the authors should define the convex
additive model at the top of page 5: $f(x_i) = \sum_{k=1}^p f_k(x_{ik})$, where the
$f_k$'s are convex.  Otherwise, the notation in equation (2.3) is
initially a bit confusing.}

We have removed the old equation (2.3) and any discussion of convex additive model at the top of page 5 because everything there is discussed again on page 6. 

\item \rc{Another notational comment: the use of the
distribution function $F$ seems unnecessary (e.g., Lemma 3.1 and
Corollary 3.1), since everything is already defined in terms of the
joint density function $p(x)$. (An additional suggestion would be to use
a different letter to represent the joint density function, since $p$ is
already used to represent the dimensionality of the distribution.)}

We sometimes need a notation for the distribution because we want to write $L^2(P), L_2(P)$ to represent, respectively, the Hilbert space of square integrable functions and the $L_2$-norm with respect to the distribution $P$. We have removed any mention of the distribution where it is not necessary.

We have changed all ``F'' to ``P'' to represent the distribution, and, to avoid confusing the density with the dimensionality, we now write $p(x)$ instead of just $p$ whenever we refer to the density. We believe that this change, along with the context, are enough to disambiguate the two quantities. 

\item \rc{I would suggest including the dependence on the positive-valued lower
bound on $p(x)$ (as assumed in (A5)) in the statement of Theorem 3. This
is a point worth commenting upon, since the earlier theorems only
require $p(x)$ to be strictly positive. Assumption (A5) seems like it
could also be interpreted as a sort of minimum signal strength
requirement for successful variable selection.}

We have added the dependence on the lower bound $c_l$ of $p(x)$ in Theorem 5.2, where we require that $p \leq O(\exp(cn))$ for $c < \frac{c_l}{32}$. We have also added the dependence on the upper bound $c_u$ of the density in Theorem 5.3. The lower and upper bounds $c_l, c_u$ do not directly play the role of the signal level; they are used to simplify technicalities in the presentation and the proof of the theorems. For example, Theorem 5.2 should also go through if one assumes only that the marginal densities $p(x_j)$'s are positive on a sub-interval of $[-1,1]$. 

\item \rc{It would be good to point to Assumption (A3) at the beginning of
Section 5 when discussing the $B$-boundedness condition. In the third
paragraph, the authors might note that the $B$-boundedness condition
arises from the analysis of Theorem 5.2. To clarify, the penultimate
paragraph on page 22 should state that Theorem 5.1 controls false
positives alone, whereas Theorem 5.3 (to follow) provides
finite-sample results controlling the false negative rate.}

We have made the suggested modifications except for the one in the second sentence. The $B$-boundedness condition is used in both Theorem 5.2 and Theorem 5.3. It is relied upon critically in Theorem 5.3 and used for convenience in Theorem 5.2. 


\item \rc{At the top
of page 23, the remark regarding mutual incoherence conditions is a
bit confusing. Aren’t the conditions imposed in Theorem 5.1 on the
covariates essentially analogous to the mutual incoherence conditions
in this case? }

The conditions in Theorem 5.1 are indeed analogous and we have edited the remark accordingly. The conditions in Theorem 5.1 are much more strict than the mutual incoherence condition though.

\item \rc{In the simulation section, the authors might consider comparing the
set of variables selected after the AC step only with the set of
variables selected via the entire AC/DC algorithm. This would help
illustrate the necessity of the concave fitting step in addition to
the convex additive fit alone.}

We have added an additional simulation comparing the variables selected via the AC stage and the DC stage. Please see the last paragraph in Section 6.1 as well as Figure 6. The DC stage is slightly helpful in recovering the true variables but its effect is insignificant. We believe that the DC stage is important in theory but not so in practice. 

Minor typos:
\begin{enumerate}[(i)]
\item \rc{In Section 2.3, it would be good to actually state the regularized
objective function being analyzed, rather than waiting until Section
4.}

We have added the objective function in Section 2.2 where we summarize the optimization algorithm.

\item \rc{Assumption (A3) on page 7 is missing the subscript $\infty$.}

The missing subscript is added.

\item \rc{The parameter $\sigma$ is never defined in Section 2.3. Reading ahead,
it seems to be the sub-Gaussian parameter of the additive noise.}

$\sigma$ has been defined in Section 2.3.

\item \rc{Perhaps Section 3 should be titled something other than ``Additive
faithfulness'' (maybe ``Population- level results''?).}

TODO

\item \rc{The first paragraph of Section 5 should read
  ``Figure 3'' rather than ``Algorithm 3.''}

Corrected.

\item \rc{The second paragraph of Section 5 has $m\hat{}u$ instead of $\hat\mu$.}

Corrected.

\item \rc{The first sentence of Section 5.1 should read optimization (4.8).}

Corrected.

\end{enumerate}

\end{enumerate}

\subsection*{Reviewer 2}


We are grateful to the reviewer for the detailed reading. We have corrected the mistakes found and further proof-read the paper. We discuss the nontrivial issues here.  

\rc{I found many (unimportant) mistakes in the proofs (I
  did not mention them all in the minor comments), which need to be
  read again carefully.}

\begin{enumerate}
\item \rc{I have a problem with the assertion of
  uniqueness in Lemma 3.1 and Therorem 3.2. First, in Lemma 3.1, it is
  clear that $f^∗$ is unique. But $(f_1^∗, \ldots, f_p^∗)$ is not. What is
  shown is that, for $k$ fixed and given $(f_j^*)_{j\neq k}$, $f_k^*$ is unique. Next,
  in Theorem 3.2, the proof of uniqueness (bottom of page 16 and top
  of page 17) is not at all correct.
   Now, to see why I think that, with the assumtions made on $X$, there is
   not uniqueness, simply suppose for instance that $p = 2$ and $X_2 = g(X_1)$
   for a certain function $g$. Then if $(f_1^∗, f_2^∗)$ is a solution,
   $(f_1^∗ + f_2^* \circ g, 0)$ is also a solution.}


We have corrected the proof of Lemma 3.1 and we have given a more rigorous proof that the components $f^*_k$'s are unique in a new Lemma 8.3 (old Lemma 8.3 was removed). Our result requires that the density $p(\mathbf{x}) > 0$ and thus, the $X_2 = g(X_1)$ example raised by the reviewer is ruled out. 

%\item \rc{In the definition of $g_k^∗$ on pages 6 and 15, it would be more precise to
%write $g_k(X_k)$ in the sum instead of $g_k$.}

%\item \rc{On page 9, in the proof of Lemma 3.1, the star has been forgotten
%twice on $f_{k'}$ in (3.2) and (3.7).}

%\item \rc{On page 10, on line 3, ``therefore the solution $f_k^∗(x_k) = \E[f(X)|x_k]
%− \E f(X)$ is unique'', the term $−\sum_{k'\neq k} f_{k'}(X_{k'})$ has been forgotten.}

%\item \rc{On page 10, on line 14, ``In particular, if $F$ is the uniform
%distribution etc'', the term $− \int f$ has been forgotten.}

%\item \rc{On page 10, on line 16, ``additively'' should be replaced by
%``additive''.}

%\item \rc{On page 11, at the bottom of the page, ``before we presenting''}

\item \rc{On page 11, after Deônition 3.2, it is written ``when the joint
density satisfies the property that
$\frac{\partial(x_{-j},x_j)}{\partial x_j} = \frac{\partial^2 p(x_{-j},
  x_j)}{\partial x_j^2} = 0$ at boundary points''.
I think the condition ``$p(x_{−j}, x_j ) = 0$ at the boundary points'' is
missing.}

It is not necessary that $p(\mathbf{x}_{-j}, x_j) = 0$ at $x_j = 0,1$; it is necessary that $p(\mathbf{x}_{-j}, x_j) > 0$. We have however, added an additional condition that $p(\mathbf{x})$ has a bounded second derivative. This condition ensures that the marginal density $p(x_j)$ is twice differentiable and $p'(x_j) = p''(x_j) = 0$ at $x_j = 0,1$. With this, it is valid to apply the quotient rule for differentiation to show that boundary flatness holds. Please see the second item in Example 3.2 for the relevant modifications. 


\item \rc{On page 12, I did not understand why the functions 
$\frac{\partial^2 p(x_{-j} |  x_j)}{\partial x_j^2}$
and adn 
$\frac{\partial^2 f(x_{-j} |  x_j)}{\partial x_j^2}$
are bounded (they are not supposed to be continuous)}


We indeed need a bounded condition here. We have modified the definition of boundary flatness so that $p(\mathbf{x_{-j}} \,|\, x_j)$ is bounded and has a bounded first and second derivative for all $x_j$ in some open set around the boundary points and for all $\mathbf{x}_{-j}$. Dominated convergenc theorem holds without problems because we only exchange the derivative and the integral at $x_j = 0,1$. We have verified in detail that the domination condition holds in the newly added Section 8.4.1.

%\item \rc{On page 13, on line 6, ``this this''.}

\item \rc{On page 13, on line 23, I do not think $\Sigma$ has been defined. Besides it
is not speciôed that $Var(X_1) = 1$.}

$\Sigma$ has been defined as the covariance matrix with diagonal entries equal to 1.

%\item \rc{On page 13, on line 29, ``additive'' should be replaced by
%``additively''.}

\item \rc{On page 13, the assumption $\E f(X) = 0$ should be made, or else the
parameter $\mu$ should be introduced as before.}

We have added the $\E f(X) = 0$ assumption.

%\item \rc{On page 16, in the expresson of $c^∗$, the
%  denominator should be $\E X_k^2 −m_k^2$.}

\item \rc{On page 16, in the proof of Theorem 3.2, Lemma 8.3 is used. This
lemma supposes that $\phi$ is continuous. Here I do not see why $\phi =
\sum_{k'\neq k} f_{k'}*$ is continuous (it is convex then continuous on the interior of
$[0,1]^p$. ).}

We have updated the statement and the proof of Theorem 3.2 so that only boundedness is required of $\phi$. Furthermore, $h^*_k$ is no longer required to be twice differentiable; it need only be square-integrable. The old Lemma 8.3 is no longer necessary and has been removed. 

\item \rc{On page 17, same problem as before with the absence of parameter $\mu$.}

$\mu$ has been added.

%\item \rc{On page 17, on line 16 ``for that variable. for each''}

%\item \rc{On page 18, at the bottom of the page :
%  ``$\beta_{\pi_k(i+1)k} 
%\geq \beta_{\pi_k(i)k}$ for $i=1,\ldots, n-1$'' , $n−1$ must be replaced by $n−2$.}

\item \rc{On page 19, $\sum_{i} f_{ik}$ instead of $\sum_i
  f_{ki}$.}

This has been fixed. We have also gone through the paper and corrected all instances of the $f_{ik}, f_{ki}$ notation inconsistency so that $i$ always come before $k$.

%\item \rc{On page 20, in the third paragraph, on the second, third and fourth
%lines, $j$ and $j − 1$ must be replaced by $j + 1$ and $j$ respectively in
%$x_{\pi_k(j)k}$ and $x_{\pi_k(j-1)k}$.}

%\item \rc{On page 20, In the central matrix equation, the
%  first vector should be
%  $\left[\begin{matrix}f_k(x_{1k}\\ f_k(x_{2k}\\ \vdots\\f_k(x_{nk}\end{matrix}\right]$. N%ext
%  in ``$\mu_k = -\frac{1}{n} 1_n^T \Delta_k d_k$``, $1_n$ is missing.}
%\item \rc{On page 21, in Section 5, at the end of the
%  second paragraph, ``$m\hat{}u$ from
%our estimation procedure''.}


%\item \rc{On page 22, in Deônition 5.1, the inequalities
%  about $d_k$ must be replaced by the same inequalities as in Proposition 4.1.}

\item \rc{On page 22 (and on page 31), in the statement of Theorem
  5.1, in\\ $\max_{i=1,\ldots, n} \frac{X_{k\pi_k(i+1)}-
    X_{k\pi_k(i)}}{\text{range}_k} \geq \frac{1}{16}$ the
sign $\geq$ must be replaced by $\leq$ . Besides I think it
is a bad idea to change notation : $X_{k\pi_k(i)}$ should
be replaced by $X_{\pi_k(i)k}$ as before (page 19 for instance).}

We have corrected all instances of this notation inconsistency.

%\item \rc{On page 23, in equation (5.1), in ``$f^∗(X)$'', the star must be
%removed.}

\item \rc{On page 24, ``$\alpha_{+} > 0$ since $f$ is the unique risk minimizer'' : I do
not understand this statement (there is an infimum, not a minimum in
the definition of $\alpha_{+}$.)}

The infimum is achieved by some $f \neq f^*$ since the constraint set $\{ f \in \mathcal{C}^p_1 \,:\, \textrm{supp}(f) \subsetneq \textrm{supp}(f^*)\}$ is a finite union of closed convex sets. It is $\bigcup_k \{ f \in \mathcal{C}^p_1 \,:\, \textrm{supp}(f) = \textrm{supp}(f^*) - \{k\}\}$. Therefore, the infimum is the minimum of a finite number of projections onto closed convex sets. We have changed the ``inf'' to ``min'' and added an explanation in the paper to remove the confusion.

%\item \rc{On page 24, in Remark 5.1, ``In important direction''.}

\item \rc{On page 25, in the statement of Theorem 5.3, the square must be
removed on $\alpha_-$.}

The square is not an error. We have a stronger requirement on the signal strength for the DC stage. This is because the $\hat{g}_k$'s are fitted onto the finite sample residual and thus, we need $\hat{f}$ to be close to $f^*$ in order to be able to prove something about the $\hat{g}_k$'s.

\item \rc{On page 25, after the statement of Corollary 5.1 : I do not understand
why ``$p = O(exp(n^c)$'' is written instead of ``$p = O(exp(cn)$''.}

We have changed it to $\exp(cn)$.

%\item \rc{On page 26, in the first paragraph ``the relevant variable set'' and
%in the third paragraph, ``seting''.}

\item \rc{I really did not understand what was represented in figures 5(a) and
5(b) (axis?). Besides it does not seem to correspond with what is
described at the bottom of page 26 and at the top of page 27. The
notation $\|\cdot\|_{\infty,1}$ is not defined.}

We have further explained the old figure 5a and 5b (now figure 7a and 7b) in the experiment section. The following has been added to the paper:

... we plot on the y-axis the norm $\|f^{(t)}_j\|_{\infty}$ of every column $j$ against the regularization strength $\lambda^{(t)}$. Instead of plotting the value of $\lambda^{(t)}$ on the x-axis however, we plot the total norm at $\lambda^{(t)}$ normalized against the total norm at $\lambda^{(1)}$: $\frac{\sum_j \|f^{(t)}_j\|_{\infty}}{\sum_j \|f^{(1)}\|_{\infty}}$. Thus, as x-axis moves from 0 to 1, the regularization goes from strong to weak.

\item \rc{On page 31, at the top of the page, ``see discussion at beginning of
Section 5'' : I think it should be ``at the beginning''. In (8.1),
same problem as before with the indices : ``$\nu_{ki} d_{ki}$ '' must be replaced
with ``$\nu_{ik}d_{ik}$'' (see for instance page 20). At the bottom of the page,
$v_{ki}$ must be replaced by $\nu_{ki}$.} 

The typo and the index notation inconsistency have been corrected.

\item \rc{On page 32, on the first line of the second paragraph, ``$d_k = 0 \mbox{for $k
\in S$}$'', $S$ must be replaced by $S^c$ and $\|\mu\|$ by $\|\mu\|_1$ and ``It clear
that''. Next, the third paragraph (``to ease notational ....'') should
be placed at the beginning of the proof (it is used from the beginning
of the proof, cf conditions on $d_k$).}

We cannot move the third paragraph to the beginning because it is not clear at that point that $k$ is fixed. We have corrected all the notations before the third paragraph so that the notations are consistent.

\item \rc{On page 33, I find that $[\lambda\Delta_k^Tu]_1 = \lambda((X_{kn} −X_{k1})\kappa$. ( If $\kappa$ is be defined
in the following way : $\kappa = \frac{1}{\lambda_n(X_{kn}-X_{k1})}[\Delta_k^T u]_1$, then everything
goes well).}

The original definition of $\kappa$ was indeed wrong; we forgot to update $\kappa$ when we edited the proof. We have corrected this in the paper.

%\item \rc{On page 33, on the second line, ``other rows of stationarity
%condition holds'' and in the third paragraph : ``following our strategy, We''.}

%\item \rc{On page 34, at the bottom of the page, the parentheses in the
%denominator of the fraction.}

\item \rc{On page 35, in the third paragraph, ``as a function of $\nu$'' : it is
$\zeta$, not $\nu$. In the fourth paragraph, $\nu$ is not defined. In the fifth
paragraph, in the equation, remove the sum $\sum_{k=1}^p
\bar\Delta_k^T u_k$  and replace
it by the vector ̄ˆ with componen$(\bar\Delta_k^T u_k)_{k=1,\ldots, p}$ In the sixth paragraph,
in $\bar\Delta \hat d_k = \bar\Delta\hat d$ replace $\hat d_k$ by
$\hat d'$.}

We have corrected the expression. The sum has been replaced by a vector.

\item \rc{On page 36, maybe it would be better to replace $d$ by $c$ here. Add
``for $k\in S^c$ after''to show that $\hat c_k =0$''.}

We have replaced $d$ by $c$. 

\item \rc{On page 37, In the first paragraph, I do not understand why $p$ appears
here. In the last paragraph, I find 4 instead of 2 on the first term of
the second line (and then 12 instead of 8 on the third line) .}

We have removed mentions of $p$ here and corrected the constants.

%\item \rc{on page 38, In the sixth paragraph, in the inequality, the sign
%should be $\leq$. In the eighth paragraph, $c''$ must be replaced by $c$. In
%the ninth paragraph, ``Taking an union''.}

%\item \rc{On page 40, on the fifth line, ``empricial''. In the last paragraph,
%``whose size is bounded'' : it is the log of the size.}


\item \rc{I had a problem with the definition of an ε-bracketing. First, in the
definition 8.1, on page 49, `` we define a bracketing'' should be
replaced by `` we define a $\epsilon$-bracketing''. Then it is said that $\rho(f_L ,
f_U ) \leq \epsilon$ and that the corresponding size is $N(\epsilon,C,\rho)$. But then, in
Proposition 8.3 (and almost everywhere else), when $N(2\epsilon,L_2(P),\rho)$ is
used, it is written $\|f_L − f_U \|_{L_2} \leq \epsilon$ (instead of $2\epsilon$. ) (same thing in
Corollary 8.4 for instance : $\epsilon$-bracketing with a size $N(2\epsilon,...)$)}

We have corrected this problem by replacing all instances of $N(2\epsilon,...)$ with $N(\epsilon, ...)$ and updating the bound for $\log N(...)$ accordingly. 

%\item \rc{On page 41, problem with $\epsilon$ (same as the previous item).}
\item \rc{On page 40, I think the functions in $G$ are bounded by $2sB$ (not $sB$.).}

This has been noted and corrected across the paper.

\item \rc{On page 49, in Definition 8.1, $f^U$ and $f^L$ must be replaced by $f_U$ and
$f_L$. On the last line of the page, ``additive convex functions with $s$
components'' : add ``components bounded by $B$''.}

We have corrected the instances in which we erroneously omitted ``... components bounded by $B$''.

\item \rc{On page 49, ``$\int p(x)^2dx \leq (\int p(x)dx)^2$ ``??}

We have corrected this mistake by placing an upper bound $c_u$ on the density $p(\mathbf{x})$ and incorporating that upper bound into this derivation. We now use the correct bound $\sqrt{ \int p(x)^2 dx } \leq c_u$.

\item \rc{On page 50, a constant 2 is missing in $\epsilon_n$.}

This mistake has been corrected across the paper.

\item \rc{On page 41, in the second paragraph, I think the bound on $\sup_{h_L} |⟨w,h_L⟩_n|$ is
wrong. I would have used the fact that the variables $h_L(X_i) W_i$ are
independent, centered and sub-Gaussian with a scale smaller than
$2\sigma s B$. It gives a bound of order $sB\sigma \sqrt{\frac{\log\frac{2}{\delta}}{n}}$.}

We have updated this part of the proof.

\item \rc{The term $\epsilon$ supposed to balance the two terms is not correct (for
instance it does not contain $\delta$). There are many small mistakes in the
calculations. In particular, the exponent in $s$ is not the right one.}

The $\epsilon$ we choose is sufficient to get the bounds we want but it does not exactly balance the terms. We left dependence on $\delta$ out of the $\epsilon$ intentionally because we need to verify that $\epsilon$ satisfies certain conditions, see Equation (8.12) for instance. These conditions are much easier to verify if $\epsilon$ does not depend on $\delta$, which we set to be $1/n$ only at the end. We have updated the paper to clearly state that we use a suboptimal $\epsilon$.

\item \rc{In the proof of Theorem 8.4 : too many small mistakes in
calculations, but the final result is OK.}

TODO

\item \rc{On page 45, the term $\epsilon$ supposed to balance the terms is not correct
(same remark about $\delta$). In the last equations, on the first line (bottom
of the page) $n$ is missing in one of the scalar products.}

We also use a suboptimal $\epsilon$ here for convenience. We have updated the paper to clearly state that.

%\item \rc{On page 46: ``taking a union bound and we have that''}
\item \rc{In Lemma 8.3, I do not see why the functions are continuous : the
functions $x_{-k} \rightarrow p(x_{-k} | x_k)$ are not supposed to be continuous .}

The old Lemma 8.3 has been removed. We now just need $x_{-k} \rightarrow p(x_{-k} \,|\, x_k)$ to be bounded, which has been added as a condition. (We now assume for convenience that the joint density $p(\mathbf{x})$ be bounded away from 0 and $\infty$.)

\item \rc{On page 47, in the proof of Lemma 8.4 : notation $\phi_{-j}$
  has not been defined.
In the definition of $A_+$, the sign should be $>$, not $\geq$. Remove ``both'' .}

The old Lemma 8.4 (now Lemma 8.3) has been updated. 

%\item \rc{On page 47, ``a sub-exponential random is''.}

\end{enumerate}

\subsection*{Reviewer 3}


Major comments:

\begin{enumerate}
\item \rc{The basis of all the results seem to be the boundary flatness
condition. This condition seems to be an artifact of the analysis and
based on my reading, it is unclear how restrctive this assumption
is. The authors claim it is weak just after stating it and then
present several straightforward examples in which it doesn't hold
(e.g. $f(x_1,x_2)=x_1 x_2$) and multivariate Gaussian examples. Hence it is unclear to me how
likely a given function is likely to fit into this framework.}

The boundary flatness condition is not restrictive in the sense that for any density $p(x)$ supported on $[0,1]^p$, and for any $\epsilon > 0$, there exists a boundary flat density whose $L_1$ distance with $p(x)$ is at most $\epsilon$ (Please see the newly added Example 3.2). This is because boundary flatness places no restriction on what $p(x)$ looks like in the interior of the hypercube. Boundary flatness allows arbitrary correlation structure, provided that $p(x) > 0$. We have added Section 3.2 to the paper to give a more detailed discussion on boundary flatness. 

$f(x_1,x_2) = x_1 x_2$ is non-convex and it is an example of how non-convex functions may not be additively faithful under the uniform distribution; it is not an example in which boundary flatness doesn't hold. The multivariate Gaussian is a counterexample but it is to show that boundary flatness may not extend easily to densities with unbounded support. 

We acknowledge that many distributions are not boundary flat. We present boundary flatness as a significant generalization of the product density, under which nonparametric variable selection is already very difficult.

Boundary flatness is not a contrived condition. We initially proved additive faithfulness only for product densities. But then, knowing that the behavior of a convex function everywhere is constrained by its behavior at the boundary, we conjectured that underlying density may only need to ``look like'' a product density at the boundary. Attempts to formalize this notion led us to the definition of boundary flatness.

\item \rc{Following on from the previous point, the simulation study also does
not add any further insight to this issue since the simulations are
for multivariate Gaussians where other parametric approaches that deal
with covariance seem more suitable. It is puzzling to me that the
authors did not consider non-parametric examples where the additive
model framework seems more suitable.}

We consider only densities with support in a hypercube. We have modified old experiments and added new experiments to use boundary flat distributions instead of the correlated multivariate Gaussian distribution. 


\end{enumerate}

Minor comments:

\begin{enumerate}
\item \rc{I didn't think there was any need for Section 2
  since it feels very repetitive and slightly confusing that parts of
  the text are repeated almost verbatim. I would suggest removing this section.}

\item \rc{Page 2, second last paragraph `and and.'}
\end{enumerate}


\vspace*{10pt}

Sincerely, 


Min Xu, Minhua Chen, and John Lafferty\\[1pt]
\today{}

\bibliography{local}
\end{document}
