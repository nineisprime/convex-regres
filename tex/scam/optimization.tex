\section{Optimization}

We describe in detail the optimization algorithm for the additive convex regression stage, the second decoupled concave regression stage follows almost identical procedure. 

We let $\bds{x}_{i}\in\mathbb{R}^{p}$ be the covariate, $Y_{i}$ be the
response and $\epsilon_{i}$ be the mean zero noise. The regression function $f(\cdot)$ we estimate is the summation of 
functions $f_{k}(\cdot)$ in each variable dimension and a scalar offset $\mu$.  
We impose an additional constraint that each $f_{k}(\cdot)$ is 
an univariate convex function, which can be represented by its supporting hyperplanes, i.e.,
\begin{equation}\label{hyper}
      f_{kj} \geq f_{ki} + \beta_{ki}(x_{kj}-x_{ki}) \quad (\forall i,j)
\end{equation}
where $f_{ki}\coloneqq f_{k}(x_{ki})$ and $\beta_{ki}$ is the
subgradient at point $x_{ki}$. We apparently need $O(n^2 p)$ constraints to
impose the supporting hyperplane constraints, which is computationally
expensive for large scale problems.  In fact, only $O(np)$
constraints suffice, since univariate convex functions are
characterized by the condition that the subgradient, which is a scalar, must
increase monotonically. This observation leads to our optimization
program:
\begin{equation}
\begin{split}
       \min_{\bds{h},\bds{\beta},\mu} & \;\; \frac{1}{2n}\sum_{i=1}^{n}
                     \Bigl( Y_{i}-\sum_{k=1}^{p}f_{ki} - \mu \Bigr)^{2} 
                         + \lambda\sum_{k=1}^{p}\|\bds{f}_{k\cdot}\|_{\infty} \\
       \textrm{subject to} &\;\; f_{k(i+1)} = f_{k(i)} + \beta_{k(i)}(x_{k(i+1)}-x_{k(i)}), \\
                     & \;\; \sum_{i=1}^{n}f_{ki}=0,\\
                     & \;\; \beta_{k(i+1)} \geq \beta_{k(i)} \ (\forall k, i)
\end{split}
\label{np}
\end{equation}
Here $\{(1),(2),\ldots,(n)\}$ is a reordering of $\{1,2,\ldots,n\}$ such that $x_{k(1)}\leq{}x_{k(2)}\leq\cdots\leq{}x_{k(n)}$. 

We can solve for $\mu$ explicitly, as  
$\mu = \frac{1}{n} \sum_{i=1}^n Y_i = \bar{Y}$ which follows from the
KKT conditions
and the constraints $\sum_i f_{ki} = 0$.
It is easy to verify that the constraints in (\ref{np}) satisfy the supporting hyperplane constraints, as
\begin{align*}
  \forall & j\geq i, \; f_{k(j)}-f_{k(i)}  = \sum\limits_{t=i}^{j-1}(f_{k(t+1)}-f_{k(t)}) \\
   &= \sum\limits_{t=i}^{j-1}\beta_{k(t)}(x_{k(t+1)}-x_{k(t)})
   \geq \beta_{k(i)}\sum\limits_{t=i}^{j-1}(x_{k(t+1)}-x_{k(t)}) 
  = \beta_{k(i)}(x_{k(j)}-x_{k(i)}) \\
  \forall & j<i,\;  f_{k(j)}-f_{k(i)} =
                \sum\limits_{t=j}^{i-1}(f_{k(t)}-f_{k(t+1)}) \\
     & = \sum\limits_{t=j}^{i-1}\beta_{k(t)}(x_{k(t)}-x_{k(t+1)}) 
     \geq \beta_{k(i)}\sum\limits_{t=j}^{i-1}(x_{k(t)}-x_{k(t+1)}) = \beta_{k(i)}(x_{k(j)}-x_{k(i)}).
\end{align*}


%\begin{SCfigure}
%\label{fig:outer_approximation}
%\includegraphics[width=0.3\textwidth]{figs/outer_approximation.pdf}
%\caption{With the 5 sample points $(X_i, h(X_i))$, the
%  black and the blue convex function represent equivalent fits. SCAM
%  chooses the inner piece-wise linear convex functions.}
%\end{SCfigure}

The sparse convex additive model optimization in (\ref{np}) is a quadratic program (QP) with
$O(np)$ variables and $O(np)$ constraints. 
Directly applying a QP solver for $\bds{h},
\bds{\beta}$ would be computationally expensive for relatively large
$n$ and $p$. However, notice that variables in different feature
dimensions are only coupled in the term
$(Y_{i}-\sum_{k=1}^{p}h_{ki})^{2}$. Hence, we can apply the block
coordinate descent method, where in each step we solve the following
QP subproblem for $\{\bds{h}_{k\cdot},\bds{\beta}_{k\cdot}\}$ with the
other variables fixed. We present the one-block optimization in matrix notation:
\begin{align}
\min_{ \bds{f}_k, \bds{\beta}_k, \gamma_k}& \frac{1}{2n} \| \bds{r}_{\mh k} - \bds{f}_k \|_2^2 
     + \lambda \gamma_k \label{opt:1d_compact} \\
 \textrm{s.t. } & P \bds{f}_k = \diag(P \bds{x}_k)  \bds{\beta}_k \nonumber \\
   & D \bds{\beta}_k \leq 0 ,
   \quad -\gamma_k \mathbf{1}_n \leq \bds{f}_k \leq \gamma_k \mathbf{1}_n,
   \quad \mathbf{1}_n^\tran \bds{f}_k = 0 \nonumber
\end{align}

where $\bds{f}_k \in \R^n$ and equals $(f_{k1}, ..., f_{kn})$, $\bds{\beta}_k \in \R^{n-1}$ and equals $(\beta_{k1}, ..., \beta_{k(n-1)})$. $\bds{r}_{\mh k} \in \R^n$ is the residual vector: $\bds{r}_{\mh k}(i) = Y_i - \bar{Y} - \sum_{k' \neq k} f_{k' i}$. $\bds{x}_k \in \R^n$ and equals $(x_{k1}, ..., x_{kn})$. \\

$P \in \R^{(n-1) \times n}$ is a permutation matrix where the $i'$-th row $P_{i' \cdot}$ is all zero except $-1$ at the $(i')$-th smallest coordinate of $\bds{x}_k$ and $1$ at the $(i'+1)$-th smallest coordinate of $\bds{x}_k$. $D \in \R^{(n-2) \times (n-1)}$ is another permutation matrix where the $i'$-th row is all zero except $1$ at position $i'$ and $-1$ at position $i'+1$. $\diag( v )$ for a vector $v$ is a diagonal matrix whose diagonal entries are $v$.

% \begin{equation}
% \label{eqn:opt_1d}
% \begin{split}
%        \min_{\bds{h}_{k\cdot},\bds{\beta}_{k\cdot},\gamma_{k}} &
%              \ \frac{1}{2n}\sum_{i=1}^{n}\Bigl((Y_{i}-\bar{Y}
%                 -\sum_{r\neq{k}}f_{ri})-f_{ki}\Bigr)^{2} 
%                       + \lambda\gamma_{k} \\
%         \textrm{such that} & \ f_{k(i+1)} = f_{k(i)} + \beta_{k(i)}(x_{k(i+1)}-x_{k(i)}),\\
%         &\ \beta_{k(i+1)} \geq \beta_{k(i)}, \ -\gamma_{k}\leq f_{ki}\leq\gamma_{k}\\
%         &\  \sum_{i=1}^{n}f_{ki}=0, \ (\forall i).
% \end{split}
% \end{equation}
The extra variable $\gamma_{k}$ is introduced to deal with the $\ell_{\infty}$ norm. This QP subproblem involves $O(n)$ variables, $O(n)$ constraints and a sparse structure, 
which can be solved efficiently using optimization packages (e.g., MOSEK: \verb+http://www.mosek.com/+).  We cycle through all feature dimensions ($k$) from $1$ to $p$ multiple times until convergence.
Empirically, we observe that the algorithm converges in only a few cycles. We also implemented an ADMM solver for (\ref{np}), but found that it is not as efficient as this QP solver.

After optimization, the function estimator for any input data $\bds{x}_j$ is, according to (\ref{hyper}),
\begin{equation}
\begin{split}
\nonumber
      f(\bds{x}_j) & = \sum_{k=1}^{p}f_k(x_{kj})+\mu 
= \sum_{k=1}^{p}\max_{i} \{f_{ki}+\beta_{ki}(x_{kj}-x_{ki})\} +
      \mu.
\end{split}
\end{equation} 

The univariate concave function estimation is a straightforward modification of optimization~\ref{opt:1d_compact}. We need only modify one linear inequality constraint to enforce that the subgradients must be non-increasing: $\beta_{k(i+1)} \leq \beta_{k(i)}$.


\subsection{Alternative Formulation}
Optimization (\ref{np}) can be reformulated in terms of the 2nd derivatives, a form which we analyze in our theoretical analysis. The alternative formulation replaces the ordering
constraints $\beta_{k(i+1)} \geq \beta_{k(i)}$ with positivity
constraints, which simplifies theoretical analysis.
Define $d_{k(i)}$ as the second derivative:
$d_{k(1)} = \beta_{k(1)}$, and $d_{k(2)} =
\beta_{k(2)} - \beta_{k(1)}$. The convexity constraint is
equivalent to the constraint that $d_{k(i)} \geq 0$ for all $i >
1$.

It is easy to verify that $\beta_{k(i)} = \sum_{j \leq i} d_{k(j)}$ and 
\begin{align*}
f_k(x_{k(i)}) = & f_k(x_{k(i-1)}) + \beta_{k(i-1)}(x_{k(i)} - x_{k(i-1)}) \\
 =& f_k(x_{k1}) + \sum_{j < i} \beta_{k(j)} (x_{k(j)} - x_{k(j-1)}) \\
 =& f_k(x_{k1}) + \sum_{j < i} \sum_{j' \leq j} d_{k(j')} (x_{k(j)} - x_{k(j-1)})\\
 =& f_k(x_{k1}) + \sum_{j' < i} d_{k(j')} \sum_{i > j \geq j'} (x_{k(j)} - x_{k(j-1)}) \\
 =& f_k(x_{k1}) + \sum_{j' < i} d_{k(j')} (x_{k(i)} - x_{k(j')})
\end{align*}
We can write this more compactly in matrix notations.
\begin{equation*}
\begin{split}
\left[ \begin{array}{c}
f_k(x_{k1}) \\
... \\
f_k(x_{kn}) 
\end{array} \right] =
\left[ \begin{array}{ccc}
    |x_{k1} - x_{k(1)}|_+ & ... & |x_{k1} - x_{k(n-1)}|_+ \\
    ... & & \\
    |x_{kn} - x_{k(1)}|_+ & ... & |x_{kn} - x_{k(n-1)}|_+ 
\end{array} \right]
\left[ \begin{array}{c}
    d_{k(1)} \\
    ... \\
    d_{k(n-1)}
\end{array} \right] 
+ \mu_k \equiv \Delta_k d_k + \mu_k
\end{split}
\end{equation*}


Where $\Delta_k$ is a $n\times n-1$ matrix such that $\Delta_k(i,j) = |x_{ki} - x_{k(j)}|_+$, $d_k = (d_{k(1)} ,..., d_{k(n-1)})$, and $\mu_k = f_k(x_{k1})$. 

Because $f_k$ has to be centered, $\mu_k = - \frac{1}{n} \mathbf{1}_n^\tran \Delta_k d_k$, therefore:
\[
\Delta_k d_k + \mu_k \mathbf{1}_n = 
   \Delta_k d_k - \frac{1}{n} \mathbf{1}_n \mathbf{1}_n^\tran \Delta_k d_k = 
   \bar{\Delta}_k d_k 
\]
where $\bar{\Delta}_k \equiv \Delta_k - \frac{1}{n} \mathbf{1}_n \mathbf{1}_n^\tran \Delta_k$ is $\Delta_k$ with the mean of each column subtracted.

We can now reformulate (\ref{np}) as an equivalent optimization program with only centering and positivity constraints:
\begin{align}
\min_{d_k}& \frac{1}{2n} 
       \Bigl\| Y - \sum_{k=1}^p 
              \bar{\Delta}_k d_k \Bigr\|_2^2 
               + \lambda_n \sum_{k=1}^p \|\bar{\Delta}_k d_k \|_\infty   
     \label{opt:alternate_opt} \\
\trm{s.t. }  & d_{k(2)}, \ldots , d_{k(n-1)} \geq 0  	
               \qquad \trm{(convexity)} \nonumber 
\end{align}

The decoupled concave postprocessing stage optimization is again similar. Suppose $\hat{d}_k$'s are the output of optimization~\ref{opt:alternate_opt}, define $\hat{r} = Y - \sum_{k=1}^p \bar{\Delta}_k \hat{d}_k$. 
\begin{align}
\textrm{for all}&\textrm{ $k$ such that $\hat{d}_k = 0$: }  \nonumber \\
  \min_{c_k} & 
      \frac{1}{2n} \Bigl \| \hat{r} - \Delta_k c_k \Bigr \|_2^2
      + \lambda_n \| \Delta_k c_k \|_\infty 
      \label{opt:alternate_opt_concave}\\
 \trm{s.t.} & c_{k(2)}, \ldots, c_{k(n-1)} \leq 0 \qquad \trm{(concavity)} \nonumber
\end{align}

We can use either the off-centered $\Delta_k$ matrix or the centered $\bar{\Delta}_k$ matrix because the concave estimations are decoupled and hence suffer no identifiability problems.\\

\begin{remark}
in sparsistency analysis, we assume that $L$, an upper bound to the coordinate-wise Lipschitz smoothness of the true function $f_0$, is known and that we constrain our estimate $\hat{f}$ to obey the same Lipschitz condition, that is, each $\hat{f}_k$ must be $L$-Lipschitz. This Lipschitz constraint can be easily added to our optimization program. In optimization~\ref{opt:alternate_opt}, we can enforce the Lipschitz condition by adding two constraints: $d_{k(1)} \geq - L$ and $\sum_{j=2}^{n-1} d_{k(j)} \leq L$ (similarly for optimization~\ref{opt:alternate_opt_concave}). We emphasize that we use the Lipschitz constraint only in our theoretical analysis; all of our experiments do not impose any Lipschitz condition.
\end{remark}


% DO NOT CHANGE; RefTex variables -minx

%%% Local Variables: ***
%%% mode:latex ***
%%% TeX-master: "paper.tex" ***
%%% End: ***